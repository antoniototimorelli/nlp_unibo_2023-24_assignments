{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "\n",
        "- evitare di utilizzare la punctuation nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole)+ preprocessing di termini strani;\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "be0272b7-af95-4214-ac63-82f1f873fedd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:41 Time:  0:02:41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "#ignore = [':', '\"', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88824ae-2dd4-4455-e843-0635df17ac33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         word     tag sentence\n",
            "0      Pierre     NNP        0\n",
            "1      Vinken     NNP        0\n",
            "2           ,       ,        0\n",
            "3          61      CD        0\n",
            "4       years     NNS        0\n",
            "...       ...     ...      ...\n",
            "50743      he     PRP     1962\n",
            "50744     has     VBZ     1962\n",
            "50745   faced     VBN     1962\n",
            "50746   *T*-2  -NONE-     1962\n",
            "50747       .       .     1962\n",
            "\n",
            "[50748 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "13bf8719-bbf9-4be8-9346-0e1ac3c4e50e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 46\n",
            "Val tags number: 45\n",
            "Test tags number: 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jnYemdda_wzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "q32n9MjbqC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "jOf1nvNsqGfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index =  {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "76024379-f487-48e3-c9c6-81b35079b52b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "lxxunk3yqOzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "def update_vocab(df,embeddings_index,embedding_dim):\n",
        "  oov_c = 0\n",
        "  for word in df.word:\n",
        "    if word not in embeddings_index:\n",
        "      oov_c += 1\n",
        "      random_embed = np.random.rand(embedding_dim)\n",
        "      embeddings_index[word] = random_embed\n",
        "  print(\"Added\",oov_c,\"OOV words + respective embeddings to the vocabulary.\")\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = update_vocab(train_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "id": "NuseO3gBjoN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a580ef7-48b3-42c0-9390-5773ce32012a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 2777 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "_2RFWZLCtXZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(val_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSiNQkytU0B",
        "outputId": "12d3a058-2202-4121-9780-5325eae4330d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 951 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "JIW5jaKdt0Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(test_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSoqiGXt4qG",
        "outputId": "1521799f-4472-4c2b-fb44-10c0c89f1c43"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 455 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "index2word = OrderedDict()\n",
        "word2index = OrderedDict()\n",
        "    \n",
        "curr_idx = 0\n",
        "for key in embeddings_index.keys():\n",
        "  word2index[key] = curr_idx\n",
        "  index2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "#word_listing = list(index2word.values())\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(index2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2index)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF0VtzwbBMXO",
        "outputId": "06de33eb-b602-4351-c4b3-e1fa2475a75b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 404183\n",
            "[Debug] Word -> Index vocabulary size: 404183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2index = OrderedDict()\n",
        "index2tag = OrderedDict()\n",
        "\n",
        "curr_id = 0\n",
        "\n",
        "for tag in tags_train:\n",
        "  tag2index[tag] = curr_id\n",
        "  index2tag[curr_id] = tag\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Index -> Tag vocabulary size: {len(index2tag)}')\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2index)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLHdqXbDIzS",
        "outputId": "321091c0-0da2-48d3-aa10-f34845641701"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Tag vocabulary size: 46\n",
            "[Debug] Tag -> Index vocabulary size: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vjNK0jx3_jVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']"
      ],
      "metadata": {
        "id": "WuLsYIaYIBgn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2index[word])\n",
        "   \n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2index[word])\n",
        "\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2index[word])\n",
        "\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "tbPprk74bhkC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANCHE LA PARTE DELL'EMBEDDING LAYER E' ADATTATA AL NUOVO VOCABOLARIO "
      ],
      "metadata": {
        "id": "WiR5y6dm_4-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2index), embedding_dim))\n",
        "for word, i in word2index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2index):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2index), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "14623be8-4012-4733-d1f7-8ed155f7b53d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 271, 300)          121254900 \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 271, 512)         1140736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 271, 46)          23598     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 271, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,419,234\n",
            "Trainable params: 1,164,334\n",
            "Non-trainable params: 121,254,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2index)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2index))),\n",
        "          batch_size=128, epochs= 15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8Dsz2oYyiO",
        "outputId": "1db3193f-c7ee-4dac-f8c9-52f8c0bf65a5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "16/16 [==============================] - 168s 11s/step - loss: 0.7329 - accuracy: 0.8505 - ignore_accuracy: 0.3705 - val_loss: 0.3099 - val_accuracy: 0.9186 - val_ignore_accuracy: 0.1618\n",
            "Epoch 2/15\n",
            "16/16 [==============================] - 106s 7s/step - loss: 0.2856 - accuracy: 0.9249 - ignore_accuracy: 0.2253 - val_loss: 0.2629 - val_accuracy: 0.9323 - val_ignore_accuracy: 0.2929\n",
            "Epoch 3/15\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.2462 - accuracy: 0.9363 - ignore_accuracy: 0.3473 - val_loss: 0.2341 - val_accuracy: 0.9403 - val_ignore_accuracy: 0.3715\n",
            "Epoch 4/15\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.2188 - accuracy: 0.9464 - ignore_accuracy: 0.4427 - val_loss: 0.2086 - val_accuracy: 0.9483 - val_ignore_accuracy: 0.4489\n",
            "Epoch 5/15\n",
            "16/16 [==============================] - 106s 7s/step - loss: 0.1925 - accuracy: 0.9525 - ignore_accuracy: 0.5051 - val_loss: 0.1822 - val_accuracy: 0.9560 - val_ignore_accuracy: 0.5299\n",
            "Epoch 6/15\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.1663 - accuracy: 0.9591 - ignore_accuracy: 0.5738 - val_loss: 0.1567 - val_accuracy: 0.9617 - val_ignore_accuracy: 0.5903\n",
            "Epoch 7/15\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.1421 - accuracy: 0.9655 - ignore_accuracy: 0.6375 - val_loss: 0.1349 - val_accuracy: 0.9669 - val_ignore_accuracy: 0.6439\n",
            "Epoch 8/15\n",
            "16/16 [==============================] - 106s 7s/step - loss: 0.1221 - accuracy: 0.9706 - ignore_accuracy: 0.6926 - val_loss: 0.1176 - val_accuracy: 0.9708 - val_ignore_accuracy: 0.6848\n",
            "Epoch 9/15\n",
            "16/16 [==============================] - 103s 7s/step - loss: 0.1064 - accuracy: 0.9740 - ignore_accuracy: 0.7281 - val_loss: 0.1041 - val_accuracy: 0.9744 - val_ignore_accuracy: 0.7234\n",
            "Epoch 10/15\n",
            "16/16 [==============================] - 111s 7s/step - loss: 0.0942 - accuracy: 0.9770 - ignore_accuracy: 0.7592 - val_loss: 0.0936 - val_accuracy: 0.9768 - val_ignore_accuracy: 0.7492\n",
            "Epoch 11/15\n",
            "16/16 [==============================] - 110s 7s/step - loss: 0.0848 - accuracy: 0.9795 - ignore_accuracy: 0.7858 - val_loss: 0.0854 - val_accuracy: 0.9788 - val_ignore_accuracy: 0.7730\n",
            "Epoch 12/15\n",
            "16/16 [==============================] - 107s 7s/step - loss: 0.0773 - accuracy: 0.9813 - ignore_accuracy: 0.8052 - val_loss: 0.0788 - val_accuracy: 0.9805 - val_ignore_accuracy: 0.7908\n",
            "Epoch 13/15\n",
            "16/16 [==============================] - 108s 7s/step - loss: 0.0713 - accuracy: 0.9828 - ignore_accuracy: 0.8195 - val_loss: 0.0736 - val_accuracy: 0.9818 - val_ignore_accuracy: 0.8047\n",
            "Epoch 14/15\n",
            "16/16 [==============================] - 122s 8s/step - loss: 0.0664 - accuracy: 0.9838 - ignore_accuracy: 0.8298 - val_loss: 0.0692 - val_accuracy: 0.9828 - val_ignore_accuracy: 0.8156\n",
            "Epoch 15/15\n",
            "16/16 [==============================] - 105s 7s/step - loss: 0.0622 - accuracy: 0.9848 - ignore_accuracy: 0.8416 - val_loss: 0.0657 - val_accuracy: 0.9835 - val_ignore_accuracy: 0.8231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2index)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "f658e636-4467-4296-de0c-8cfd7e293792"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 15s 726ms/step - loss: 0.0581 - accuracy: 0.9853 - ignore_accuracy: 0.8437\n",
            "21/21 [==============================] - 16s 710ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ignore = [':', '#', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# Convert the class probabilities into class labels\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "true_pos = defaultdict(int)\n",
        "false_pos = defaultdict(int)\n",
        "false_neg = defaultdict(int)\n",
        "precision = defaultdict(float)\n",
        "recall = defaultdict(float)\n",
        "f1score = defaultdict(float)\n",
        "\n",
        "#index2tag = {v: k for k, v in tag2index.items()}\n",
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    for idx_sentence in range(len(test_tags_y)):\n",
        "      for idx_word in range(MAX_LENGTH):\n",
        "        if index2tag[test_tags_y[idx_sentence][idx_word]] == tag:\n",
        "          # If the predicted tag matches ground truth we increase true positive count for that tag\n",
        "          if predicted_labels[idx_sentence][idx_word] == test_tags_y[idx_sentence][idx_word]:\n",
        "            true_pos[index2tag[test_tags_y[idx_sentence][idx_word]]] += 1\n",
        "          else:\n",
        "          # If the predicted tag does not match ground truth we increase false negative count for that tag\n",
        "          # and false positive count for the wrongly predicted tag\n",
        "            false_neg[index2tag[test_tags_y[idx_sentence][idx_word]]] += 1\n",
        "            false_pos[index2tag[predicted_labels[idx_sentence][idx_word]]] += 1\n",
        "    # We then compute precision, recall and F1 scores\n",
        "    if true_pos[tag] != 0:\n",
        "      precision[tag] = true_pos[tag] / (true_pos[tag] + false_pos[tag])\n",
        "      recall[tag] = true_pos[tag] / (true_pos[tag] + false_neg[tag])\n",
        "      f1score[tag] = 2 * ((precision[tag] * recall[tag]) / (precision[tag] + recall[tag])) \n",
        "    else:\n",
        "      print(tag)\n",
        "      print(true_pos[tag])\n",
        "      print(false_pos[tag])\n",
        "      print(false_neg[tag])\n",
        "      print()"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a8a93a-0760-4a6b-991d-120cf8d3c40d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EX\n",
            "0\n",
            "0\n",
            "5\n",
            "\n",
            "JJR\n",
            "0\n",
            "0\n",
            "59\n",
            "\n",
            "JJS\n",
            "0\n",
            "0\n",
            "31\n",
            "\n",
            "NNPS\n",
            "0\n",
            "0\n",
            "44\n",
            "\n",
            "PDT\n",
            "0\n",
            "0\n",
            "4\n",
            "\n",
            "RBR\n",
            "0\n",
            "0\n",
            "15\n",
            "\n",
            "RBS\n",
            "0\n",
            "0\n",
            "3\n",
            "\n",
            "RP\n",
            "0\n",
            "0\n",
            "33\n",
            "\n",
            "WP$\n",
            "0\n",
            "0\n",
            "4\n",
            "\n",
            "WRB\n",
            "0\n",
            "0\n",
            "24\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    print(f'Tag: {tag}')\n",
        "    print(f'\\tPrecision: {precision[tag]}')\n",
        "    print(f'\\tRecall: {recall[tag]}')\n",
        "    print(f'\\tF1-score: {f1score[tag]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAGbZGkvkY42",
        "outputId": "d8f8f2aa-8be6-4c66-a94b-5a7d8e913cdf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: CC\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9344262295081968\n",
            "\tF1-score: 0.9661016949152543\n",
            "Tag: CD\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.824009324009324\n",
            "\tF1-score: 0.9035143769968051\n",
            "Tag: DT\n",
            "\tPrecision: 0.9868319132455461\n",
            "\tRecall: 0.954307116104869\n",
            "\tF1-score: 0.9702970297029703\n",
            "Tag: EX\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: IN\n",
            "\tPrecision: 0.9900727994705493\n",
            "\tRecall: 0.9177914110429448\n",
            "\tF1-score: 0.9525628780643106\n",
            "Tag: JJ\n",
            "\tPrecision: 0.9425531914893617\n",
            "\tRecall: 0.48257080610021785\n",
            "\tF1-score: 0.638328530259366\n",
            "Tag: JJR\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: JJS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: MD\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9520958083832335\n",
            "\tF1-score: 0.9754601226993864\n",
            "Tag: NN\n",
            "\tPrecision: 0.8698224852071006\n",
            "\tRecall: 0.863617289131347\n",
            "\tF1-score: 0.8667087807959569\n",
            "Tag: NNP\n",
            "\tPrecision: 0.861904761904762\n",
            "\tRecall: 0.9627659574468085\n",
            "\tF1-score: 0.9095477386934673\n",
            "Tag: NNPS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: NNS\n",
            "\tPrecision: 0.8155216284987278\n",
            "\tRecall: 0.6811902231668437\n",
            "\tF1-score: 0.7423277359583091\n",
            "Tag: PDT\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: POS\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9473684210526315\n",
            "\tF1-score: 0.972972972972973\n",
            "Tag: PRP\n",
            "\tPrecision: 0.8895027624309392\n",
            "\tRecall: 0.8385416666666666\n",
            "\tF1-score: 0.8632707774798927\n",
            "Tag: PRP$\n",
            "\tPrecision: 0.9893617021276596\n",
            "\tRecall: 0.9393939393939394\n",
            "\tF1-score: 0.9637305699481866\n",
            "Tag: RB\n",
            "\tPrecision: 0.6621621621621622\n",
            "\tRecall: 0.5144356955380578\n",
            "\tF1-score: 0.5790251107828657\n",
            "Tag: RBR\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: RBS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: RP\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: TO\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 1.0\n",
            "\tF1-score: 1.0\n",
            "Tag: VB\n",
            "\tPrecision: 0.8487584650112867\n",
            "\tRecall: 0.9330024813895782\n",
            "\tF1-score: 0.8888888888888888\n",
            "Tag: VBD\n",
            "\tPrecision: 0.895017793594306\n",
            "\tRecall: 0.7933753943217665\n",
            "\tF1-score: 0.8411371237458194\n",
            "Tag: VBG\n",
            "\tPrecision: 0.8369565217391305\n",
            "\tRecall: 0.34841628959276016\n",
            "\tF1-score: 0.49201277955271566\n",
            "Tag: VBN\n",
            "\tPrecision: 0.7774193548387097\n",
            "\tRecall: 0.6584699453551912\n",
            "\tF1-score: 0.71301775147929\n",
            "Tag: VBP\n",
            "\tPrecision: 0.925531914893617\n",
            "\tRecall: 0.6492537313432836\n",
            "\tF1-score: 0.763157894736842\n",
            "Tag: VBZ\n",
            "\tPrecision: 0.8976377952755905\n",
            "\tRecall: 0.8142857142857143\n",
            "\tF1-score: 0.8539325842696629\n",
            "Tag: WDT\n",
            "\tPrecision: 0.9565217391304348\n",
            "\tRecall: 0.7857142857142857\n",
            "\tF1-score: 0.8627450980392156\n",
            "Tag: WP\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.85\n",
            "\tF1-score: 0.9189189189189189\n",
            "Tag: WP$\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: WRB\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "provola = [el[1] for el in f1score.items()]\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for p in provola:\n",
        "  sum += p\n",
        "\n",
        "print(f'MACRO F1-score: {sum / len(f1score.items())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "3c9fe7ce-65ec-4813-ba21-d84249c8735b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MACRO F1-score: 0.5824268549656594\n"
          ]
        }
      ]
    }
  ]
}