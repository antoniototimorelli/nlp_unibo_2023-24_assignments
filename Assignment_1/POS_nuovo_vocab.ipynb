{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "- Guardare creazione dizionario, bisogna rispettare i punti dell'assignment;\n",
        "- Non togliere punctuation e symbols ma evitare di utilizzarli nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole);\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "250278a3-885f-41ca-b932-f5ca0ce36268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:38 Time:  0:02:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] not in ignore]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] not in ignore]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] not in ignore]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8deca96c-3e02-408d-8960-084ef68e01bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              word  tag sentence\n",
            "0           Pierre  NNP        0\n",
            "1           Vinken  NNP        0\n",
            "2               61   CD        0\n",
            "3            years  NNS        0\n",
            "4              old   JJ        0\n",
            "...            ...  ...      ...\n",
            "41269  acquisition   NN     1962\n",
            "41270    challenge   NN     1962\n",
            "41271           he  PRP     1962\n",
            "41272          has  VBZ     1962\n",
            "41273        faced  VBN     1962\n",
            "\n",
            "[41274 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n",
        "\n",
        "if len(tags_test) != len(tags_val) or len(tags_test) != len(tags_train):\n",
        "  print('\\nMismatching numbers.')\n",
        "  print('Removing extra classes:')\n",
        "\n",
        "  missing_classes_train = [x for x in tags_train if x not in tags_test]\n",
        "  missing_classes_val   = [x for x in tags_val if x not in tags_test]\n",
        "\n",
        "  missing_classes = list(set(missing_classes_train + missing_classes_val))\n",
        "  print(missing_classes)\n",
        "\n",
        "  for cl in missing_classes:\n",
        "    train_df = train_df[train_df.tag != cl]\n",
        "    val_df = val_df[val_df.tag != cl]\n",
        "\n",
        "  tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "  tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "  tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "  print('\\nNew Train tags number:',len(tags_train))\n",
        "  print('New Val tags number:',len(tags_val))\n",
        "  print('New Test tags number:',len(tags_test))\n",
        "\n",
        "print('\\nTags:')\n",
        "for tag in tags_train:\n",
        "  print(f'-{tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "5b0bd559-bf72-4476-a784-935050703201"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 35\n",
            "Val tags number: 35\n",
            "Test tags number: 32\n",
            "\n",
            "Mismatching numbers.\n",
            "Removing extra classes:\n",
            "['LS', 'FW', 'UH']\n",
            "\n",
            "New Train tags number: 32\n",
            "New Val tags number: 32\n",
            "New Test tags number: 32\n",
            "\n",
            "Tags:\n",
            "-CC\n",
            "-CD\n",
            "-DT\n",
            "-EX\n",
            "-IN\n",
            "-JJ\n",
            "-JJR\n",
            "-JJS\n",
            "-MD\n",
            "-NN\n",
            "-NNP\n",
            "-NNPS\n",
            "-NNS\n",
            "-PDT\n",
            "-POS\n",
            "-PRP\n",
            "-PRP$\n",
            "-RB\n",
            "-RBR\n",
            "-RBS\n",
            "-RP\n",
            "-TO\n",
            "-VB\n",
            "-VBD\n",
            "-VBG\n",
            "-VBN\n",
            "-VBP\n",
            "-VBZ\n",
            "-WDT\n",
            "-WP\n",
            "-WP$\n",
            "-WRB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "q32n9MjbqC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "jOf1nvNsqGfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index =  {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "d8fcf22a-550c-452a-885c-f3536ab10000"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "lxxunk3yqOzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "for word in train_df.word:\n",
        "  if word not in embeddings_index:\n",
        "    random_embed = np.random.rand(embedding_dim)\n",
        "    embeddings_index[word] = random_embed\n",
        "print(len(embeddings_index))"
      ],
      "metadata": {
        "id": "NuseO3gBjoN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cad5a54-b9d7-48cc-e2c7-eeee66ea346b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "402338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "_2RFWZLCtXZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in val_df.word:\n",
        "  if word not in embeddings_index:\n",
        "    random_embed = np.random.rand(embedding_dim)\n",
        "    embeddings_index[word] = random_embed\n",
        "print(len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSiNQkytU0B",
        "outputId": "c8389764-3faf-4f68-cdb1-79757a426560"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "JIW5jaKdt0Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in test_df.word:\n",
        "  if word not in embeddings_index:\n",
        "    random_embed = np.random.rand(embedding_dim)\n",
        "    embeddings_index[word] = random_embed\n",
        "print(len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSoqiGXt4qG",
        "outputId": "7cccabd2-d928-4040-c5df-39edf6a64e85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "idx2word = OrderedDict()\n",
        "word2idx = OrderedDict()\n",
        "    \n",
        "curr_idx = 0\n",
        "for key in embeddings_index.keys():\n",
        "  word2idx[key] = curr_idx\n",
        "  idx2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "word_listing = list(idx2word.values())\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF0VtzwbBMXO",
        "outputId": "2db4fe37-d909-4cac-c7e0-e89ff76174bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 403735\n",
            "[Debug] Word -> Index vocabulary size: 403735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2idx = OrderedDict()\n",
        "\n",
        "curr_id = 0\n",
        "\n",
        "for tag in tags_train:\n",
        "  tag2idx[tag] = curr_id\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLHdqXbDIzS",
        "outputId": "c4c6b323-8fcf-4716-e0ca-1b73f4838626"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Tag -> Index vocabulary size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "true_pos_tag = defaultdict(int)\n",
        "false_pos_tag = defaultdict(int)\n",
        "false_neg_tag = defaultdict(int)\n",
        "precision_tags = defaultdict(float)\n",
        "accuracy_tags = defaultdict(float)\n",
        "recall_tags = defaultdict(float)\n",
        "f1score_tags = defaultdict(float)\n"
      ],
      "metadata": {
        "id": "WuLsYIaYIBgn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "   \n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "tbPprk74bhkC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2idx):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2idx), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2idx))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2idx))\n",
        "\n",
        "#Training the model\n",
        "# model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2index)),\\\n",
        "#           validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2index))),\n",
        "#           batch_size=128, epochs= 9, validation_split=0.2)\n",
        "model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2idx)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2idx))),\n",
        "          batch_size=128, epochs= 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "01459b99-3353-49ae-c238-3a0f950e6877"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 271, 300)          121120500 \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 271, 512)         1140736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 271, 32)          16416     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 271, 32)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,277,652\n",
            "Trainable params: 1,157,152\n",
            "Non-trainable params: 121,120,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/9\n",
            "16/16 [==============================] - 99s 6s/step - loss: 0.5656 - accuracy: 0.8681 - ignore_accuracy: 0.3460 - val_loss: 0.2314 - val_accuracy: 0.9354 - val_ignore_accuracy: 0.2341\n",
            "Epoch 2/9\n",
            "16/16 [==============================] - 94s 6s/step - loss: 0.2136 - accuracy: 0.9435 - ignore_accuracy: 0.2933 - val_loss: 0.2015 - val_accuracy: 0.9455 - val_ignore_accuracy: 0.3179\n",
            "Epoch 3/9\n",
            "16/16 [==============================] - 94s 6s/step - loss: 0.1891 - accuracy: 0.9484 - ignore_accuracy: 0.3645 - val_loss: 0.1860 - val_accuracy: 0.9486 - val_ignore_accuracy: 0.3633\n",
            "Epoch 4/9\n",
            "16/16 [==============================] - 92s 6s/step - loss: 0.1742 - accuracy: 0.9528 - ignore_accuracy: 0.4120 - val_loss: 0.1707 - val_accuracy: 0.9539 - val_ignore_accuracy: 0.4147\n",
            "Epoch 5/9\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.1589 - accuracy: 0.9576 - ignore_accuracy: 0.4668 - val_loss: 0.1543 - val_accuracy: 0.9594 - val_ignore_accuracy: 0.4825\n",
            "Epoch 6/9\n",
            "16/16 [==============================] - 92s 6s/step - loss: 0.1428 - accuracy: 0.9624 - ignore_accuracy: 0.5262 - val_loss: 0.1378 - val_accuracy: 0.9637 - val_ignore_accuracy: 0.5360\n",
            "Epoch 7/9\n",
            "16/16 [==============================] - 91s 6s/step - loss: 0.1270 - accuracy: 0.9662 - ignore_accuracy: 0.5738 - val_loss: 0.1227 - val_accuracy: 0.9676 - val_ignore_accuracy: 0.5852\n",
            "Epoch 8/9\n",
            "16/16 [==============================] - 94s 6s/step - loss: 0.1126 - accuracy: 0.9702 - ignore_accuracy: 0.6212 - val_loss: 0.1093 - val_accuracy: 0.9709 - val_ignore_accuracy: 0.6269\n",
            "Epoch 9/9\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.1004 - accuracy: 0.9737 - ignore_accuracy: 0.6643 - val_loss: 0.0982 - val_accuracy: 0.9747 - val_ignore_accuracy: 0.6715\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31d7d4a6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2idx)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in tag2idx.items()})\n",
        "#y_prob_class = model.predict_classes(test_sentences_X, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "27a72e5e-b559-4434-bf90-61718ebd7de1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 15s 691ms/step - loss: 0.0909 - accuracy: 0.9763 - ignore_accuracy: 0.6946\n",
            "21/21 [==============================] - 14s 673ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen_num in range(len(test_tags)):\n",
        "    for i,tag in enumerate(test_tags[sen_num]):\n",
        "      \n",
        "        conf_mat_df[tag][pred_sequence[sen_num][i]] +=1\n",
        "        if test_tags[sen_num][i] == pred_sequence[sen_num][i]:\n",
        "          true_pos_tag[tag] += 1\n",
        "        else:\n",
        "          false_neg_tag[tag] += 1\n",
        "          false_pos_tag[pred_sequence[sen_num][i]] += 1\n",
        "\n",
        "for tag in tag_list[1:]:\n",
        "    if (true_pos_tag[tag] + false_pos_tag[tag]) != 0:\n",
        "      precision_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_pos_tag[tag])\n",
        "      recall_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag])\n",
        "      f1score_tags[tag] = 2 * precision_tags[tag] * recall_tags[tag] / (precision_tags[tag] + recall_tags[tag])\n",
        "      accuracy_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag] + false_pos_tag[tag])"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for el in f1score_tags.items():\n",
        "  sum += el[1]\n",
        "print(sum/len(f1score_tags.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "7d9b7bc2-673e-4aeb-a525-0ad3fa6c629f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6301537559764908\n"
          ]
        }
      ]
    }
  ]
}