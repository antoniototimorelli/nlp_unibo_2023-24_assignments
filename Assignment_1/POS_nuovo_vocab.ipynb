{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "- Guardare creazione dizionario, bisogna rispettare i punti dell'assignment;\n",
        "- Non togliere punctuation e symbols ma evitare di utilizzarli nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole);\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "250278a3-885f-41ca-b932-f5ca0ce36268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:38 Time:  0:02:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] not in ignore]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] not in ignore]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] not in ignore]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8deca96c-3e02-408d-8960-084ef68e01bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              word  tag sentence\n",
            "0           Pierre  NNP        0\n",
            "1           Vinken  NNP        0\n",
            "2               61   CD        0\n",
            "3            years  NNS        0\n",
            "4              old   JJ        0\n",
            "...            ...  ...      ...\n",
            "41269  acquisition   NN     1962\n",
            "41270    challenge   NN     1962\n",
            "41271           he  PRP     1962\n",
            "41272          has  VBZ     1962\n",
            "41273        faced  VBN     1962\n",
            "\n",
            "[41274 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n",
        "\n",
        "if len(tags_test) != len(tags_val) or len(tags_test) != len(tags_train):\n",
        "  print('\\nMismatching numbers.')\n",
        "  print('Removing extra classes:')\n",
        "\n",
        "  missing_classes_train = [x for x in tags_train if x not in tags_test]\n",
        "  missing_classes_val   = [x for x in tags_val if x not in tags_test]\n",
        "\n",
        "  missing_classes = list(set(missing_classes_train + missing_classes_val))\n",
        "  print(missing_classes)\n",
        "\n",
        "  for cl in missing_classes:\n",
        "    train_df = train_df[train_df.tag != cl]\n",
        "    val_df = val_df[val_df.tag != cl]\n",
        "\n",
        "  tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "  tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "  tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "  print('\\nNew Train tags number:',len(tags_train))\n",
        "  print('New Val tags number:',len(tags_val))\n",
        "  print('New Test tags number:',len(tags_test))\n",
        "\n",
        "print('\\nTags:')\n",
        "for tag in tags_train:\n",
        "  print(f'-{tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "5b0bd559-bf72-4476-a784-935050703201"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 35\n",
            "Val tags number: 35\n",
            "Test tags number: 32\n",
            "\n",
            "Mismatching numbers.\n",
            "Removing extra classes:\n",
            "['LS', 'FW', 'UH']\n",
            "\n",
            "New Train tags number: 32\n",
            "New Val tags number: 32\n",
            "New Test tags number: 32\n",
            "\n",
            "Tags:\n",
            "-CC\n",
            "-CD\n",
            "-DT\n",
            "-EX\n",
            "-IN\n",
            "-JJ\n",
            "-JJR\n",
            "-JJS\n",
            "-MD\n",
            "-NN\n",
            "-NNP\n",
            "-NNPS\n",
            "-NNS\n",
            "-PDT\n",
            "-POS\n",
            "-PRP\n",
            "-PRP$\n",
            "-RB\n",
            "-RBR\n",
            "-RBS\n",
            "-RP\n",
            "-TO\n",
            "-VB\n",
            "-VBD\n",
            "-VBG\n",
            "-VBN\n",
            "-VBP\n",
            "-VBZ\n",
            "-WDT\n",
            "-WP\n",
            "-WP$\n",
            "-WRB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "q32n9MjbqC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "jOf1nvNsqGfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index =  {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "a197e1b8-dd4f-4a4e-f188-efaff4aac5b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "lxxunk3yqOzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "def update_vocab(df,embeddings_index,embedding_dim):\n",
        "  oov_c = 0\n",
        "  for word in df.word:\n",
        "    if word not in embeddings_index:\n",
        "      oov_c += 1\n",
        "      random_embed = np.random.rand(embedding_dim)\n",
        "      embeddings_index[word] = random_embed\n",
        "  print(\"Added\",oov_c,\"OOV words + respective embeddings to the vocabulary.\")\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = update_vocab(train_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "id": "NuseO3gBjoN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa1242e-2e3d-49d8-a960-f0207b72d22e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 2338 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "_2RFWZLCtXZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(val_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSiNQkytU0B",
        "outputId": "ab867a77-f667-4454-b5e8-3b5e1024961f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 942 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "JIW5jaKdt0Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(test_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSoqiGXt4qG",
        "outputId": "f3ad97dc-1ab1-4d77-a7ab-28787474109c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 455 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "idx2word = OrderedDict()\n",
        "word2idx = OrderedDict()\n",
        "    \n",
        "curr_idx = 0\n",
        "for key in embeddings_index.keys():\n",
        "  word2idx[key] = curr_idx\n",
        "  idx2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "#word_listing = list(idx2word.values())\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF0VtzwbBMXO",
        "outputId": "2db4fe37-d909-4cac-c7e0-e89ff76174bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 403735\n",
            "[Debug] Word -> Index vocabulary size: 403735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2idx = OrderedDict()\n",
        "\n",
        "curr_id = 0\n",
        "\n",
        "for tag in tags_train:\n",
        "  tag2idx[tag] = curr_id\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLHdqXbDIzS",
        "outputId": "c4c6b323-8fcf-4716-e0ca-1b73f4838626"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Tag -> Index vocabulary size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "true_pos_tag = defaultdict(int)\n",
        "false_pos_tag = defaultdict(int)\n",
        "false_neg_tag = defaultdict(int)\n",
        "precision_tags = defaultdict(float)\n",
        "accuracy_tags = defaultdict(float)\n",
        "recall_tags = defaultdict(float)\n",
        "f1score_tags = defaultdict(float)\n"
      ],
      "metadata": {
        "id": "WuLsYIaYIBgn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "   \n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "tbPprk74bhkC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2idx):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2idx), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2idx))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2idx))\n",
        "\n",
        "#Training the model\n",
        "# model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2index)),\\\n",
        "#           validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2index))),\n",
        "#           batch_size=128, epochs= 9, validation_split=0.2)\n",
        "model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2idx)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2idx))),\n",
        "          batch_size=64, epochs= 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "ef8c89d9-ef0e-443f-ac06-b65ffb1f1412"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 271, 300)          121120500 \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 271, 512)         1140736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 271, 32)          16416     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 271, 32)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,277,652\n",
            "Trainable params: 1,157,152\n",
            "Non-trainable params: 121,120,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "31/31 [==============================] - 118s 4s/step - loss: 0.3866 - accuracy: 0.9048 - ignore_accuracy: 0.3557 - val_loss: 0.2056 - val_accuracy: 0.9439 - val_ignore_accuracy: 0.3051\n",
            "Epoch 2/20\n",
            "31/31 [==============================] - 114s 4s/step - loss: 0.1845 - accuracy: 0.9493 - ignore_accuracy: 0.3752 - val_loss: 0.1734 - val_accuracy: 0.9535 - val_ignore_accuracy: 0.4141\n",
            "Epoch 3/20\n",
            "31/31 [==============================] - 116s 4s/step - loss: 0.1540 - accuracy: 0.9595 - ignore_accuracy: 0.4906 - val_loss: 0.1415 - val_accuracy: 0.9624 - val_ignore_accuracy: 0.5236\n",
            "Epoch 4/20\n",
            "31/31 [==============================] - 132s 4s/step - loss: 0.1237 - accuracy: 0.9674 - ignore_accuracy: 0.5876 - val_loss: 0.1131 - val_accuracy: 0.9698 - val_ignore_accuracy: 0.6137\n",
            "Epoch 5/20\n",
            "31/31 [==============================] - 120s 4s/step - loss: 0.0992 - accuracy: 0.9743 - ignore_accuracy: 0.6723 - val_loss: 0.0928 - val_accuracy: 0.9760 - val_ignore_accuracy: 0.6889\n",
            "Epoch 6/20\n",
            "31/31 [==============================] - 136s 4s/step - loss: 0.0817 - accuracy: 0.9797 - ignore_accuracy: 0.7377 - val_loss: 0.0790 - val_accuracy: 0.9797 - val_ignore_accuracy: 0.7358\n",
            "Epoch 7/20\n",
            "31/31 [==============================] - 131s 4s/step - loss: 0.0701 - accuracy: 0.9824 - ignore_accuracy: 0.7711 - val_loss: 0.0698 - val_accuracy: 0.9818 - val_ignore_accuracy: 0.7613\n",
            "Epoch 8/20\n",
            "31/31 [==============================] - 119s 4s/step - loss: 0.0621 - accuracy: 0.9841 - ignore_accuracy: 0.7930 - val_loss: 0.0633 - val_accuracy: 0.9833 - val_ignore_accuracy: 0.7813\n",
            "Epoch 9/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0559 - accuracy: 0.9858 - ignore_accuracy: 0.8136 - val_loss: 0.0584 - val_accuracy: 0.9846 - val_ignore_accuracy: 0.7980\n",
            "Epoch 10/20\n",
            "31/31 [==============================] - 109s 4s/step - loss: 0.0512 - accuracy: 0.9869 - ignore_accuracy: 0.8292 - val_loss: 0.0546 - val_accuracy: 0.9857 - val_ignore_accuracy: 0.8109\n",
            "Epoch 11/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0473 - accuracy: 0.9879 - ignore_accuracy: 0.8413 - val_loss: 0.0512 - val_accuracy: 0.9863 - val_ignore_accuracy: 0.8191\n",
            "Epoch 12/20\n",
            "31/31 [==============================] - 111s 4s/step - loss: 0.0440 - accuracy: 0.9887 - ignore_accuracy: 0.8515 - val_loss: 0.0490 - val_accuracy: 0.9870 - val_ignore_accuracy: 0.8281\n",
            "Epoch 13/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0411 - accuracy: 0.9893 - ignore_accuracy: 0.8597 - val_loss: 0.0464 - val_accuracy: 0.9874 - val_ignore_accuracy: 0.8340\n",
            "Epoch 14/20\n",
            "31/31 [==============================] - 109s 4s/step - loss: 0.0386 - accuracy: 0.9899 - ignore_accuracy: 0.8673 - val_loss: 0.0448 - val_accuracy: 0.9880 - val_ignore_accuracy: 0.8420\n",
            "Epoch 15/20\n",
            "31/31 [==============================] - 111s 4s/step - loss: 0.0365 - accuracy: 0.9903 - ignore_accuracy: 0.8724 - val_loss: 0.0432 - val_accuracy: 0.9881 - val_ignore_accuracy: 0.8433\n",
            "Epoch 16/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0345 - accuracy: 0.9909 - ignore_accuracy: 0.8802 - val_loss: 0.0420 - val_accuracy: 0.9884 - val_ignore_accuracy: 0.8470\n",
            "Epoch 17/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0328 - accuracy: 0.9913 - ignore_accuracy: 0.8856 - val_loss: 0.0407 - val_accuracy: 0.9887 - val_ignore_accuracy: 0.8499\n",
            "Epoch 18/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0310 - accuracy: 0.9918 - ignore_accuracy: 0.8922 - val_loss: 0.0391 - val_accuracy: 0.9890 - val_ignore_accuracy: 0.8544\n",
            "Epoch 19/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0294 - accuracy: 0.9921 - ignore_accuracy: 0.8965 - val_loss: 0.0380 - val_accuracy: 0.9894 - val_ignore_accuracy: 0.8595\n",
            "Epoch 20/20\n",
            "31/31 [==============================] - 110s 4s/step - loss: 0.0280 - accuracy: 0.9926 - ignore_accuracy: 0.9026 - val_loss: 0.0370 - val_accuracy: 0.9894 - val_ignore_accuracy: 0.8605\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31cdcb4e20>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2idx)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in tag2idx.items()})\n",
        "#y_prob_class = model.predict_classes(test_sentences_X, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "69c8f2ee-34bb-4286-a4ae-8182c04b45db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 14s 649ms/step - loss: 0.0320 - accuracy: 0.9910 - ignore_accuracy: 0.8802\n",
            "21/21 [==============================] - 14s 641ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen_num in range(len(test_tags)):\n",
        "    for i,tag in enumerate(test_tags[sen_num]):\n",
        "      \n",
        "        conf_mat_df[tag][pred_sequence[sen_num][i]] +=1\n",
        "        if test_tags[sen_num][i] == pred_sequence[sen_num][i]:\n",
        "          true_pos_tag[tag] += 1\n",
        "        else:\n",
        "          false_neg_tag[tag] += 1\n",
        "          false_pos_tag[pred_sequence[sen_num][i]] += 1\n",
        "\n",
        "for tag in tag_list[1:]:\n",
        "    if (true_pos_tag[tag] + false_pos_tag[tag]) != 0:\n",
        "      precision_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_pos_tag[tag])\n",
        "      recall_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag])\n",
        "      f1score_tags[tag] = 2 * precision_tags[tag] * recall_tags[tag] / (precision_tags[tag] + recall_tags[tag])\n",
        "      accuracy_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag] + false_pos_tag[tag])"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for el in f1score_tags.items():\n",
        "  sum += el[1]\n",
        "print(sum/len(f1score_tags.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "bfa54d85-ddc7-4ae0-fd7e-cacdb21bcf5a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5487107560173037\n"
          ]
        }
      ]
    }
  ]
}