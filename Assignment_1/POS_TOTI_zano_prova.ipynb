{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "- Guardare creazione dizionario, bisogna rispettare i punti dell'assignment;\n",
        "- Non togliere punctuation e symbols ma evitare di utilizzarli nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole);\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 11/01/2022 (dd/mm/yyyy)\n",
        "\n",
        "If you deliver it by 11/12/2021 your assignment will be graded by 11/01/2022.\n",
        "\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures\n",
        "\n",
        "# Execution\n",
        "## 0. Utils"
      ],
      "metadata": {
        "id": "Y0BInycOeQL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar\n",
        "from IPython.display import display_html\n",
        "from itertools import chain,cycle\n",
        "\n",
        "# Preprocessing of words\n",
        "def pre_process(df,string):\n",
        "    def text_pre_process(text):\n",
        "        ret = re.sub('RT @(.)+?:\\s|(&#[0-9]+;)|@([\\w\\-]+)|(#)\\S+|(http)s?\\S+|&gt;|^\\s+|\\b\\s+|\\n', '', text)\n",
        "        ret = re.sub('\\s\\s+|[^a-zA-Z\\d\\s:]' , ' ', ret).rstrip().lower()\n",
        "        return ret\n",
        "    return df[string].apply(text_pre_process)\n",
        "\n",
        "# Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Display dataframes\n",
        "def display(*args,titles=cycle([''])):\n",
        "    html_str=''\n",
        "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
        "        html_str+='<th style=\"text-align:left\"><td style=\"vertical-align:top\">'\n",
        "        html_str+=f'<h4 style=\"text-align: left;\">{title}</h2>'\n",
        "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
        "        html_str+='</td></th>'\n",
        "    display_html(html_str,raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "d30f31c0-5984-42fd-8273-9ba0722d7d8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:39 Time:  0:02:39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Corpus\n",
        "### 1.1 Pre-processing\n",
        "\n",
        "From the original tags list we removed all the symbols and english punctuation plus:\n",
        "- FW, Foreign Word, because there are no examples in the test set;\n",
        "- UH, Interjection, because there are no examples in the test set;\n",
        "- LS, List Item Marker, because there are no examples in the test set (and because it denotes symbols as well);"
      ],
      "metadata": {
        "id": "5filE0ydeiga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank tagged sentences\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "# ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] not in ignore]\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] not in ignore]\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] not in ignore]\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "#train_df['word'] = train_df['word'].str.lower()\n",
        "# train_df['word'] = pre_process(train_df,'word')\n",
        "\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "#val_df['word'] = val_df['word'].str.lower()\n",
        "# val_df['word'] = pre_process(val_df,'word')\n",
        "\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "#test_df['word'] = test_df['word'].str.lower()\n",
        "# test_df['word'] = pre_process(test_df,'word')\n",
        "\n",
        "display(train_df.describe(), val_df.describe(), test_df.describe(), titles = [f'Training set {train_df.shape}', f'Validation set {val_df.shape}', f'Test set {test_df.shape}'])"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "64e6f4e7-c84f-43b1-b20e-c977aa2a4569"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Training set (50748, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50748</td>\n",
              "      <td>50748</td>\n",
              "      <td>50748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>8442</td>\n",
              "      <td>46</td>\n",
              "      <td>1963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>,</td>\n",
              "      <td>NN</td>\n",
              "      <td>1854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2570</td>\n",
              "      <td>6270</td>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Validation set (33260, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>33260</td>\n",
              "      <td>33260</td>\n",
              "      <td>33260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>6068</td>\n",
              "      <td>45</td>\n",
              "      <td>1299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>,</td>\n",
              "      <td>NN</td>\n",
              "      <td>339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1528</td>\n",
              "      <td>4513</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Test set (16668, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16668</td>\n",
              "      <td>16668</td>\n",
              "      <td>16668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3648</td>\n",
              "      <td>41</td>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>,</td>\n",
              "      <td>NN</td>\n",
              "      <td>232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>787</td>\n",
              "      <td>2383</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "max_tags_list = max([len(tags_train),len(tags_val),len(tags_test)])\n",
        "\n",
        "# Training set tags list\n",
        "print(f'Train tags number: {len(tags_train)}')\n",
        "print(f'Train tags list: {tags_train}')\n",
        "\n",
        "exceeding_validation = [el for el in tags_train if el not in tags_val]\n",
        "if exceeding_validation != []:\n",
        "  print(f'\\tClasses in training set for which there are no samples in validation set: {exceeding_validation}')\n",
        "\n",
        "exceeding_test = [el for el in tags_train if el not in tags_test]\n",
        "\n",
        "if exceeding_test != []:\n",
        "  print(f'\\tClasses in training set for which there are no samples in test set: {exceeding_test}')\n",
        "\n",
        "\n",
        "# Validation set tags list\n",
        "print(f'\\nValidation tags number: {len(tags_val)}')\n",
        "print(f'Validation tags list: {tags_val}')\n",
        "\n",
        "exceeding_training = [el for el in tags_val if el not in tags_train]\n",
        "if exceeding_training != []:\n",
        "  print(f'\\tClasses in validation set for which there are no samples in training set: {exceeding_training}')\n",
        "\n",
        "exceeding_test = [el for el in tags_val if el not in tags_test]\n",
        "if exceeding_test != []:\n",
        "  print(f'\\tClasses in validation set for which there are no samples in test set: {exceeding_test}')\n",
        "\n",
        "# Validation set tags list\n",
        "print(f'\\nTest tags number: {len(tags_test)}')\n",
        "print(f'Test tags list: {tags_test}')\n",
        "\n",
        "exceeding_training = [el for el in tags_test if el not in tags_train]\n",
        "if exceeding_training != []:\n",
        "  print(f'\\tClasses in test set for which there are no samples in training set: {exceeding_training}')\n",
        "\n",
        "exceeding_val = [el for el in tags_test if el not in tags_val]\n",
        "if exceeding_val != []:\n",
        "  print(f'\\tClasses in test set set for which there are no samples in validation set: {exceeding_val}')\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tags_train, index=tags_train)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "f82770e0-4c93-4db4-83f2-bc3a535669e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 46\n",
            "Train tags list: ['#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "\tClasses in training set for which there are no samples in validation set: ['SYM']\n",
            "\tClasses in training set for which there are no samples in test set: ['#', 'FW', 'LS', 'SYM', 'UH']\n",
            "\n",
            "Validation tags number: 45\n",
            "Validation tags list: ['#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "\tClasses in validation set for which there are no samples in test set: ['#', 'FW', 'LS', 'UH']\n",
            "\n",
            "Test tags number: 41\n",
            "Test tags list: ['$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriving prepocessed data\n",
        "X_train_raw = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "X_val_raw = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "X_test_raw = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "y_train_raw = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "y_val_raw = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "y_test_raw = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "# Creating sets of words and tags\n",
        "#words = set([word.lower() for sentence in [*X_train_raw, *X_val_raw] for word in sentence])\n",
        "#tags = set([tag for sentence_tags in [*y_train_raw, *y_val_raw] for tag in sentence_tags])"
      ],
      "metadata": {
        "id": "jlBmUbmIlG0o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##-Vocabulary part-"
      ],
      "metadata": {
        "id": "U5kuIrRML6i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "7W8oM6e9LIMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "embedding_dim = 300\n",
        "embedding_dict = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, f'glove.6B.{embedding_dim}d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_dict[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "06bca4fa-3b41-477b-986b-bc4d68bfcf96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "i896YzW_LSxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_vocab(df,embeddings_index,embedding_dim):\n",
        "  oov_c = 0\n",
        "  for word in df.word:\n",
        "    if word not in embeddings_index:\n",
        "      oov_c += 1\n",
        "      random_embed = np.random.rand(embedding_dim)\n",
        "      embeddings_index[word] = random_embed\n",
        "  print(\"Added\",oov_c,\"OOV words + respective embeddings to the vocabulary.\")\n",
        "  return embeddings_index\n",
        "\n",
        "embedding_dict = update_vocab(train_df,embedding_dict,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGUNPlD0LYzI",
        "outputId": "7363cc17-3a9e-492b-a9b8-d16084600bef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 2777 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "2Y0leABCLdUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = update_vocab(val_df,embedding_dict,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-ArNTCLeKY",
        "outputId": "62ab86ba-cc68-4eee-a885-b704ad4a4e34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 951 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "iOF5z9auLf7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = update_vocab(test_df,embedding_dict,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_JWqKoDLjBD",
        "outputId": "ac5478eb-ae26-40cf-8481-ddaa78e6bc06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 455 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "index2word = OrderedDict()\n",
        "word2index = OrderedDict()\n",
        "\n",
        "# 0 is assigned for padding\n",
        "#word2index['-PAD-'] = 0  \n",
        "#index2word[0] = '-PAD-'  \n",
        "\n",
        "curr_idx = 0\n",
        "for key in embedding_dict.keys():\n",
        "  word2index[key] = curr_idx\n",
        "  index2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "#word_listing = list(index2word.values())\n",
        "vocab_length = len(word2index) \n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(index2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2index)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKuxUTPNLkhz",
        "outputId": "0daaa6c6-0f60-4672-bc82-2985a9f10d19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 404183\n",
            "[Debug] Word -> Index vocabulary size: 404183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2index = OrderedDict()\n",
        "index2tag = OrderedDict()\n",
        "\n",
        "# 0 is assigned for padding\n",
        "#tag2index['-PAD-'] = 0  \n",
        "#index2tag[0] = '-PAD-'\n",
        "\n",
        "curr_id = 0\n",
        "for tag in tags_train:\n",
        "  tag2index[tag] = curr_id\n",
        "  index2tag[curr_id] = tag\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Index -> Tag vocabulary size: {len(index2tag)}')\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2index)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aKNy1vALyWm",
        "outputId": "9ca396ef-c119-4d0a-d683-0d17e8468bad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Tag vocabulary size: 46\n",
            "[Debug] Tag -> Index vocabulary size: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###da qua continua il preprocessing"
      ],
      "metadata": {
        "id": "iKZGDveoMIUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenising words and tags by their indexes in vocabulary\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = [], [], [], [], [], []\n",
        "\n",
        "# Encode X\n",
        "for sentence in X_train_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "            sent_int.append(word2index[word])\n",
        "    X_train.append(sent_int)\n",
        "\n",
        "for sentence in X_val_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "            sent_int.append(word2index[word])\n",
        "    X_val.append(sent_int)\n",
        "\n",
        "for sentence in X_test_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "            sent_int.append(word2index[word])\n",
        "    X_test.append(sent_int)\n",
        "\n",
        "# Encode Y\n",
        "for sent_tags in y_train_raw:\n",
        "    y_train.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in y_val_raw:\n",
        "    y_val.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in y_test_raw:\n",
        "    y_test.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "print('-Not encoded')\n",
        "print('\\t',X_train_raw[0]) \n",
        "print('\\t',y_train_raw[0])\n",
        "print('-Encoded')\n",
        "print('\\t',X_train[0])\n",
        "print('\\t',y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlvH2AwynXAk",
        "outputId": "e958f36e-f2b5-437d-8191-d83b48fdbda9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-Not encoded\n",
            "\t ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "\t ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n",
            "-Encoded\n",
            "\t [400000, 400001, 1, 4978, 82, 167, 1, 43, 1429, 0, 534, 19, 7, 128564, 369, 400002, 1263, 2]\n",
            "\t [21, 21, 3, 10, 23, 15, 3, 19, 35, 11, 20, 14, 11, 15, 20, 21, 10, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(sentence) for sentence in X_train_raw]\n",
        "MAX_LENGTH = max(lengths) # maximum words in a sentence\n",
        "\n",
        "print(f'Length of longest sentence: {MAX_LENGTH}')\n",
        "print(f'Average length: {int(sum(lengths)/len(lengths))}')\n",
        "\n",
        "# Add padding to sentences\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=MAX_LENGTH, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "y_train = pad_sequences(y_train, maxlen=MAX_LENGTH, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=MAX_LENGTH, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "print('-Padded')\n",
        "print('\\tX:',X_train[0])\n",
        "print('\\n\\ty:',y_train[0])\n",
        "\n",
        "# Create a sample weight vector where the weights for padded samples, punctuation and symbols \n",
        "# are set to 0 and the weights for the other samples are set to 1\n",
        "ignore = [':', '#', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "# sample_weight = np.ones(X_train.shape)\n",
        "# for i in range(X_train.shape[0]):\n",
        "#     for j in range(X_train.shape[1]):\n",
        "#         if X_train[i][j] == 0 or y_train_raw[i][j] in ignore:\n",
        "#             sample_weight[i][j] = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRSBSBqosWyK",
        "outputId": "4b002cac-69a9-494a-8058-6a81cdf8a86f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of longest sentence: 271\n",
            "Average length: 25\n",
            "-Padded\n",
            "\tX: [400000 400001      1   4978     82    167      1     43   1429      0\n",
            "    534     19      7 128564    369 400002   1263      2      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0]\n",
            "\n",
            "\ty: [21 21  3 10 23 15  3 19 35 11 20 14 11 15 20 21 10  7  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = to_categorical(y_train, len(tag2index))\n",
        "y_val_one_hot = to_categorical(y_val, len(tag2index))\n",
        "y_test_one_hot = to_categorical(y_test, len(tag2index))"
      ],
      "metadata": {
        "id": "Tt3IPOjD1AER"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GloVe \n",
        "GloVe (Global Vectors for Word Representation) is a method for learning vector representations of words, called \"word embeddings,\" from a large corpus of text. Word embeddings are numerical representations of words that capture the semantic relationships between words in a continuous, low-dimensional space. They are commonly used as input to natural language processing models, such as language translation and language modeling.\n",
        "\n",
        "GloVe works by learning the co-occurrence statistics of words in a corpus, and using this information to learn word embeddings that capture the semantic relationships between words. The GloVe method produces word embeddings that are trained on a global corpus, as opposed to embeddings that are trained on a specific task or dataset.\n",
        "\n",
        "There are different versions of the GloVe word embeddings, including 50-dimensional, 100-dimensional, and 200-dimensional embeddings. The 50-dimensional version of GloVe embeddings may be better in some applications because they have a lower dimensionality, which can make them easier to work with and more computationally efficient.\n",
        "\n",
        "By using GloVe embeddings as the initial weights for a model, we can take advantage of these pre-trained word representations and fine-tune them for a specific task."
      ],
      "metadata": {
        "id": "1SqOgc6v1DfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_matrix = np.zeros((len(word2index), embedding_dim))\n",
        "# embedding_matrix = np.standard(0,1,(len(word2index), embedding_dim))\n",
        "for word, i in word2index.items():\n",
        "    embedding_vector = embedding_dict.get(word)\n",
        "    if i < len(word2index):\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "YcvBdLXW1QTY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model\n",
        "### 3.1 Baseline \n",
        "Bidirectional LSTM layers are able to process sequential data in both the forward and backward directions, which can allow the model to capture contextual information from both the past and the future. This can be particularly useful for natural language processing tasks, where the meaning of a word can depend on the context in which it is used.\n",
        "\n",
        "In the context of POS tagging, TimeDistributed can be used to apply a tag prediction layer to each word in a sentence. For example, you might have an RNN that processes a sequence of words in a sentence, and at each time step, the RNN outputs a hidden state. You could then apply a TimeDistributed dense layer to the hidden states, which would allow you to predict the POS tag for each word in the sentence.\n",
        "\n",
        "One advantage of using TimeDistributed for POS tagging is that it allows you to predict the POS tag for each word in the sentence simultaneously, rather than having to process the sentence one word at a time. This can be particularly useful when dealing with long sentences, as it can make the tagging process more efficient.\n",
        "\n",
        "Overall, using TimeDistributed for POS tagging can help you build more accurate and efficient models for natural language processing tasks that involve sequential data."
      ],
      "metadata": {
        "id": "AvF_XXvV3Qxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "\n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "\n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JDmW_pKQidcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ignore_class_accuracy(classes=[0]):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        "        \n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32')\n",
        "        for to_ignore in classes:\n",
        "          ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "          matches = matches * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "# Define the model\n",
        "baseline_model = Sequential(name='Baseline')\n",
        "\n",
        "# Add the Embedding layer\n",
        "baseline_model.add(Embedding(input_dim=vocab_length, output_dim=embedding_dim, \\\n",
        "                    weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=False))\n",
        "\n",
        "# Add the Bidirectional LSTM layer\n",
        "baseline_model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "baseline_model.add(TimeDistributed(Dense(units=len(tag2index), activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "baseline_model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy',\\\n",
        "                      ignore_class_accuracy([0,*[tag2index[tag] for tag in ignore]])])\n",
        "\n",
        "# Summary\n",
        "baseline_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuLsYIaYIBgn",
        "outputId": "07e9f763-4803-46d2-ceab-5b8528f8bfa4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 271, 300)          121254900 \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 271, 512)         1140736   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 271, 46)          23598     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,419,234\n",
            "Trainable params: 1,164,334\n",
            "Non-trainable params: 121,254,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_baseline = baseline_model.fit(X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot),\\\n",
        "                                      batch_size=128, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "ffd93531-43b7-4ef3-80cb-e38931de8a85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 90s 5s/step - loss: 0.7617 - accuracy: 0.8490 - ignore_accuracy: 0.0039 - val_loss: 0.3146 - val_accuracy: 0.9169 - val_ignore_accuracy: 0.0111\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 84s 5s/step - loss: 0.2885 - accuracy: 0.9243 - ignore_accuracy: 0.0201 - val_loss: 0.2657 - val_accuracy: 0.9286 - val_ignore_accuracy: 0.0222\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.2485 - accuracy: 0.9374 - ignore_accuracy: 0.0276 - val_loss: 0.2361 - val_accuracy: 0.9401 - val_ignore_accuracy: 0.0278\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 82s 5s/step - loss: 0.2206 - accuracy: 0.9448 - ignore_accuracy: 0.0308 - val_loss: 0.2098 - val_accuracy: 0.9488 - val_ignore_accuracy: 0.0328\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.1936 - accuracy: 0.9525 - ignore_accuracy: 0.0359 - val_loss: 0.1820 - val_accuracy: 0.9562 - val_ignore_accuracy: 0.0381\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 82s 5s/step - loss: 0.1661 - accuracy: 0.9588 - ignore_accuracy: 0.0407 - val_loss: 0.1558 - val_accuracy: 0.9609 - val_ignore_accuracy: 0.0422\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.1413 - accuracy: 0.9651 - ignore_accuracy: 0.0464 - val_loss: 0.1335 - val_accuracy: 0.9666 - val_ignore_accuracy: 0.0473\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.1209 - accuracy: 0.9706 - ignore_accuracy: 0.0504 - val_loss: 0.1160 - val_accuracy: 0.9719 - val_ignore_accuracy: 0.0519\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.1052 - accuracy: 0.9745 - ignore_accuracy: 0.0541 - val_loss: 0.1027 - val_accuracy: 0.9750 - val_ignore_accuracy: 0.0549\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 83s 5s/step - loss: 0.0932 - accuracy: 0.9776 - ignore_accuracy: 0.0564 - val_loss: 0.0928 - val_accuracy: 0.9774 - val_ignore_accuracy: 0.0574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = baseline_model.evaluate(X_test, y_test_one_hot, return_dict = True)\n",
        "\n",
        "# Obtain the predictions made by the model on the test set\n",
        "predictions = baseline_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "b9808ca8-8da0-4eba-d3f2-de8a623a0457"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 11s 525ms/step - loss: 0.0831 - accuracy: 0.9796 - ignore_accuracy: 0.0586\n",
            "21/21 [==============================] - 11s 494ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the class probabilities into class labels\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "true_pos = defaultdict(int)\n",
        "false_pos = defaultdict(int)\n",
        "false_neg = defaultdict(int)\n",
        "precision = defaultdict(float)\n",
        "recall = defaultdict(float)\n",
        "f1score = defaultdict(float)\n",
        "\n",
        "index2tag = {v: k for k, v in tag2index.items()}\n",
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    for idx_sentence in range(len(y_test)):\n",
        "      for idx_word in range(MAX_LENGTH):\n",
        "        if index2tag[y_test[idx_sentence][idx_word]] == tag:\n",
        "          # If the predicted tag matches ground truth we increase true positive count for that tag\n",
        "          if predicted_labels[idx_sentence][idx_word] == y_test[idx_sentence][idx_word]:\n",
        "            true_pos[index2tag[y_test[idx_sentence][idx_word]]] += 1\n",
        "          else:\n",
        "          # If the predicted tag does not match ground truth we increase false negative count for that tag\n",
        "          # and false positive count for the wrongly predicted tag\n",
        "            false_neg[index2tag[y_test[idx_sentence][idx_word]]] += 1\n",
        "            false_pos[index2tag[predicted_labels[idx_sentence][idx_word]]] += 1\n",
        "    # We then compute precision, recall and F1 scores\n",
        "    if true_pos[tag] != 0:\n",
        "      precision[tag] = true_pos[tag] / (true_pos[tag] + false_pos[tag])\n",
        "      recall[tag] = true_pos[tag] / (true_pos[tag] + false_neg[tag])\n",
        "      f1score[tag] = 2 * ((precision[tag] * recall[tag]) / (precision[tag] + recall[tag])) \n",
        "    else:\n",
        "      print(tag)\n",
        "      print(true_pos[tag])\n",
        "      print(false_pos[tag])\n",
        "      print(false_neg[tag])\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP2TlmEVynlM",
        "outputId": "f3760649-76f1-4e0e-840f-740ab32625d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EX\n",
            "0\n",
            "0\n",
            "5\n",
            "\n",
            "JJR\n",
            "0\n",
            "0\n",
            "59\n",
            "\n",
            "JJS\n",
            "0\n",
            "0\n",
            "31\n",
            "\n",
            "NNPS\n",
            "0\n",
            "0\n",
            "44\n",
            "\n",
            "PDT\n",
            "0\n",
            "0\n",
            "4\n",
            "\n",
            "RBR\n",
            "0\n",
            "0\n",
            "15\n",
            "\n",
            "RBS\n",
            "0\n",
            "0\n",
            "3\n",
            "\n",
            "RP\n",
            "0\n",
            "0\n",
            "33\n",
            "\n",
            "WDT\n",
            "0\n",
            "0\n",
            "84\n",
            "\n",
            "WP\n",
            "0\n",
            "0\n",
            "20\n",
            "\n",
            "WP$\n",
            "0\n",
            "0\n",
            "4\n",
            "\n",
            "WRB\n",
            "0\n",
            "0\n",
            "24\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FORSE DOBBIAMO CONSIDERARE TUTTE LE CLASSI; ANCHE QUELLE PRESENTI SOLO NEL TRAINING TEST, DANDO COME f1 SCORE 0\n",
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    print(f'Tag: {tag}')\n",
        "    print(f'\\tPrecision: {precision[tag]}')\n",
        "    print(f'\\tRecall: {recall[tag]}')\n",
        "    print(f'\\tF1-score: {f1score[tag]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1AkaNs61oC6",
        "outputId": "d617b037-40d6-4e4e-8195-bb1f600ff8df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: CC\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.8715846994535519\n",
            "\tF1-score: 0.9313868613138686\n",
            "Tag: CD\n",
            "\tPrecision: 0.9984472049689441\n",
            "\tRecall: 0.7494172494172494\n",
            "\tF1-score: 0.856191744340879\n",
            "Tag: DT\n",
            "\tPrecision: 0.9896579156722355\n",
            "\tRecall: 0.9318352059925094\n",
            "\tF1-score: 0.9598765432098765\n",
            "Tag: EX\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: IN\n",
            "\tPrecision: 0.9666221628838452\n",
            "\tRecall: 0.8883435582822086\n",
            "\tF1-score: 0.9258312020460358\n",
            "Tag: JJ\n",
            "\tPrecision: 0.8761682242990654\n",
            "\tRecall: 0.4084967320261438\n",
            "\tF1-score: 0.5572065378900446\n",
            "Tag: JJR\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: JJS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: MD\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.874251497005988\n",
            "\tF1-score: 0.9329073482428115\n",
            "Tag: NN\n",
            "\tPrecision: 0.8315473639091299\n",
            "\tRecall: 0.8140998741082669\n",
            "\tF1-score: 0.8227311280746396\n",
            "Tag: NNP\n",
            "\tPrecision: 0.8437686344663089\n",
            "\tRecall: 0.9408244680851063\n",
            "\tF1-score: 0.8896573404589752\n",
            "Tag: NNPS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: NNS\n",
            "\tPrecision: 0.7290167865707434\n",
            "\tRecall: 0.6461211477151966\n",
            "\tF1-score: 0.6850704225352112\n",
            "Tag: PDT\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: POS\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9276315789473685\n",
            "\tF1-score: 0.962457337883959\n",
            "Tag: PRP\n",
            "\tPrecision: 0.9137931034482759\n",
            "\tRecall: 0.5520833333333334\n",
            "\tF1-score: 0.6883116883116883\n",
            "Tag: PRP$\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.6868686868686869\n",
            "\tF1-score: 0.8143712574850299\n",
            "Tag: RB\n",
            "\tPrecision: 0.5803108808290155\n",
            "\tRecall: 0.29396325459317585\n",
            "\tF1-score: 0.39024390243902435\n",
            "Tag: RBR\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: RBS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: RP\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: TO\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 1.0\n",
            "\tF1-score: 1.0\n",
            "Tag: VB\n",
            "\tPrecision: 0.7785087719298246\n",
            "\tRecall: 0.8808933002481389\n",
            "\tF1-score: 0.8265424912689173\n",
            "Tag: VBD\n",
            "\tPrecision: 0.8237476808905381\n",
            "\tRecall: 0.7003154574132492\n",
            "\tF1-score: 0.7570332480818415\n",
            "Tag: VBG\n",
            "\tPrecision: 0.9\n",
            "\tRecall: 0.04072398190045249\n",
            "\tF1-score: 0.07792207792207792\n",
            "Tag: VBN\n",
            "\tPrecision: 0.7211155378486056\n",
            "\tRecall: 0.49453551912568305\n",
            "\tF1-score: 0.586709886547812\n",
            "Tag: VBP\n",
            "\tPrecision: 0.8505747126436781\n",
            "\tRecall: 0.5522388059701493\n",
            "\tF1-score: 0.669683257918552\n",
            "Tag: VBZ\n",
            "\tPrecision: 0.7905982905982906\n",
            "\tRecall: 0.6607142857142857\n",
            "\tF1-score: 0.7198443579766536\n",
            "Tag: WDT\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: WP\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: WP$\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: WRB\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "provola = [el[1] for el in f1score.items()]\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for p in provola:\n",
        "  sum += p\n",
        "\n",
        "print(f'MACRO F1-score: {sum / len(f1score.items())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r14KwxMp6pxZ",
        "outputId": "90cab9bf-4e60-42fc-e242-5290acc3a909"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MACRO F1-score: 0.4704368323108718\n"
          ]
        }
      ]
    }
  ]
}