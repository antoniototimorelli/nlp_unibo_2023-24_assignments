{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 11/01/2022 (dd/mm/yyyy)\n",
        "\n",
        "If you deliver it by 11/12/2021 your assignment will be graded by 11/01/2022.\n",
        "\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Execution\n",
        "https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn\n",
        "\n",
        "A bunch of libraries and functions that will be used throughout the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, GRU\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Bk3FPDTGw_4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b113346-8baa-4f32-cd7f-f8240cf09400"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "  vocab_size = len(word_index)+1\n",
        "  embedding_matrix = np.zeros((vocab_size,embedding_dim))\n",
        "\n",
        "  with open(filepath, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word in word_index:\n",
        "        idx = word_index[word]\n",
        "        embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "Kx5qPakFMJFw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Corpus\n",
        "### 1.1 Pre-processing\n",
        "\n",
        "From the original tags list we removed all the symbols and english punctuation plus:\n",
        "- FW, Foreign Word, because there are no examples in the test set;\n",
        "- UH, Interjection, because there are no examples in the test set;\n",
        "- LS, List Item Marker, because there are no examples in the test set (and because it denotes symbols as well);"
      ],
      "metadata": {
        "id": "pJVKURDWIGMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])"
      ],
      "metadata": {
        "id": "8NhdgM6UVi-T"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-', 'FW', 'UH', 'LS']\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "for sentence in train_corpus:\n",
        "  X_sentence = []\n",
        "  y_sentence = []\n",
        "  for entity in sentence:\n",
        "    if entity[1] not in remove:         \n",
        "      X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "      y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "  X_train.append(X_sentence)\n",
        "  y_train.append(y_sentence)\n",
        "\n",
        "\n",
        "X_val = []\n",
        "y_val = []\n",
        "for sentence in val_corpus:\n",
        "  X_sentence = []\n",
        "  y_sentence = []\n",
        "  for entity in sentence:\n",
        "    if entity[1] not in remove:         \n",
        "      X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "      y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "  X_val.append(X_sentence)\n",
        "  y_val.append(y_sentence)\n",
        "\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "for sentence in test_corpus:\n",
        "  X_sentence = []\n",
        "  y_sentence = []\n",
        "  for entity in sentence:\n",
        "    if entity[1] not in remove:         \n",
        "      X_sentence.append(entity[0])  # entity[0] contains the word\n",
        "      y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
        "  X_test.append(X_sentence)\n",
        "  y_test.append(y_sentence)"
      ],
      "metadata": {
        "id": "iETje_H_RvIC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set([word.lower() for sentence in X_train for word in sentence]))\n",
        "num_classes = len(set([word.lower() for sentence in y_train for word in sentence]))\n",
        "\n",
        "print(\"Total number of tagged sentences: {}\".format(len(X_train)))\n",
        "print(\"Vocabulary size: {}\".format(vocab_size))\n",
        "print(\"Total number of tags: {}\".format(num_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKqyJEjeEt0e",
        "outputId": "550afa07-7ce1-4528-9451-c01fc7e29aa3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tagged sentences: 1963\n",
            "Vocabulary size: 7381\n",
            "Total number of tags: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "num_words = 9000\n",
        "word_tokenizer.word_index = {e:i for e,i in word_tokenizer.word_index.items() if i <= num_words}\n",
        "word_tokenizer.word_index[word_tokenizer.oov_token] = num_words + 1\n",
        "\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_val = word_tokenizer.texts_to_sequences(X_val)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)\n",
        "vocab_size = len(word_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "DYlL3zP1aErC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "y_train = tag_tokenizer.texts_to_sequences(y_train)\n",
        "y_val = tag_tokenizer.texts_to_sequences(y_val)\n",
        "y_test = tag_tokenizer.texts_to_sequences(y_test)"
      ],
      "metadata": {
        "id": "mL61KV8GFioU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check length of longest sentence\n",
        "lengths = [len(seq) for seq in X_train]\n",
        "print(\"Length of longest sentence: {}\".format(max(lengths)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKBX32OvGyCH",
        "outputId": "60f8eb43-3ae2-43e1-f141-aa3975928949"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of longest sentence: 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "X_train = pad_sequences(X_train,padding='post',maxlen=max_len)\n",
        "X_val = pad_sequences(X_val,padding='post',maxlen=max_len)\n",
        "X_test = pad_sequences(X_test,padding='post',maxlen=max_len)\n",
        "\n",
        "y_train = pad_sequences(y_train,padding='post',maxlen=max_len)\n",
        "y_val = pad_sequences(y_val,padding='post',maxlen=max_len)\n",
        "y_test = pad_sequences(y_test,padding='post',maxlen=max_len)"
      ],
      "metadata": {
        "id": "wRuwom1nZhlp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "J0uW8doKHf_i"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GozDrRpbCDw",
        "outputId": "dfcc4a0f-42f2-4faa-fa41-697e303973d1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1963, 100)\n",
            "(1963, 100, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GloVe \n",
        "GloVe (Global Vectors for Word Representation) is a method for learning vector representations of words, called \"word embeddings,\" from a large corpus of text. Word embeddings are numerical representations of words that capture the semantic relationships between words in a continuous, low-dimensional space. They are commonly used as input to natural language processing models, such as language translation and language modeling.\n",
        "\n",
        "GloVe works by learning the co-occurrence statistics of words in a corpus, and using this information to learn word embeddings that capture the semantic relationships between words. The GloVe method produces word embeddings that are trained on a global corpus, as opposed to embeddings that are trained on a specific task or dataset.\n",
        "\n",
        "There are different versions of the GloVe word embeddings, including 50-dimensional, 100-dimensional, and 200-dimensional embeddings. The 50-dimensional version of GloVe embeddings may be better in some applications because they have a lower dimensionality, which can make them easier to work with and more computationally efficient."
      ],
      "metadata": {
        "id": "WjIVKo-zH0AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "whkzW_UI9Amy",
        "outputId": "ee284844-3b48-47ec-8f1c-b7132fc78904"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (1064960 of 862182613) |            | Elapsed Time: 0:00:00 ETA:   0:10:47"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c1b976fc3781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Download the GloVe embeddings file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://nlp.stanford.edu/data/glove.6B.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.6B.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Extract the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe embeddings into a dictionary\n",
        "embedding_dict = {}\n",
        "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_dict[word] = coefs\n",
        "\n",
        "# Print the number of words in the embeddings dictionary\n",
        "print(f'Found {len(embedding_dict)} word vectors.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHAd-w3p9V-u",
        "outputId": "3aefd879-bb42-49c0-858d-19f7981d68c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embedding_dict.keys(), key=lambda word: np.linalg.norm(embedding_dict[word]- embedding))[:5]\n",
        "\n",
        "find_closest_embeddings(embedding_dict['iphone'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icops2y7-fNz",
        "outputId": "6ee4aa94-4974-4257-f4c9-a9014e16a2ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iphone', 'ipad', 'smartphone', 'ipod', 'android']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(f'glove.6B.{embedding_dim}d.txt', word_tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "uSgIkyfvaaHi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model\n",
        "### 3.1 Baseline (MACRO f1 0.84)\n",
        "Bidirectional LSTM layers are able to process sequential data in both the forward and backward directions, which can allow the model to capture contextual information from both the past and the future. This can be particularly useful for natural language processing tasks, where the meaning of a word can depend on the context in which it is used."
      ],
      "metadata": {
        "id": "hcBHOgcP3OW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TimeDistributed\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential(name='Baseline')\n",
        "\n",
        "# Add the Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \\\n",
        "                    weights = [embedding_matrix], input_length = max_len, trainable=True))\n",
        "\n",
        "# Add the Bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "# model.add(Dense(units=32, activation='softmax'))\n",
        "model.add(TimeDistributed(Dense(33, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RTDDziCuKz0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56bf188f-ea81-4e0e-9d83-9bc11c783f12"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 50)           369150    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 100, 256)         183296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 100, 33)          8481      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 560,927\n",
            "Trainable params: 560,927\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(X_train, y_train, epochs=10, verbose = True, validation_data=(X_val,y_val), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlB64WAzDr_p",
        "outputId": "ecfaa993-2349-47ff-a400-1913989f7c7d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "62/62 [==============================] - 28s 373ms/step - loss: 1.0425 - accuracy: 0.8221 - val_loss: 0.6294 - val_accuracy: 0.8298\n",
            "Epoch 2/10\n",
            "62/62 [==============================] - 19s 314ms/step - loss: 0.4271 - accuracy: 0.8865 - val_loss: 0.6398 - val_accuracy: 0.8433\n",
            "Epoch 3/10\n",
            "62/62 [==============================] - 21s 333ms/step - loss: 0.2750 - accuracy: 0.9295 - val_loss: 0.7423 - val_accuracy: 0.8501\n",
            "Epoch 4/10\n",
            "62/62 [==============================] - 21s 345ms/step - loss: 0.1813 - accuracy: 0.9543 - val_loss: 0.8507 - val_accuracy: 0.8541\n",
            "Epoch 5/10\n",
            "62/62 [==============================] - 22s 352ms/step - loss: 0.1314 - accuracy: 0.9663 - val_loss: 0.9369 - val_accuracy: 0.8559\n",
            "Epoch 6/10\n",
            "62/62 [==============================] - 20s 329ms/step - loss: 0.1020 - accuracy: 0.9733 - val_loss: 1.0089 - val_accuracy: 0.8571\n",
            "Epoch 7/10\n",
            "62/62 [==============================] - 16s 265ms/step - loss: 0.0830 - accuracy: 0.9781 - val_loss: 1.0703 - val_accuracy: 0.8580\n",
            "Epoch 8/10\n",
            "62/62 [==============================] - 16s 266ms/step - loss: 0.0691 - accuracy: 0.9820 - val_loss: 1.1078 - val_accuracy: 0.8586\n",
            "Epoch 9/10\n",
            "62/62 [==============================] - 17s 272ms/step - loss: 0.0587 - accuracy: 0.9847 - val_loss: 1.1545 - val_accuracy: 0.8589\n",
            "Epoch 10/10\n",
            "62/62 [==============================] - 16s 264ms/step - loss: 0.0503 - accuracy: 0.9869 - val_loss: 1.1865 - val_accuracy: 0.8592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko3L8znnFWPx",
        "outputId": "1b3d6a81-3d08-4d93-bfa6-ca46d912bf91"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 1s 65ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.multiclass import type_of_target\n",
        "\n",
        "print(y_pred.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "y_test = y_test[:, -1].astype(int)\n",
        "y_pred = y_pred[:, -1].astype(int)\n",
        "\n",
        "print(type_of_target(y_pred))\n",
        "print(type_of_target(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv7SQA7xf8p8",
        "outputId": "1b79fa60-80e2-4a72-81d0-69d062a5d71c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(652, 100, 33)\n",
            "(652, 100, 33)\n",
            "multilabel-indicator\n",
            "multilabel-indicator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# th = 0.1\n",
        "# y_pred[:][y_pred >= th] = 1 \n",
        "# y_pred[:][y_pred  < th] = 0\n",
        "\n",
        "tags_train = [\n",
        "    'CC',\n",
        "    'CD',\n",
        "    'DT',\n",
        "    'EX',\n",
        "    'IN',\n",
        "    'JJ',\n",
        "    'JJR',\n",
        "    'JJS',\n",
        "    'MD',\n",
        "    'NN',\n",
        "    'NNP',\n",
        "    'NNPS',\n",
        "    'NNS',\n",
        "    'PDT',\n",
        "    'POS',\n",
        "    'PRP',\n",
        "    'PRP$',\n",
        "    'RB',\n",
        "    'RBR',\n",
        "    'RBS',\n",
        "    'RP',\n",
        "    'TO',\n",
        "    'VB',\n",
        "    'VBD',\n",
        "    'VBG',\n",
        "    'VBN',\n",
        "    'VBP',\n",
        "    'VBZ',\n",
        "    'WDT',\n",
        "    'WP',\n",
        "    'WP$'\n",
        "]\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names = tags_train, zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRAL4g-wOHjo",
        "outputId": "90e9274e-639c-4dba-f33e-8e9f9641fd24"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       1.00      0.00      0.00       652\n",
            "          CD       1.00      1.00      1.00         0\n",
            "          DT       1.00      1.00      1.00         0\n",
            "          EX       1.00      1.00      1.00         0\n",
            "          IN       1.00      1.00      1.00         0\n",
            "          JJ       1.00      1.00      1.00         0\n",
            "         JJR       1.00      1.00      1.00         0\n",
            "         JJS       1.00      1.00      1.00         0\n",
            "          MD       1.00      1.00      1.00         0\n",
            "          NN       1.00      1.00      1.00         0\n",
            "         NNP       1.00      1.00      1.00         0\n",
            "        NNPS       1.00      1.00      1.00         0\n",
            "         NNS       1.00      1.00      1.00         0\n",
            "         PDT       1.00      1.00      1.00         0\n",
            "         POS       1.00      1.00      1.00         0\n",
            "         PRP       1.00      1.00      1.00         0\n",
            "        PRP$       1.00      1.00      1.00         0\n",
            "          RB       1.00      1.00      1.00         0\n",
            "         RBR       1.00      1.00      1.00         0\n",
            "         RBS       1.00      1.00      1.00         0\n",
            "          RP       1.00      1.00      1.00         0\n",
            "          TO       1.00      1.00      1.00         0\n",
            "          VB       1.00      1.00      1.00         0\n",
            "         VBD       1.00      1.00      1.00         0\n",
            "         VBG       1.00      1.00      1.00         0\n",
            "         VBN       1.00      1.00      1.00         0\n",
            "         VBP       1.00      1.00      1.00         0\n",
            "         VBZ       1.00      1.00      1.00         0\n",
            "         WDT       1.00      1.00      1.00         0\n",
            "          WP       1.00      1.00      1.00         0\n",
            "         WP$       1.00      1.00      1.00         0\n",
            "       prova       1.00      1.00      1.00         0\n",
            "     provino       1.00      1.00      1.00         0\n",
            "\n",
            "   micro avg       1.00      0.00      0.00       652\n",
            "   macro avg       1.00      0.97      0.97       652\n",
            "weighted avg       1.00      0.00      0.00       652\n",
            " samples avg       1.00      0.00      0.00       652\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 GRU \n",
        "Unica che non funzia dio po"
      ],
      "metadata": {
        "id": "MZ88BEh9qBkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TimeDistributed\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential(name='GRU')\n",
        "\n",
        "# Add the Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \\\n",
        "                    weights = [embedding_matrix], input_length = max_len, trainable=True))\n",
        "\n",
        "# Add the GRU layer\n",
        "model.add(GRU(units=32, return_sequences=True))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "# model.add(Dense(units=len(tags_train), activation='softmax'))\n",
        "model.add(TimeDistributed(Dense(len(tags_train), activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcb3355-97b7-4855-aba3-17a7a5a3d985",
        "id": "IazFXPFgyMf3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GRU\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 50)           369150    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 100, 32)           8064      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 100, 33)          1089      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 378,303\n",
            "Trainable params: 378,303\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(X_train, y_train, epochs=10, verbose = True, validation_data=(X_val,y_val), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1863076e-3662-457d-f3d8-017132daec73",
        "id": "Y5JzsycPyMf5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "62/62 [==============================] - 8s 72ms/step - loss: 2.1826 - accuracy: 0.7963 - val_loss: 0.7333 - val_accuracy: 0.8160\n",
            "Epoch 2/10\n",
            "62/62 [==============================] - 4s 58ms/step - loss: 0.6159 - accuracy: 0.8517 - val_loss: 0.6656 - val_accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "62/62 [==============================] - 4s 59ms/step - loss: 0.5013 - accuracy: 0.8740 - val_loss: 0.6600 - val_accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "62/62 [==============================] - 5s 74ms/step - loss: 0.4105 - accuracy: 0.8957 - val_loss: 0.6735 - val_accuracy: 0.8415\n",
            "Epoch 5/10\n",
            "62/62 [==============================] - 4s 64ms/step - loss: 0.3373 - accuracy: 0.9161 - val_loss: 0.6986 - val_accuracy: 0.8465\n",
            "Epoch 6/10\n",
            "62/62 [==============================] - 4s 65ms/step - loss: 0.2790 - accuracy: 0.9345 - val_loss: 0.7290 - val_accuracy: 0.8491\n",
            "Epoch 7/10\n",
            "62/62 [==============================] - 4s 61ms/step - loss: 0.2327 - accuracy: 0.9461 - val_loss: 0.7616 - val_accuracy: 0.8514\n",
            "Epoch 8/10\n",
            "62/62 [==============================] - 4s 66ms/step - loss: 0.1955 - accuracy: 0.9558 - val_loss: 0.7949 - val_accuracy: 0.8530\n",
            "Epoch 9/10\n",
            "62/62 [==============================] - 5s 89ms/step - loss: 0.1657 - accuracy: 0.9631 - val_loss: 0.8286 - val_accuracy: 0.8542\n",
            "Epoch 10/10\n",
            "62/62 [==============================] - 4s 64ms/step - loss: 0.1418 - accuracy: 0.9684 - val_loss: 0.8615 - val_accuracy: 0.8552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bda3431-8ead-44e6-9a5b-72f764861615",
        "id": "hPIRHnThyMf6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "428/428 [==============================] - 6s 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th = 0.1\n",
        "y_pred[y_pred >= th] = 1 \n",
        "y_pred[y_pred  < th] = 0\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names = tags_train, zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f797b30e-342c-4002-a885-594fee4408f2",
        "id": "o47IL2wJyMf8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       1.00      0.00      0.00       366\n",
            "          CD       1.00      0.00      0.00       858\n",
            "          DT       0.10      1.00      0.18      1335\n",
            "          EX       1.00      0.00      0.00         5\n",
            "          IN       0.12      1.00      0.21      1630\n",
            "          JJ       1.00      0.00      0.00       918\n",
            "         JJR       1.00      0.00      0.00        59\n",
            "         JJS       1.00      0.00      0.00        31\n",
            "          MD       1.00      0.00      0.00       167\n",
            "          NN       0.17      1.00      0.30      2383\n",
            "         NNP       0.11      1.00      0.20      1504\n",
            "        NNPS       1.00      0.00      0.00        44\n",
            "         NNS       1.00      0.00      0.00       941\n",
            "         PDT       1.00      0.00      0.00         4\n",
            "         POS       1.00      0.00      0.00       152\n",
            "         PRP       1.00      0.00      0.00       192\n",
            "        PRP$       1.00      0.00      0.00        99\n",
            "          RB       1.00      0.00      0.00       381\n",
            "         RBR       1.00      0.00      0.00        15\n",
            "         RBS       1.00      0.00      0.00         3\n",
            "          RP       1.00      0.00      0.00        33\n",
            "          TO       1.00      0.00      0.00       386\n",
            "          VB       1.00      0.00      0.00       403\n",
            "         VBD       1.00      0.00      0.00       634\n",
            "         VBG       1.00      0.00      0.00       221\n",
            "         VBN       1.00      0.00      0.00       366\n",
            "         VBP       1.00      0.00      0.00       134\n",
            "         VBZ       1.00      0.00      0.00       280\n",
            "         WDT       1.00      0.00      0.00        84\n",
            "          WP       1.00      0.00      0.00        20\n",
            "         WP$       1.00      0.00      0.00         4\n",
            "         WRB       1.00      0.00      0.00        24\n",
            "\n",
            "   micro avg       0.13      0.50      0.20     13676\n",
            "   macro avg       0.89      0.12      0.03     13676\n",
            "weighted avg       0.57      0.50      0.12     13676\n",
            " samples avg       0.13      0.50      0.20     13676\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Additional LSTM layer (MACRO f1 0.82) \n",
        "Using two bidirectional LSTM layers can allow the model to learn more complex patterns in the data and make more accurate predictions. \n",
        "However, they can increase the computational complexity of our model, which may require more computational resources to train.\n",
        "\n",
        "Indeed, here the train was slower and the results similar to the baseline architecture."
      ],
      "metadata": {
        "id": "V0DshbMkqj2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Sequential(name='Additional_LSTM')\n",
        "\n",
        "# Add the Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \\\n",
        "                    weights = [embedding_matrix], input_length = max_len, trainable=True))\n",
        "\n",
        "# Add the Bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "\n",
        "# Add another LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "model.add(Dense(units=len(tags_train), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8115a6c7-491f-48b3-b959-5c5704c107ae",
        "id": "bHg9dRXf018R"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Additional_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 50, 50)            158450    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 50, 128)          58880     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,274\n",
            "Trainable params: 320,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(X_train, y_train, epochs=10, verbose = True, validation_data=(X_val,y_val), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1f1eda-292e-4c37-c5c4-ef8db0a0d905",
        "id": "cvMOu5xe018T"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "428/428 [==============================] - 68s 142ms/step - loss: 1.5208 - accuracy: 0.5648 - val_loss: 0.8600 - val_accuracy: 0.7560\n",
            "Epoch 2/10\n",
            "428/428 [==============================] - 53s 124ms/step - loss: 0.7112 - accuracy: 0.7964 - val_loss: 0.5132 - val_accuracy: 0.8484\n",
            "Epoch 3/10\n",
            "428/428 [==============================] - 56s 131ms/step - loss: 0.4941 - accuracy: 0.8539 - val_loss: 0.3821 - val_accuracy: 0.8882\n",
            "Epoch 4/10\n",
            "428/428 [==============================] - 50s 118ms/step - loss: 0.3723 - accuracy: 0.8909 - val_loss: 0.2882 - val_accuracy: 0.9165\n",
            "Epoch 5/10\n",
            "428/428 [==============================] - 50s 118ms/step - loss: 0.3013 - accuracy: 0.9108 - val_loss: 0.2428 - val_accuracy: 0.9283\n",
            "Epoch 6/10\n",
            "428/428 [==============================] - 52s 122ms/step - loss: 0.2544 - accuracy: 0.9221 - val_loss: 0.2105 - val_accuracy: 0.9355\n",
            "Epoch 7/10\n",
            "428/428 [==============================] - 62s 145ms/step - loss: 0.2238 - accuracy: 0.9291 - val_loss: 0.1886 - val_accuracy: 0.9400\n",
            "Epoch 8/10\n",
            "428/428 [==============================] - 58s 135ms/step - loss: 0.1994 - accuracy: 0.9338 - val_loss: 0.1835 - val_accuracy: 0.9395\n",
            "Epoch 9/10\n",
            "428/428 [==============================] - 50s 118ms/step - loss: 0.1924 - accuracy: 0.9335 - val_loss: 0.1812 - val_accuracy: 0.9414\n",
            "Epoch 10/10\n",
            "428/428 [==============================] - 54s 126ms/step - loss: 0.1717 - accuracy: 0.9411 - val_loss: 0.1823 - val_accuracy: 0.9241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91df81ec-4cf0-4c9a-8ac1-e6daa0a49454",
        "id": "uShCzDvl018U"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "428/428 [==============================] - 12s 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th = 0.1\n",
        "y_pred[y_pred >= th] = 1 \n",
        "y_pred[y_pred  < th] = 0\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names = tags_train, zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528043a8-6be0-44ad-aacd-2eea833793a5",
        "id": "ogdkHUtd018V"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.99      0.96      0.98       366\n",
            "          CD       0.98      1.00      0.99       858\n",
            "          DT       0.91      1.00      0.95      1335\n",
            "          EX       0.71      1.00      0.83         5\n",
            "          IN       0.76      1.00      0.86      1630\n",
            "          JJ       0.85      0.99      0.91       918\n",
            "         JJR       0.77      1.00      0.87        59\n",
            "         JJS       0.84      1.00      0.91        31\n",
            "          MD       0.97      1.00      0.98       167\n",
            "          NN       0.86      0.99      0.92      2383\n",
            "         NNP       0.81      0.97      0.88      1504\n",
            "        NNPS       0.71      0.89      0.79        44\n",
            "         NNS       0.95      0.99      0.97       941\n",
            "         PDT       0.22      0.50      0.31         4\n",
            "         POS       0.93      0.95      0.94       152\n",
            "         PRP       0.99      1.00      0.99       192\n",
            "        PRP$       0.99      1.00      0.99        99\n",
            "          RB       0.86      0.94      0.90       381\n",
            "         RBR       0.25      1.00      0.41        15\n",
            "         RBS       1.00      0.00      0.00         3\n",
            "          RP       0.24      0.88      0.37        33\n",
            "          TO       1.00      1.00      1.00       386\n",
            "          VB       0.65      0.97      0.78       403\n",
            "         VBD       0.87      0.98      0.92       634\n",
            "         VBG       0.73      0.98      0.83       221\n",
            "         VBN       0.65      0.98      0.78       366\n",
            "         VBP       0.73      0.86      0.79       134\n",
            "         VBZ       0.94      0.97      0.96       280\n",
            "         WDT       0.46      1.00      0.63        84\n",
            "          WP       1.00      1.00      1.00        20\n",
            "         WP$       0.57      1.00      0.73         4\n",
            "         WRB       1.00      1.00      1.00        24\n",
            "\n",
            "   micro avg       0.84      0.99      0.91     13676\n",
            "   macro avg       0.79      0.93      0.82     13676\n",
            "weighted avg       0.85      0.99      0.91     13676\n",
            " samples avg       0.91      0.99      0.93     13676\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Additional dense layer (MACRO f1 0.85)\n",
        "\n",
        "Using two dense layers, one with a non-linear activation function and one with a softmax activation function, is a common pattern in neural network architectures for classification tasks.\n",
        "\n",
        "The purpose of the non-linear dense layer is to introduce non-linearity into the model, which can allow the model to learn more complex patterns in the data. Common choices for the activation function in this layer include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
        "\n",
        "The purpose of the softmax dense layer is to produce a probability distribution over the possible classes. The softmax activation function transforms the output of the preceding layer into a probability distribution, where the sum of the probabilities is equal to 1. This is useful for classification tasks, where you want to predict the probability that an input belongs to each of the possible classes. Using two dense layers in this way can allow the model to learn more complex patterns in the data and make more accurate predictions."
      ],
      "metadata": {
        "id": "kpmMUtH4qnsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Sequential(name='Baseline')\n",
        "\n",
        "# Add the Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \\\n",
        "                    weights = [embedding_matrix], input_length = max_len, trainable=True))\n",
        "\n",
        "# Add the Bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=128)))\n",
        "\n",
        "# Add another Dense layer\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "model.add(Dense(units=len(tags_train), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78450581-11c1-4876-d017-02d7d3c741c3",
        "id": "E7o-z-q502ea"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 50, 50)            158450    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 256)              183296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                8224      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 415,762\n",
            "Trainable params: 415,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(X_train, y_train, epochs=10, verbose = True, validation_data=(X_val,y_val), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d791c96-5220-47c6-b791-d4e68a45228c",
        "id": "97QAgS-D02ec"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "428/428 [==============================] - 63s 138ms/step - loss: 1.2939 - accuracy: 0.6378 - val_loss: 0.6953 - val_accuracy: 0.8005\n",
            "Epoch 2/10\n",
            "428/428 [==============================] - 58s 136ms/step - loss: 0.5945 - accuracy: 0.8265 - val_loss: 0.4121 - val_accuracy: 0.8823\n",
            "Epoch 3/10\n",
            "428/428 [==============================] - 58s 136ms/step - loss: 0.4019 - accuracy: 0.8820 - val_loss: 0.3018 - val_accuracy: 0.9100\n",
            "Epoch 4/10\n",
            "428/428 [==============================] - 60s 139ms/step - loss: 0.3035 - accuracy: 0.9081 - val_loss: 0.2439 - val_accuracy: 0.9258\n",
            "Epoch 5/10\n",
            "428/428 [==============================] - 59s 139ms/step - loss: 0.2442 - accuracy: 0.9236 - val_loss: 0.1910 - val_accuracy: 0.9387\n",
            "Epoch 6/10\n",
            "428/428 [==============================] - 67s 157ms/step - loss: 0.2069 - accuracy: 0.9295 - val_loss: 0.1682 - val_accuracy: 0.9443\n",
            "Epoch 7/10\n",
            "428/428 [==============================] - 66s 153ms/step - loss: 0.1818 - accuracy: 0.9359 - val_loss: 0.1488 - val_accuracy: 0.9471\n",
            "Epoch 8/10\n",
            "428/428 [==============================] - 63s 146ms/step - loss: 0.1676 - accuracy: 0.9392 - val_loss: 0.1442 - val_accuracy: 0.9497\n",
            "Epoch 9/10\n",
            "428/428 [==============================] - 65s 152ms/step - loss: 0.1583 - accuracy: 0.9411 - val_loss: 0.1413 - val_accuracy: 0.9467\n",
            "Epoch 10/10\n",
            "428/428 [==============================] - 59s 138ms/step - loss: 0.1517 - accuracy: 0.9427 - val_loss: 0.1321 - val_accuracy: 0.9493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75db27e-2711-4da0-cc65-11d6440dcbd1",
        "id": "GgLsDktF02ec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "428/428 [==============================] - 16s 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th = 0.1\n",
        "y_pred[y_pred >= th] = 1 \n",
        "y_pred[y_pred  < th] = 0\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names = tags_train, zero_division=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ae63f3-83f7-4913-dcb2-b8a87993c927",
        "id": "TExtgi5i02ee"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CC       0.81      1.00      0.89       366\n",
            "          CD       1.00      1.00      1.00       858\n",
            "          DT       0.99      0.99      0.99      1335\n",
            "          EX       0.83      1.00      0.91         5\n",
            "          IN       0.95      0.99      0.97      1630\n",
            "          JJ       0.84      0.99      0.91       918\n",
            "         JJR       0.79      1.00      0.88        59\n",
            "         JJS       0.91      1.00      0.95        31\n",
            "          MD       0.97      1.00      0.98       167\n",
            "          NN       0.88      0.99      0.93      2383\n",
            "         NNP       0.84      0.97      0.90      1504\n",
            "        NNPS       0.59      0.95      0.73        44\n",
            "         NNS       0.96      1.00      0.98       941\n",
            "         PDT       0.12      0.50      0.20         4\n",
            "         POS       0.94      0.95      0.94       152\n",
            "         PRP       1.00      1.00      1.00       192\n",
            "        PRP$       1.00      1.00      1.00        99\n",
            "          RB       0.75      0.97      0.84       381\n",
            "         RBR       0.40      0.80      0.53        15\n",
            "         RBS       0.25      1.00      0.40         3\n",
            "          RP       0.62      0.48      0.54        33\n",
            "          TO       1.00      1.00      1.00       386\n",
            "          VB       0.72      0.98      0.83       403\n",
            "         VBD       0.85      1.00      0.92       634\n",
            "         VBG       0.76      1.00      0.86       221\n",
            "         VBN       0.68      0.98      0.80       366\n",
            "         VBP       0.63      0.96      0.76       134\n",
            "         VBZ       0.97      0.96      0.97       280\n",
            "         WDT       0.46      1.00      0.63        84\n",
            "          WP       1.00      1.00      1.00        20\n",
            "         WP$       1.00      1.00      1.00         4\n",
            "         WRB       1.00      1.00      1.00        24\n",
            "\n",
            "   micro avg       0.87      0.99      0.93     13676\n",
            "   macro avg       0.80      0.95      0.85     13676\n",
            "weighted avg       0.89      0.99      0.93     13676\n",
            " samples avg       0.93      0.99      0.95     13676\n",
            "\n"
          ]
        }
      ]
    }
  ]
}