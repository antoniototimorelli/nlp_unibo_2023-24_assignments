{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "- Guardare creazione dizionario, bisogna rispettare i punti dell'assignment;\n",
        "- Non togliere punctuation e symbols ma evitare di utilizzarli nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole);\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 11/01/2022 (dd/mm/yyyy)\n",
        "\n",
        "If you deliver it by 11/12/2021 your assignment will be graded by 11/01/2022.\n",
        "\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures\n",
        "\n",
        "# Execution\n",
        "## 0. Utils"
      ],
      "metadata": {
        "id": "Y0BInycOeQL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar\n",
        "from IPython.display import display_html\n",
        "from itertools import chain,cycle\n",
        "\n",
        "# Preprocessing of words\n",
        "def pre_process(df,string):\n",
        "    def text_pre_process(text):\n",
        "        ret = re.sub('RT @(.)+?:\\s|(&#[0-9]+;)|@([\\w\\-]+)|(#)\\S+|(http)s?\\S+|&gt;|^\\s+|\\b\\s+|\\n', '', text)\n",
        "        ret = re.sub('\\s\\s+|[^a-zA-Z\\d\\s:]' , ' ', ret).rstrip().lower()\n",
        "        return ret\n",
        "    return df[string].apply(text_pre_process)\n",
        "\n",
        "# Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Display dataframes\n",
        "def display(*args,titles=cycle([''])):\n",
        "    html_str=''\n",
        "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
        "        html_str+='<th style=\"text-align:left\"><td style=\"vertical-align:top\">'\n",
        "        html_str+=f'<h4 style=\"text-align: left;\">{title}</h2>'\n",
        "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
        "        html_str+='</td></th>'\n",
        "    display_html(html_str,raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "6c58f1fc-a77d-44a6-c55d-6ee19866ef10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:38 Time:  0:02:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Corpus\n",
        "### 1.1 Pre-processing\n",
        "\n",
        "From the original tags list we removed all the symbols and english punctuation plus:\n",
        "- FW, Foreign Word, because there are no examples in the test set;\n",
        "- UH, Interjection, because there are no examples in the test set;\n",
        "- LS, List Item Marker, because there are no examples in the test set (and because it denotes symbols as well);"
      ],
      "metadata": {
        "id": "5filE0ydeiga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank tagged sentences\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "# ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] not in ignore]\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] not in ignore]\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] not in ignore]\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist]\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist]\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "train_df['word'] = train_df['word'].str.lower()\n",
        "# train_df['word'] = pre_process(train_df,'word')\n",
        "\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df['word'] = val_df['word'].str.lower()\n",
        "# val_df['word'] = pre_process(val_df,'word')\n",
        "\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df['word'] = test_df['word'].str.lower()\n",
        "# test_df['word'] = pre_process(test_df,'word')\n",
        "\n",
        "display(train_df.describe(), val_df.describe(), test_df.describe(), titles = [f'Training set {train_df.shape}', f'Validation set {val_df.shape}', f'Test set {test_df.shape}'])"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "0ded186c-32b4-45be-d840-67f7864d116c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Training set (50748, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50748</td>\n",
              "      <td>50748</td>\n",
              "      <td>50748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>7837</td>\n",
              "      <td>46</td>\n",
              "      <td>1963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>,</td>\n",
              "      <td>NN</td>\n",
              "      <td>1854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2570</td>\n",
              "      <td>6270</td>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Validation set (33260, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>33260</td>\n",
              "      <td>33260</td>\n",
              "      <td>33260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>5596</td>\n",
              "      <td>45</td>\n",
              "      <td>1299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>the</td>\n",
              "      <td>NN</td>\n",
              "      <td>339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1670</td>\n",
              "      <td>4513</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th><th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\">Test set (16668, 3)</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16668</td>\n",
              "      <td>16668</td>\n",
              "      <td>16668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3432</td>\n",
              "      <td>41</td>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>,</td>\n",
              "      <td>NN</td>\n",
              "      <td>232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>787</td>\n",
              "      <td>2383</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table style=\"display:inline\"></td></th>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "max_tags_list = max([len(tags_train),len(tags_val),len(tags_test)])\n",
        "\n",
        "# Training set tags list\n",
        "print(f'Train tags number: {len(tags_train)}')\n",
        "print(f'Train tags list: {tags_train}')\n",
        "\n",
        "exceeding_validation = [el for el in tags_train if el not in tags_val]\n",
        "if exceeding_validation != []:\n",
        "  print(f'\\tClasses in training set for which there are no samples in validation set: {exceeding_validation}')\n",
        "\n",
        "exceeding_test = [el for el in tags_train if el not in tags_test]\n",
        "\n",
        "if exceeding_test != []:\n",
        "  print(f'\\tClasses in training set for which there are no samples in test set: {exceeding_test}')\n",
        "\n",
        "\n",
        "# Validation set tags list\n",
        "print(f'\\nValidation tags number: {len(tags_val)}')\n",
        "print(f'Validation tags list: {tags_val}')\n",
        "\n",
        "exceeding_training = [el for el in tags_val if el not in tags_train]\n",
        "if exceeding_training != []:\n",
        "  print(f'\\tClasses in validation set for which there are no samples in training set: {exceeding_training}')\n",
        "\n",
        "exceeding_test = [el for el in tags_val if el not in tags_test]\n",
        "if exceeding_test != []:\n",
        "  print(f'\\tClasses in validation set for which there are no samples in test set: {exceeding_test}')\n",
        "\n",
        "# Validation set tags list\n",
        "print(f'\\nTest tags number: {len(tags_test)}')\n",
        "print(f'Test tags list: {tags_test}')\n",
        "\n",
        "exceeding_training = [el for el in tags_test if el not in tags_train]\n",
        "if exceeding_training != []:\n",
        "  print(f'\\tClasses in test set for which there are no samples in training set: {exceeding_training}')\n",
        "\n",
        "exceeding_val = [el for el in tags_test if el not in tags_val]\n",
        "if exceeding_val != []:\n",
        "  print(f'\\tClasses in test set set for which there are no samples in validation set: {exceeding_val}')\n",
        "\n",
        "# conf_mat_df = pd.DataFrame(columns=tags_train, index=tags_train)\n",
        "# conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "01b6917f-9d7e-4f18-bf6c-c1608fbd021a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 46\n",
            "Train tags list: ['#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "\tClasses in training set for which there are no samples in validation set: ['SYM']\n",
            "\tClasses in training set for which there are no samples in test set: ['#', 'FW', 'LS', 'SYM', 'UH']\n",
            "\n",
            "Validation tags number: 45\n",
            "Validation tags list: ['#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
            "\tClasses in validation set for which there are no samples in test set: ['#', 'FW', 'LS', 'UH']\n",
            "\n",
            "Test tags number: 41\n",
            "Test tags list: ['$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriving prepocessed data\n",
        "X_train_raw = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "X_val_raw = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "X_test_raw = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "y_train_raw = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "y_val_raw = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "y_test_raw = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "# Creating sets of words and tags\n",
        "words = set([word.lower() for sentence in [*X_train_raw, *X_val_raw] for word in sentence])\n",
        "tags = set([tag for sentence_tags in [*y_train_raw, *y_val_raw] for tag in sentence_tags])\n",
        "\n",
        "# Bulding vocabulary of words and tags \n",
        "word2index = {word: i + 2 for i, word in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "word2index['-OOV-'] = 1  # 1 is assigned for unknown words\n",
        "tag2index = {tag: i for i, tag in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "\n",
        "vocab_length = len(word2index)\n",
        "print(f'Length vocabulary: {vocab_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlBmUbmIlG0o",
        "outputId": "156be670-15a4-44d3-9f3a-1922199bec47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length vocabulary: 10343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenising words and tags by their indexes in vocabulary\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = [], [], [], [], [], []\n",
        "\n",
        "# Encode X\n",
        "for sentence in X_train_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    X_train.append(sent_int)\n",
        "\n",
        "for sentence in X_val_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    X_val.append(sent_int)\n",
        "\n",
        "for sentence in X_test_raw:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    X_test.append(sent_int)\n",
        "\n",
        "# Encode Y\n",
        "for sent_tags in y_train_raw:\n",
        "    y_train.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in y_val_raw:\n",
        "    y_val.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in y_test_raw:\n",
        "    y_test.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "print('-Not encoded')\n",
        "print('\\t',X_train_raw[0]) \n",
        "print('\\t',y_train_raw[0])\n",
        "print('-Encoded')\n",
        "print('\\t',X_train[0])\n",
        "print('\\t',y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlvH2AwynXAk",
        "outputId": "9d1eb5d3-1d9f-42ad-a175-03a22cae4aaf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-Not encoded\n",
            "\t ['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n",
            "\t ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n",
            "-Encoded\n",
            "\t [5680, 8733, 6504, 1962, 9494, 3958, 6504, 8467, 5149, 4672, 6961, 4731, 5030, 5220, 8902, 7998, 10064, 9358]\n",
            "\t [30, 30, 0, 32, 26, 12, 0, 38, 36, 3, 8, 22, 3, 12, 8, 30, 32, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = [len(sentence) for sentence in [*X_train_raw,*X_val_raw,*X_test_raw]]\n",
        "MAX_LENGTH = max(lengths) # maximum words in a sentence\n",
        "AVG_LENGTH = int(sum(lengths)/len(lengths))\n",
        "\n",
        "print(f'Length of longest sentence: {MAX_LENGTH}')\n",
        "print(f'Average length: {AVG_LENGTH}')\n",
        "\n",
        "# Add padding to sentences\n",
        "# X_train = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
        "# X_val = pad_sequences(X_val, maxlen=MAX_LENGTH, padding='post')\n",
        "# X_test = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# y_train = pad_sequences(y_train, maxlen=MAX_LENGTH, padding='post')\n",
        "# y_val = pad_sequences(y_val, maxlen=MAX_LENGTH, padding='post')\n",
        "# y_test = pad_sequences(y_test, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=AVG_LENGTH, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=AVG_LENGTH, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=AVG_LENGTH, padding='post')\n",
        "\n",
        "y_train = pad_sequences(y_train, maxlen=AVG_LENGTH, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=AVG_LENGTH, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=AVG_LENGTH, padding='post')\n",
        "\n",
        "\n",
        "print('-Padded')\n",
        "print('\\tX:',X_train[0])\n",
        "print('\\n\\ty:',y_train[0])\n",
        "\n",
        "# Create a sample weight vector where the weights for padded samples, punctuation and symbols \n",
        "# are set to 0 and the weights for the other samples are set to 1\n",
        "ignore = [':', '#', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "# sample_weight = np.ones(X_train.shape)\n",
        "# for i in range(X_train.shape[0]):\n",
        "#     for j in range(X_train.shape[1]):\n",
        "#         if X_train[i][j] == 0 or y_train_raw[i][j] in ignore:\n",
        "#             sample_weight[i][j] = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRSBSBqosWyK",
        "outputId": "d6b04671-d942-48ad-acdf-e119504d173d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of longest sentence: 271\n",
            "Average length: 25\n",
            "-Padded\n",
            "\tX: [ 5680  8733  6504  1962  9494  3958  6504  8467  5149  4672  6961  4731\n",
            "  5030  5220  8902  7998 10064  9358     0     0     0     0     0     0\n",
            "     0]\n",
            "\n",
            "\ty: [30 30  0 32 26 12  0 38 36  3  8 22  3 12  8 30 32  4  0  0  0  0  0  0\n",
            "  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = to_categorical(y_train, max_tags_list)\n",
        "y_val_one_hot = to_categorical(y_val, max_tags_list)\n",
        "y_test_one_hot = to_categorical(y_test, max_tags_list)"
      ],
      "metadata": {
        "id": "Tt3IPOjD1AER"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. GloVe \n",
        "GloVe (Global Vectors for Word Representation) is a method for learning vector representations of words, called \"word embeddings,\" from a large corpus of text. Word embeddings are numerical representations of words that capture the semantic relationships between words in a continuous, low-dimensional space. They are commonly used as input to natural language processing models, such as language translation and language modeling.\n",
        "\n",
        "GloVe works by learning the co-occurrence statistics of words in a corpus, and using this information to learn word embeddings that capture the semantic relationships between words. The GloVe method produces word embeddings that are trained on a global corpus, as opposed to embeddings that are trained on a specific task or dataset.\n",
        "\n",
        "There are different versions of the GloVe word embeddings, including 50-dimensional, 100-dimensional, and 200-dimensional embeddings. The 50-dimensional version of GloVe embeddings may be better in some applications because they have a lower dimensionality, which can make them easier to work with and more computationally efficient.\n",
        "\n",
        "By using GloVe embeddings as the initial weights for a model, we can take advantage of these pre-trained word representations and fine-tune them for a specific task."
      ],
      "metadata": {
        "id": "1SqOgc6v1DfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "embedding_dim = 300\n",
        "embedding_dict = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, f'glove.6B.{embedding_dim}d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_dict[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embedding_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "30d5be48-fd1c-44c6-ff23-f23eb4d21fb0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "# embedding_matrix = np.standard(0,1,(vocab_length, embedding_dim))\n",
        "for word, i in word2index.items():\n",
        "    embedding_vector = embedding_dict.get(word)\n",
        "    if i < len(word2index):\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "YcvBdLXW1QTY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model\n",
        "### 3.1 Baseline \n",
        "Bidirectional LSTM layers are able to process sequential data in both the forward and backward directions, which can allow the model to capture contextual information from both the past and the future. This can be particularly useful for natural language processing tasks, where the meaning of a word can depend on the context in which it is used.\n",
        "\n",
        "In the context of POS tagging, TimeDistributed can be used to apply a tag prediction layer to each word in a sentence. For example, you might have an RNN that processes a sequence of words in a sentence, and at each time step, the RNN outputs a hidden state. You could then apply a TimeDistributed dense layer to the hidden states, which would allow you to predict the POS tag for each word in the sentence.\n",
        "\n",
        "One advantage of using TimeDistributed for POS tagging is that it allows you to predict the POS tag for each word in the sentence simultaneously, rather than having to process the sentence one word at a time. This can be particularly useful when dealing with long sentences, as it can make the tagging process more efficient.\n",
        "\n",
        "Overall, using TimeDistributed for POS tagging can help you build more accurate and efficient models for natural language processing tasks that involve sequential data."
      ],
      "metadata": {
        "id": "AvF_XXvV3Qxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ignore_class_accuracy(classes=[0]):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        "        \n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32')\n",
        "        for to_ignore in classes:\n",
        "          ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "          matches = matches * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "# Define the model\n",
        "baseline_model = Sequential(name='Baseline')\n",
        "\n",
        "# Add the Embedding layer\n",
        "# baseline_model.add(Embedding(input_dim=vocab_length, output_dim=embedding_dim, \\\n",
        "#                     weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=False))\n",
        "baseline_model.add(Embedding(input_dim=vocab_length, output_dim=embedding_dim, \\\n",
        "                    weights=[embedding_matrix], input_length=AVG_LENGTH, trainable=False))\n",
        "\n",
        "# Add the Bidirectional LSTM layer\n",
        "baseline_model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
        "\n",
        "# Add the Dense/Fully-Connected layer\n",
        "baseline_model.add(TimeDistributed(Dense(units=max_tags_list, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "baseline_model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy',\\\n",
        "                      ignore_class_accuracy([0,*[tag2index[tag] for tag in ignore]])])\n",
        "\n",
        "# Summary\n",
        "baseline_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuLsYIaYIBgn",
        "outputId": "7e6defc1-a56e-499d-a07a-d5337290c943"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 25, 300)           3102900   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 25, 512)          1140736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, 25, 46)           23598     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,267,234\n",
            "Trainable params: 1,164,334\n",
            "Non-trainable params: 3,102,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_baseline = baseline_model.fit(X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot),\\\n",
        "                                      batch_size=128, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "fb93d059-d39d-4881-88e1-e26424d0e451"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 52s 3s/step - loss: 2.6958 - accuracy: 0.3435 - ignore_accuracy: 0.1563 - val_loss: 1.9748 - val_accuracy: 0.4935 - val_ignore_accuracy: 0.2600\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 10s 615ms/step - loss: 1.6179 - accuracy: 0.5901 - ignore_accuracy: 0.3341 - val_loss: 1.2555 - val_accuracy: 0.6909 - val_ignore_accuracy: 0.4131\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 9s 593ms/step - loss: 1.0265 - accuracy: 0.7493 - ignore_accuracy: 0.4629 - val_loss: 0.8678 - val_accuracy: 0.7747 - val_ignore_accuracy: 0.4877\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 9s 590ms/step - loss: 0.7306 - accuracy: 0.8142 - ignore_accuracy: 0.5250 - val_loss: 0.6774 - val_accuracy: 0.8219 - val_ignore_accuracy: 0.5366\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 9s 580ms/step - loss: 0.5819 - accuracy: 0.8486 - ignore_accuracy: 0.5556 - val_loss: 0.5739 - val_accuracy: 0.8488 - val_ignore_accuracy: 0.5616\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 10s 611ms/step - loss: 0.4927 - accuracy: 0.8709 - ignore_accuracy: 0.5762 - val_loss: 0.5128 - val_accuracy: 0.8629 - val_ignore_accuracy: 0.5748\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 9s 593ms/step - loss: 0.4332 - accuracy: 0.8853 - ignore_accuracy: 0.5936 - val_loss: 0.4661 - val_accuracy: 0.8724 - val_ignore_accuracy: 0.5837\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 9s 598ms/step - loss: 0.3860 - accuracy: 0.8969 - ignore_accuracy: 0.6037 - val_loss: 0.4344 - val_accuracy: 0.8799 - val_ignore_accuracy: 0.5928\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 11s 704ms/step - loss: 0.3500 - accuracy: 0.9050 - ignore_accuracy: 0.6099 - val_loss: 0.4134 - val_accuracy: 0.8834 - val_ignore_accuracy: 0.5967\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 12s 768ms/step - loss: 0.3204 - accuracy: 0.9130 - ignore_accuracy: 0.6230 - val_loss: 0.3923 - val_accuracy: 0.8909 - val_ignore_accuracy: 0.6035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = baseline_model.evaluate(X_test, y_test_one_hot, return_dict = True)\n",
        "\n",
        "# Obtain the predictions made by the model on the test set\n",
        "predictions = baseline_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "ebe085d8-7360-45cd-e50c-719019ce24eb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 2s 75ms/step - loss: 0.4270 - accuracy: 0.8737 - ignore_accuracy: 0.6140\n",
            "21/21 [==============================] - 2s 67ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the class probabilities into class labels\n",
        "# [0,*[tag2index[tag] for tag in ignore]\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "true_pos = defaultdict(int)\n",
        "false_pos = defaultdict(int)\n",
        "false_neg = defaultdict(int)\n",
        "precision = defaultdict(float)\n",
        "recall = defaultdict(float)\n",
        "f1score = defaultdict(float)\n",
        "\n",
        "index2tag = {v: k for k, v in tag2index.items()}\n",
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    for idx_sentence in range(len(y_test)):\n",
        "      for idx_word in range(AVG_LENGTH):\n",
        "      # for idx_word in range(MAX_LENGTH):\n",
        "        if index2tag[y_test[idx_sentence][idx_word]] == tag:\n",
        "          # If the predicted tag matches ground truth we increase true positive count for that tag\n",
        "          if predicted_labels[idx_sentence][idx_word] == y_test[idx_sentence][idx_word]:\n",
        "            true_pos[index2tag[y_test[idx_sentence][idx_word]]] += 1\n",
        "          else:\n",
        "          # If the predicted tag does not match ground truth we increase false negative count for that tag\n",
        "          # and false positive count for the wrongly predicted tag\n",
        "            false_neg[index2tag[y_test[idx_sentence][idx_word]]] += 1\n",
        "            false_pos[index2tag[predicted_labels[idx_sentence][idx_word]]] += 1\n",
        "    # We then compute precision, recall and F1 scores\n",
        "    if true_pos[tag] != 0:\n",
        "      precision[tag] = true_pos[tag] / (true_pos[tag] + false_pos[tag])\n",
        "      recall[tag] = true_pos[tag] / (true_pos[tag] + false_neg[tag])\n",
        "      f1score[tag] = 2 * ((precision[tag] * recall[tag]) / (precision[tag] + recall[tag])) \n",
        "    else:\n",
        "      print(tag)\n",
        "      print(true_pos[tag])\n",
        "      print(false_pos[tag])\n",
        "      print(false_neg[tag])\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP2TlmEVynlM",
        "outputId": "e2e89593-9361-46c1-e8e2-5d1e367a57dc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNPS\n",
            "0\n",
            "0\n",
            "35\n",
            "\n",
            "PDT\n",
            "0\n",
            "0\n",
            "3\n",
            "\n",
            "RBS\n",
            "0\n",
            "0\n",
            "2\n",
            "\n",
            "WP$\n",
            "0\n",
            "0\n",
            "3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FORSE DOBBIAMO CONSIDERARE TUTTE LE CLASSI; ANCHE QUELLE PRESENTI SOLO NEL TRAINING TEST, DANDO COME f1 SCORE 0\n",
        "for tag in tags_test:\n",
        "  if tag not in ignore:\n",
        "    print(f'Tag: {tag}')\n",
        "    print(f'\\tPrecision: {precision[tag]}')\n",
        "    print(f'\\tRecall: {recall[tag]}')\n",
        "    print(f'\\tF1-score: {f1score[tag]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1AkaNs61oC6",
        "outputId": "62d11176-ca2b-4eb0-f220-62d4eb4551df"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag: CC\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9737704918032787\n",
            "\tF1-score: 0.9867109634551495\n",
            "Tag: CD\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.8103448275862069\n",
            "\tF1-score: 0.8952380952380952\n",
            "Tag: DT\n",
            "\tPrecision: 0.998046875\n",
            "\tRecall: 0.983638113570741\n",
            "\tF1-score: 0.990790111488124\n",
            "Tag: EX\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.75\n",
            "\tF1-score: 0.8571428571428571\n",
            "Tag: IN\n",
            "\tPrecision: 0.9992025518341308\n",
            "\tRecall: 0.9357729648991785\n",
            "\tF1-score: 0.9664481295796375\n",
            "Tag: JJ\n",
            "\tPrecision: 0.9442446043165468\n",
            "\tRecall: 0.68717277486911\n",
            "\tF1-score: 0.7954545454545455\n",
            "Tag: JJR\n",
            "\tPrecision: 0.9285714285714286\n",
            "\tRecall: 0.5306122448979592\n",
            "\tF1-score: 0.6753246753246754\n",
            "Tag: JJS\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.25\n",
            "\tF1-score: 0.4\n",
            "Tag: MD\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9928571428571429\n",
            "\tF1-score: 0.996415770609319\n",
            "Tag: NN\n",
            "\tPrecision: 0.9304973037747154\n",
            "\tRecall: 0.7815802717664821\n",
            "\tF1-score: 0.849562363238512\n",
            "Tag: NNP\n",
            "\tPrecision: 0.7955752212389381\n",
            "\tRecall: 0.8157894736842105\n",
            "\tF1-score: 0.8055555555555556\n",
            "Tag: NNPS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: NNS\n",
            "\tPrecision: 0.8490028490028491\n",
            "\tRecall: 0.739454094292804\n",
            "\tF1-score: 0.7904509283819628\n",
            "Tag: PDT\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: POS\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 1.0\n",
            "\tF1-score: 1.0\n",
            "Tag: PRP\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.9645390070921985\n",
            "\tF1-score: 0.9819494584837545\n",
            "Tag: PRP$\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 1.0\n",
            "\tF1-score: 1.0\n",
            "Tag: RB\n",
            "\tPrecision: 0.7936507936507936\n",
            "\tRecall: 0.6329113924050633\n",
            "\tF1-score: 0.704225352112676\n",
            "Tag: RBR\n",
            "\tPrecision: 0.3333333333333333\n",
            "\tRecall: 0.14285714285714285\n",
            "\tF1-score: 0.2\n",
            "Tag: RBS\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: RP\n",
            "\tPrecision: 0.39285714285714285\n",
            "\tRecall: 0.8148148148148148\n",
            "\tF1-score: 0.5301204819277108\n",
            "Tag: TO\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 1.0\n",
            "\tF1-score: 1.0\n",
            "Tag: VB\n",
            "\tPrecision: 0.9029411764705882\n",
            "\tRecall: 0.8599439775910365\n",
            "\tF1-score: 0.8809182209469154\n",
            "Tag: VBD\n",
            "\tPrecision: 0.910958904109589\n",
            "\tRecall: 0.8193018480492813\n",
            "\tF1-score: 0.8627027027027028\n",
            "Tag: VBG\n",
            "\tPrecision: 0.8\n",
            "\tRecall: 0.44692737430167595\n",
            "\tF1-score: 0.5734767025089605\n",
            "Tag: VBN\n",
            "\tPrecision: 0.8622448979591837\n",
            "\tRecall: 0.5652173913043478\n",
            "\tF1-score: 0.6828282828282828\n",
            "Tag: VBP\n",
            "\tPrecision: 0.845360824742268\n",
            "\tRecall: 0.803921568627451\n",
            "\tF1-score: 0.8241206030150755\n",
            "Tag: VBZ\n",
            "\tPrecision: 0.9521531100478469\n",
            "\tRecall: 0.8805309734513275\n",
            "\tF1-score: 0.9149425287356322\n",
            "Tag: WDT\n",
            "\tPrecision: 0.8266666666666667\n",
            "\tRecall: 0.9393939393939394\n",
            "\tF1-score: 0.8794326241134751\n",
            "Tag: WP\n",
            "\tPrecision: 1.0\n",
            "\tRecall: 0.8823529411764706\n",
            "\tF1-score: 0.9375\n",
            "Tag: WP$\n",
            "\tPrecision: 0.0\n",
            "\tRecall: 0.0\n",
            "\tF1-score: 0.0\n",
            "Tag: WRB\n",
            "\tPrecision: 0.8888888888888888\n",
            "\tRecall: 0.42105263157894735\n",
            "\tF1-score: 0.5714285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "provola = [el[1] for el in f1score.items()]\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for p in provola:\n",
        "  sum += p\n",
        "\n",
        "print(f'MACRO F1-score: {sum / len(f1score.items())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r14KwxMp6pxZ",
        "outputId": "7f4aeadc-b108-429a-cb78-55384d6e14a1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MACRO F1-score: 0.7047731101335059\n"
          ]
        }
      ]
    }
  ]
}