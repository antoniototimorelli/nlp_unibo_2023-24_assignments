{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "\n",
        "- evitare di utilizzare la punctuation nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole)+ preprocessing di termini strani;\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "bb663c75-8a75-4006-9325-0ec14477f2fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:38 Time:  0:02:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "#ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e24b64d-7a0c-43e4-e351-4a586780e2a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         word     tag sentence\n",
            "0      Pierre     NNP        0\n",
            "1      Vinken     NNP        0\n",
            "2           ,       ,        0\n",
            "3          61      CD        0\n",
            "4       years     NNS        0\n",
            "...       ...     ...      ...\n",
            "50743      he     PRP     1962\n",
            "50744     has     VBZ     1962\n",
            "50745   faced     VBN     1962\n",
            "50746   *T*-2  -NONE-     1962\n",
            "50747       .       .     1962\n",
            "\n",
            "[50748 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "a1e4b11b-5221-4c4f-bd22-26cab96a67f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 46\n",
            "Val tags number: 45\n",
            "Test tags number: 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "q32n9MjbqC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "jOf1nvNsqGfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index =  {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "4510397a-6d24-4d2c-cd74-d6109f9f41ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "lxxunk3yqOzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "def update_vocab(df,embeddings_index,embedding_dim):\n",
        "  oov_c = 0\n",
        "  for word in df.word:\n",
        "    if word not in embeddings_index:\n",
        "      oov_c += 1\n",
        "      random_embed = np.random.rand(embedding_dim)\n",
        "      embeddings_index[word] = random_embed\n",
        "  print(\"Added\",oov_c,\"OOV words + respective embeddings to the vocabulary.\")\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = update_vocab(train_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "id": "NuseO3gBjoN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07573145-ed24-4a0d-8e83-a19697109548"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 2777 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "_2RFWZLCtXZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(val_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSiNQkytU0B",
        "outputId": "0c8c939d-455e-4934-e8fb-aa601093b62d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 951 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "JIW5jaKdt0Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(test_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSoqiGXt4qG",
        "outputId": "bca49068-c320-4905-a3f4-1bcdaf4a1438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 455 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "idx2word = OrderedDict()\n",
        "word2idx = OrderedDict()\n",
        "    \n",
        "curr_idx = 0\n",
        "for key in embeddings_index.keys():\n",
        "  word2idx[key] = curr_idx\n",
        "  idx2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "#word_listing = list(idx2word.values())\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF0VtzwbBMXO",
        "outputId": "6ca29ff7-485f-4e82-dc39-3a3204c49229"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 404183\n",
            "[Debug] Word -> Index vocabulary size: 404183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2idx = OrderedDict()\n",
        "\n",
        "curr_id = 0\n",
        "\n",
        "for tag in tags_train:\n",
        "  tag2idx[tag] = curr_id\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLHdqXbDIzS",
        "outputId": "38f355f9-dcf2-4ef2-cdfc-38cd5fc3ecd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Tag -> Index vocabulary size: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "true_pos_tag = defaultdict(int)\n",
        "false_pos_tag = defaultdict(int)\n",
        "false_neg_tag = defaultdict(int)\n",
        "precision_tags = defaultdict(float)\n",
        "accuracy_tags = defaultdict(float)\n",
        "recall_tags = defaultdict(float)\n",
        "f1score_tags = defaultdict(float)\n"
      ],
      "metadata": {
        "id": "WuLsYIaYIBgn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "   \n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "tbPprk74bhkC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2idx):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2idx), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2idx))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2idx))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "fab5c888-6750-417d-e170-85e35a3856bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 271, 300)          121254900 \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 271, 512)         1140736   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 271, 46)          23598     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 271, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,419,234\n",
            "Trainable params: 1,164,334\n",
            "Non-trainable params: 121,254,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2idx)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2idx))),\n",
        "          batch_size=128, epochs= 15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8Dsz2oYyiO",
        "outputId": "6e40190f-af1c-4f9c-bd6e-88609cf8a5c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "16/16 [==============================] - 90s 5s/step - loss: 0.7113 - accuracy: 0.8499 - ignore_accuracy: 0.2735 - val_loss: 0.3138 - val_accuracy: 0.9161 - val_ignore_accuracy: 0.1609\n",
            "Epoch 2/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.2881 - accuracy: 0.9241 - ignore_accuracy: 0.2222 - val_loss: 0.2636 - val_accuracy: 0.9290 - val_ignore_accuracy: 0.2553\n",
            "Epoch 3/15\n",
            "16/16 [==============================] - 87s 5s/step - loss: 0.2471 - accuracy: 0.9369 - ignore_accuracy: 0.3561 - val_loss: 0.2351 - val_accuracy: 0.9410 - val_ignore_accuracy: 0.3834\n",
            "Epoch 4/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.2187 - accuracy: 0.9465 - ignore_accuracy: 0.4469 - val_loss: 0.2071 - val_accuracy: 0.9495 - val_ignore_accuracy: 0.4629\n",
            "Epoch 5/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.1901 - accuracy: 0.9531 - ignore_accuracy: 0.5120 - val_loss: 0.1785 - val_accuracy: 0.9560 - val_ignore_accuracy: 0.5310\n",
            "Epoch 6/15\n",
            "16/16 [==============================] - 87s 5s/step - loss: 0.1618 - accuracy: 0.9601 - ignore_accuracy: 0.5830 - val_loss: 0.1519 - val_accuracy: 0.9629 - val_ignore_accuracy: 0.6032\n",
            "Epoch 7/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.1371 - accuracy: 0.9669 - ignore_accuracy: 0.6540 - val_loss: 0.1300 - val_accuracy: 0.9687 - val_ignore_accuracy: 0.6647\n",
            "Epoch 8/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.1174 - accuracy: 0.9716 - ignore_accuracy: 0.7033 - val_loss: 0.1132 - val_accuracy: 0.9722 - val_ignore_accuracy: 0.7007\n",
            "Epoch 9/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.1025 - accuracy: 0.9749 - ignore_accuracy: 0.7370 - val_loss: 0.1004 - val_accuracy: 0.9751 - val_ignore_accuracy: 0.7310\n",
            "Epoch 10/15\n",
            "16/16 [==============================] - 84s 5s/step - loss: 0.0910 - accuracy: 0.9777 - ignore_accuracy: 0.7665 - val_loss: 0.0906 - val_accuracy: 0.9776 - val_ignore_accuracy: 0.7583\n",
            "Epoch 11/15\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.0820 - accuracy: 0.9800 - ignore_accuracy: 0.7901 - val_loss: 0.0828 - val_accuracy: 0.9796 - val_ignore_accuracy: 0.7808\n",
            "Epoch 12/15\n",
            "16/16 [==============================] - 84s 5s/step - loss: 0.0748 - accuracy: 0.9818 - ignore_accuracy: 0.8104 - val_loss: 0.0766 - val_accuracy: 0.9809 - val_ignore_accuracy: 0.7951\n",
            "Epoch 13/15\n",
            "16/16 [==============================] - 85s 5s/step - loss: 0.0689 - accuracy: 0.9832 - ignore_accuracy: 0.8237 - val_loss: 0.0715 - val_accuracy: 0.9822 - val_ignore_accuracy: 0.8094\n",
            "Epoch 14/15\n",
            "16/16 [==============================] - 86s 5s/step - loss: 0.0640 - accuracy: 0.9843 - ignore_accuracy: 0.8345 - val_loss: 0.0672 - val_accuracy: 0.9833 - val_ignore_accuracy: 0.8210\n",
            "Epoch 15/15\n",
            "16/16 [==============================] - 96s 6s/step - loss: 0.0600 - accuracy: 0.9852 - ignore_accuracy: 0.8441 - val_loss: 0.0638 - val_accuracy: 0.9839 - val_ignore_accuracy: 0.8275\n",
            "<keras.callbacks.History object at 0x7fa792b48070>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2idx)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in tag2idx.items()})\n",
        "#y_prob_class = model.predict_classes(test_sentences_X, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "1f7e338f-6172-4af9-ae70-3a5eba000871"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 15s 708ms/step - loss: 0.0564 - accuracy: 0.9856 - ignore_accuracy: 0.8471\n",
            "21/21 [==============================] - 14s 598ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen_num in range(len(test_tags)):\n",
        "    for i,tag in enumerate(test_tags[sen_num]):\n",
        "      \n",
        "        conf_mat_df[tag][pred_sequence[sen_num][i]] +=1\n",
        "        if test_tags[sen_num][i] == pred_sequence[sen_num][i]:\n",
        "          true_pos_tag[tag] += 1\n",
        "        else:\n",
        "          false_neg_tag[tag] += 1\n",
        "          false_pos_tag[pred_sequence[sen_num][i]] += 1\n",
        "\n",
        "for tag in tag_list[1:]:\n",
        "    if (true_pos_tag[tag] + false_pos_tag[tag]) != 0:\n",
        "      precision_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_pos_tag[tag])\n",
        "      recall_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag])\n",
        "      f1score_tags[tag] = 2 * precision_tags[tag] * recall_tags[tag] / (precision_tags[tag] + recall_tags[tag])\n",
        "      accuracy_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag] + false_pos_tag[tag])"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for el in f1score_tags.items():\n",
        "  sum += el[1]\n",
        "print(sum/len(f1score_tags.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "17ab2c21-48e5-4ac8-80ba-f6a73551da26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8353767352674445\n"
          ]
        }
      ]
    }
  ]
}