{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "- Guardare creazione dizionario, bisogna rispettare i punti dell'assignment;\n",
        "- Non togliere punctuation e symbols ma evitare di utilizzarli nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole);\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "f93964fd-be45-4e38-b97f-fd9c298cb9fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:38 Time:  0:02:38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "c9626443-4005-4fb4-c4a0-559ab3a99c79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "ignore = [':', '#', '\"', '$', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM', '-NONE-']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] not in ignore]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] not in ignore]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] not in ignore]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])"
      ],
      "metadata": {
        "id": "-ulx6UMBHint"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n",
        "\n",
        "if len(tags_test) != len(tags_val) or len(tags_test) != len(tags_train):\n",
        "  print('\\nMismatching numbers.')\n",
        "  print('Removing extra classes:')\n",
        "\n",
        "  missing_classes_train = [x for x in tags_train if x not in tags_test]\n",
        "  missing_classes_val   = [x for x in tags_val if x not in tags_test]\n",
        "\n",
        "  missing_classes = list(set(missing_classes_train + missing_classes_val))\n",
        "  print(missing_classes)\n",
        "\n",
        "  for cl in missing_classes:\n",
        "    train_df = train_df[train_df.tag != cl]\n",
        "    val_df = val_df[val_df.tag != cl]\n",
        "\n",
        "  tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "  tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "  tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "  print('\\nNew Train tags number:',len(tags_train))\n",
        "  print('New Val tags number:',len(tags_val))\n",
        "  print('New Test tags number:',len(tags_test))\n",
        "\n",
        "print('\\nTags:')\n",
        "for tag in tags_train:\n",
        "  print(f'-{tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "07284343-824b-4a57-9bb5-67c427ad1a39"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 35\n",
            "Val tags number: 35\n",
            "Test tags number: 32\n",
            "\n",
            "Mismatching numbers.\n",
            "Removing extra classes:\n",
            "['FW', 'UH', 'LS']\n",
            "\n",
            "New Train tags number: 32\n",
            "New Val tags number: 32\n",
            "New Test tags number: 32\n",
            "\n",
            "Tags:\n",
            "-CC\n",
            "-CD\n",
            "-DT\n",
            "-EX\n",
            "-IN\n",
            "-JJ\n",
            "-JJR\n",
            "-JJS\n",
            "-MD\n",
            "-NN\n",
            "-NNP\n",
            "-NNPS\n",
            "-NNS\n",
            "-PDT\n",
            "-POS\n",
            "-PRP\n",
            "-PRP$\n",
            "-RB\n",
            "-RBR\n",
            "-RBS\n",
            "-RP\n",
            "-TO\n",
            "-VB\n",
            "-VBD\n",
            "-VBG\n",
            "-VBN\n",
            "-VBP\n",
            "-VBZ\n",
            "-WDT\n",
            "-WP\n",
            "-WP$\n",
            "-WRB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "true_pos_tag = defaultdict(int)\n",
        "false_pos_tag = defaultdict(int)\n",
        "false_neg_tag = defaultdict(int)\n",
        "precision_tags = defaultdict(float)\n",
        "accuracy_tags = defaultdict(float)\n",
        "recall_tags = defaultdict(float)\n",
        "f1score_tags = defaultdict(float)\n",
        "\n",
        "words, tags = set([]), set([])\n",
        "\n",
        "#creating sets of words and tags \n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        words.add(word.lower())\n",
        "\n",
        "for tag_sent in train_tags:\n",
        "    for tag in tag_sent:\n",
        "        tags.add(tag)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    for word in sentence:\n",
        "        words.add(word.lower())\n",
        "\n",
        "for tag_sent in val_tags:\n",
        "    for tag in tag_sent:\n",
        "        tags.add(tag)\n",
        "\n",
        "\n",
        "#bulding vocabulary of words and tags \n",
        "word2index = {word: i + 2 for i, word in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "word2index['-OOV-'] = 1  # 1 is assigned for unknown words\n",
        "tag2index = {tag: i + 1 for i, tag in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "\n",
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            sent_int.append(word2index[word.lower()])\n",
        "        except KeyError:\n",
        "            sent_int.append(word2index['-OOV-'])\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2index[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2index), embedding_dim))\n",
        "for word, i in word2index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2index):\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2index), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuLsYIaYIBgn",
        "outputId": "be1d7ea6-f096-4cb2-b6a5-9dd2f1d9f723"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 271, 300)          2963400   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 271, 512)         1140736   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, 271, 33)          16929     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 271, 33)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,121,065\n",
            "Trainable params: 1,157,665\n",
            "Non-trainable params: 2,963,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "# model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2index)),\\\n",
        "#           validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2index))),\n",
        "#           batch_size=128, epochs= 9, validation_split=0.2)\n",
        "model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2index)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2index))),\n",
        "          batch_size=128, epochs= 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "PA776HlcMC_h",
        "outputId": "4d932fe4-31cb-42ea-e1f6-6ebd3b9aad57"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "16/16 [==============================] - 100s 6s/step - loss: 1.4670 - accuracy: 0.9094 - ignore_accuracy: 0.2388 - val_loss: 0.2445 - val_accuracy: 0.9434 - val_ignore_accuracy: 0.3422\n",
            "Epoch 2/9\n",
            "16/16 [==============================] - 96s 6s/step - loss: 0.1984 - accuracy: 0.9508 - ignore_accuracy: 0.4067 - val_loss: 0.1800 - val_accuracy: 0.9534 - val_ignore_accuracy: 0.4256\n",
            "Epoch 3/9\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.1576 - accuracy: 0.9589 - ignore_accuracy: 0.4953 - val_loss: 0.1482 - val_accuracy: 0.9623 - val_ignore_accuracy: 0.5409\n",
            "Epoch 4/9\n",
            "16/16 [==============================] - 97s 6s/step - loss: 0.1290 - accuracy: 0.9669 - ignore_accuracy: 0.5930 - val_loss: 0.1219 - val_accuracy: 0.9690 - val_ignore_accuracy: 0.6185\n",
            "Epoch 5/9\n",
            "16/16 [==============================] - 99s 6s/step - loss: 0.1076 - accuracy: 0.9725 - ignore_accuracy: 0.6633 - val_loss: 0.1047 - val_accuracy: 0.9730 - val_ignore_accuracy: 0.6680\n",
            "Epoch 6/9\n",
            "16/16 [==============================] - 104s 7s/step - loss: 0.0918 - accuracy: 0.9764 - ignore_accuracy: 0.7116 - val_loss: 0.0916 - val_accuracy: 0.9763 - val_ignore_accuracy: 0.7084\n",
            "Epoch 7/9\n",
            "16/16 [==============================] - 101s 6s/step - loss: 0.0800 - accuracy: 0.9794 - ignore_accuracy: 0.7487 - val_loss: 0.0818 - val_accuracy: 0.9783 - val_ignore_accuracy: 0.7341\n",
            "Epoch 8/9\n",
            "16/16 [==============================] - 97s 6s/step - loss: 0.0710 - accuracy: 0.9814 - ignore_accuracy: 0.7720 - val_loss: 0.0742 - val_accuracy: 0.9802 - val_ignore_accuracy: 0.7584\n",
            "Epoch 9/9\n",
            "16/16 [==============================] - 97s 6s/step - loss: 0.0639 - accuracy: 0.9830 - ignore_accuracy: 0.7911 - val_loss: 0.0683 - val_accuracy: 0.9816 - val_ignore_accuracy: 0.7747\n",
            "21/21 [==============================] - 15s 726ms/step - loss: 0.0656 - accuracy: 0.9817 - ignore_accuracy: 0.7773\n",
            "21/21 [==============================] - 15s 687ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '-PAD-'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-54e217fab9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mconf_mat_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '-PAD-'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2index)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in tag2index.items()})\n",
        "#y_prob_class = model.predict_classes(test_sentences_X, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "_Q4shasqRWUk",
        "outputId": "f87b9c80-b796-4082-c5a3-fadf2090825b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 14s 655ms/step - loss: 0.0656 - accuracy: 0.9817 - ignore_accuracy: 0.7773\n",
            "21/21 [==============================] - 14s 653ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '-PAD-'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2433440f182a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mconf_mat_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msen_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '-PAD-'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen_num in range(len(test_tags)):\n",
        "    for i,tag in enumerate(test_tags[sen_num]):\n",
        "      if pred_sequence[sen_num][i] != '-PAD-':\n",
        "        conf_mat_df[tag][pred_sequence[sen_num][i]] +=1\n",
        "        if test_tags[sen_num][i] == pred_sequence[sen_num][i]:\n",
        "          true_pos_tag[tag] += 1\n",
        "        else:\n",
        "          false_neg_tag[tag] += 1\n",
        "          false_pos_tag[pred_sequence[sen_num][i]] += 1\n",
        "\n",
        "for tag in tag_list[1:]:\n",
        "    if (true_pos_tag[tag] + false_pos_tag[tag]) != 0:\n",
        "      precision_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_pos_tag[tag])\n",
        "      recall_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag])\n",
        "      f1score_tags[tag] = 2 * precision_tags[tag] * recall_tags[tag] / (precision_tags[tag] + recall_tags[tag])\n",
        "      accuracy_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag] + false_pos_tag[tag])"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for el in f1score_tags.items():\n",
        "  sum += el[1]\n",
        "print(sum/len(f1score_tags.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "8a8d4e84-72f8-4efd-c0db-37300cd3e229"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7265950449499662\n"
          ]
        }
      ]
    }
  ]
}