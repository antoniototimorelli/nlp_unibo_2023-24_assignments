{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries"
      ],
      "metadata": {
        "id": "T4D-gKcwv5lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from urllib import request\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from typing import List, Dict, Callable\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "yFpyLOXDrpwq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download / Extraction of the dataset"
      ],
      "metadata": {
        "id": "GZfrMySWv0DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(download_path: str, url: str):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(\"Downloading dataset...\")\n",
        "        request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path: str, extract_path: str):\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "    with ZipFile(download_path) as loaded_zip:\n",
        "        loaded_zip.extractall(extract_path)\n",
        "    print(\"Extraction completed!\")"
      ],
      "metadata": {
        "id": "xiINYHffsNH2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v27X-tOqqUTz",
        "outputId": "7c6ebb8f-57ba-4153-cfb0-9866f779f62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current work directory: /content\n",
            "/content/Datasets/dependency_treebank.zip\n",
            "Downloading dataset...\n",
            "Download complete!\n",
            "Extracting dataset... (it may take a while...)\n",
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "print(f\"Current work directory: {os.getcwd()}\")\n",
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
        "print(dataset_path)\n",
        "\n",
        "download_dataset(dataset_path, url)\n",
        "extract_dataset(dataset_path, dataset_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataframe structuring"
      ],
      "metadata": {
        "id": "a_AguFkxv851"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete dataset, used and indexing for phrases and words to preserve the ordering of the sentences"
      ],
      "metadata": {
        "id": "Dak9VR2-1Qjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_dataset(dataset_name: str) -> pd.DataFrame:\n",
        "    dataframe_rows = []\n",
        "    folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name)\n",
        "    for filename in tqdm(os.listdir(folder)):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "              with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "                # read it and extract \n",
        "                file_id = int(filename.split(\"_\")[1].split(\".\")[0])\n",
        "                lines = text_file.read().splitlines()\n",
        "                c=0\n",
        "                w=0\n",
        "                for line in lines:\n",
        "                  phrase_id = str(c).zfill(2)\n",
        "                  word_id = str(w).zfill(2)\n",
        "                  if line.strip():\n",
        "                    word = line.split(\"\\t\")[0]\n",
        "                    pos = line.split(\"\\t\")[1]\n",
        "\n",
        "                    dataframe_row = {\n",
        "                        \"doc\": file_id,\n",
        "                        \"phrase_word\": phrase_id+\"_\"+word_id,\n",
        "                        \"word\": word,\n",
        "                        \"POS\": pos\n",
        "                    }\n",
        "\n",
        "                    dataframe_rows.append(dataframe_row)\n",
        "                    w += 1\n",
        "                  else:\n",
        "                    c += 1\n",
        "                    w = 0\n",
        "\n",
        "                  \n",
        "        except Exception as e:\n",
        "              print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "              sys.exit(0)\n",
        "    \n",
        "    folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # transform the list of rows in a proper dataframe\n",
        "    df = pd.DataFrame(dataframe_rows)\n",
        "    df = df[[\"doc\",\"phrase_word\",\"word\",\"POS\"]]\n",
        "    df.sort_values(by=['doc',\"phrase_word\"],inplace=True)\n",
        "    dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n",
        "    df.to_pickle(dataframe_path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "NGQes2pLwHAg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Encoding dataset...\")\n",
        "df = encode_dataset(dataset_name='dependency_treebank')\n",
        "print(\"Encoding completed!\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NejePO4qyuLg",
        "outputId": "5887002d-f89d-4c71-b61e-39ceee11cac2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199/199 [00:00<00:00, 929.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding completed!\n",
            "       doc phrase_word     word  POS\n",
            "11421    1       00_00   Pierre  NNP\n",
            "11422    1       00_01   Vinken  NNP\n",
            "11423    1       00_02        ,    ,\n",
            "11424    1       00_03       61   CD\n",
            "11425    1       00_04    years  NNS\n",
            "...    ...         ...      ...  ...\n",
            "62768  199       02_10  quarter   NN\n",
            "62769  199       02_11       of   IN\n",
            "62770  199       02_12     next   JJ\n",
            "62771  199       02_13     year   NN\n",
            "62772  199       02_14        .    .\n",
            "\n",
            "[94084 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text preprocessing/normalization"
      ],
      "metadata": {
        "id": "yMQICPaq2e0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most is already done by tokenizing the text in the creation of the first DataFrame, removing Uppercases, points and commas may not be convenient\n",
        "\n",
        "-discuss later"
      ],
      "metadata": {
        "id": "hSVCxrog2i-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@;-]')\n"
      ],
      "metadata": {
        "id": "LiI0C8SX6gKk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis, with spacing character\n",
        "    \"\"\"\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    \"\"\"\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "VZZIt_UD58f9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREPROCESSING_PIPELINE = [\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ],
      "metadata": {
        "id": "Lv9YUoxs6uIw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pre-processing text...')\n",
        "print()\n",
        "# Replace each sentence with its pre-processed version\n",
        "df['word'] = df['word'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOa8dDbL62zT",
        "outputId": "34124477-458f-4cce-bdbe-be6ee7fe87a8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing text...\n",
            "\n",
            "Pre-processing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['word'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eT1rGys7Ugg",
        "outputId": "50282d55-4c9d-43e8-bdac-97a434e85e44"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11421     pierre\n",
            "11422     vinken\n",
            "11423          ,\n",
            "11424         61\n",
            "11425      years\n",
            "          ...   \n",
            "62768    quarter\n",
            "62769         of\n",
            "62770       next\n",
            "62771       year\n",
            "62772          .\n",
            "Name: word, Length: 94084, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train/Test split"
      ],
      "metadata": {
        "id": "zZiuTuPKEhgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split in 3 different dataframes as requested"
      ],
      "metadata": {
        "id": "5vpRb5mv1fu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataframe(df: pd.DataFrame):\n",
        "\n",
        "    df_train = df[df['doc'] <= 100]\n",
        "    #folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\",\"dependency_treebank\")\n",
        "    #dataframe_path = os.path.join(folder, \"train_dataset\" + \".pkl\")\n",
        "    #df_train.to_pickle(dataframe_path)\n",
        "\n",
        "    temp_df_val = df[df['doc'] >= 101]\n",
        "    df_val = temp_df_val[temp_df_val['doc'] <= 150]\n",
        "    #dataframe_path = os.path.join(folder, \"val_dataset\" + \".pkl\")\n",
        "    #df_val.to_pickle(dataframe_path)\n",
        "\n",
        "    df_test = df[df['doc'] >= 151]\n",
        "    #dataframe_path = os.path.join(folder, \"test_dataset\" + \".pkl\")\n",
        "    #df_test.to_pickle(dataframe_path)\n",
        "\n",
        "    print(\"DataFrame splitted!\")\n",
        "\n",
        "    return df_train, df_val, df_test"
      ],
      "metadata": {
        "id": "13ib7-rxEjek"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train,df_val,df_test = split_dataframe(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Yb0fX9IZo5",
        "outputId": "afdf751f-2031-4a54-d437-b9bac20b0936"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame splitted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary creation\n"
      ],
      "metadata": {
        "id": "wq3ud_ALydhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def build_vocabulary(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "    \n",
        "    curr_idx = 0\n",
        "    for i, row in df.iterrows():\n",
        "      token = row[\"word\"]\n",
        "      if token not in word_to_idx:\n",
        "                word_to_idx[token] = curr_idx\n",
        "                idx_to_word[curr_idx] = token\n",
        "                curr_idx += 1\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "    return idx_to_word, word_to_idx, word_listing\n"
      ],
      "metadata": {
        "id": "O3FZfX2Gw_de"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_val = pd.concat([df_train,df_val])\n",
        "\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(df_train_val)\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx)}')\n",
        "print(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10) + 1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D4YwYbB0RjI",
        "outputId": "d41054ca-2d19-48ea-a7c5-d6c6765ad93b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 9898\n",
            "[Debug] Word -> Index vocabulary size: 9898\n",
            "[Debug] Some words: [('vinken', 1), (',', 2), ('61', 3), ('years', 4), ('old', 5), ('will', 6), ('join', 7), ('the', 8), ('board', 9), ('as', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GloVe embeddings"
      ],
      "metadata": {
        "id": "o8JfKZQw3HuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading GloVe model from gensim library"
      ],
      "metadata": {
        "id": "OPYx5eGz3de0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"\"\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "        \n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        print('FastText: 300')\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "metadata": {
        "id": "yG8hUccxT2i_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = load_embedding_model(model_type=\"glove\", embedding_dimension=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jjku0m2wU2m",
        "outputId": "82ccdf1d-560c-4b7f-fac5-7e28173513e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[================================================--] 97.4% 64.3/66.0MB downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking OOV terms"
      ],
      "metadata": {
        "id": "4MZF00h33rD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(embedding_model.vocab.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "B3YjC4Cr3qML"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
        "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
        "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQrn8yQV3xlD",
        "outputId": "ccfa369f-c8cd-4fcb-e9f3-c0a4b8aa9a01"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 874 (8.83%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(oov_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfblXBjI4sQ3",
        "outputId": "6e97146c-bb2c-43a3-cdd3-36adfc77046e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '2,303,328', 'freudtoy', 'four year old', 'three year', 'pre tax', 'moleculon', 'makato', '1738.1', '1992 1999', 'full time', 'secilia', 'synergistics', 'tire kickers', 'tiphook', 'ac 130u', 'senate house', 'alurralde', 'pre approved', 'red blooded', 'when issued', 'one country', 'derel', 'longer term', 'cost sharing', 'triple a', 'sogo shosha', 'product design', 'stock manipulation', 'non u.s.', 'three lawyer', 'anti takeover', 'money losing', 'ensrud', 'top selling', 'old house', 'index fund', 'food industry', 'eight month', 'car safety', 'index related', 'walbrecher', 'collective bargaining', '361,376', 'white collar', 'church goers', 'propagandizes', 'a d', 'coca cola', 'more efficient', 'shokubai', 'price support', 'subskills', '36 day', '35564.43', 'nih appointed', 'besuboru', 'reagan bush', 'trading company', '11,762', '50 state', 'two year old', '1.5755', '37 a share', 'short wave', 'water authority', 'chicago style', 'bermuda based', 'band wagon', '20 point', '5\\\\ 8', '230 215', 'jerritts', 'change ringing', 'six bottle', 'citizen sparked', 'side crash', 'norwick', 'renaissance style', 'training wage', 'sell offs', 'drop in', 'automotive lighting', 'front seat', 'two sevenths', '300 113', 'mega stadium', 'college bowl', 'unenticing', 'round trip', '446.62', 'high rate', 'subskill', 'battery operated', 'francisco based', '12\\\\ 32', 'one day', 'u.s. japanese', 'bald faced', '1928 33', 'merger related', 'one upsmanship', 'phacoflex', 'student test', 'safe deposit', 'two tiered', 'double c', '2141.7', 'non core', 'five year', 'two thirds', 'building products', 'mortgage based', 'centerbank', 'lafite rothschild', 'low cost', '1\\\\ 10th', 'clean air', 'free enterprise', 'jalaalwalikraam', 'rey\\\\ fawcett', 'deposits a', 'nesb', 'dollar yen', 'solaia', '1986 87', 'bank backed', '12 year', 'joint venture', 'year long', 'biondi santi', 'good hearted', 'test preparation', 're thought', 'low altitude', 'tie in', '415.6', 'mind boggling', 'short lived', 'would be', 'uzi model', 'cotran', 'school research', 'contingency fee', '30 point', 'four year', 'land idling', 'school improvement', '25 year old', 'dana farber', 'travel related', 'stock index', 'macmillan\\\\ mcgraw', 'low priced', 'catch up', '14,821', '54 year old', '1\\\\ 2', '877,663', 'u.s. japan', 'subminimum', 'duty free', 'egnuss', '5.276', 'nagymaros', 'home market', '240 page', 'wamre', 'dollar denominated', 'pre emptive', '3648.82', 'sino u.s.', '2 8', 'english speaking', 'mininum wage', '90 cent an hour', 'meinders', 'odd sounding', 'year to year', 'hard hitting', 'pre 1917', 'pianist comedian', '300 a share', '13\\\\ 16', 'new home', '2163.2', 'certin', 'short term', 'buy back', 'fixed income', 'how to', 'language housekeeper', 'sulfur dioxide', 'market based', 'acid rain', '51 year old', 'crocidolite', 'bumkins', 'forest products', 't shirts', 'house senate', 'well connected', '62 year old', 'old style', '12 point', 'integra a', 'incentive backed', '7\\\\ 16', '16\\\\ 32', 'co chairman', 'pension fund', 'high polluting', 'buttoned down', 'car care', 'haut brion', 'investment grade', 'herald american', 'express buick', 'agreed upon', 'foreign led', 'electrical safety', 'trs 80', 'heiwado', 'boorse', 'three digit', 'mutchin', '9,118', 'high technology', 'scypher', 'direct mail', 'anku', 'nine member', '16,072', 'prudential bache', 'second largest', 'seven year', 'forest product', 'mcgraw hill', 'rear seat', 'on campus', '236.79', 'pennview', 'c 90', 'test drive', 'takeover stock', '10 lap', 'akerfeldt', 'day care', 'off off', 'marie louise', '271 147', 'lookee loos', 'weisfield', 'over the counter', 'purhasing', 'ctbs', '142.85', 'northy', 'talk show', 'retin a', '8.575', 'recession inspired', 'three sevenths', 'sometimes exhausting', 'arbitraging', '2\\\\ 32', 'autions', 'market share', 'malizia', 'profit taking', 'sharedata', 'anti drug', 'super absorbent', 'teacher cadet', 'cup tote', '62.625', '7.458', 'cop killer', 'modern day', 'third highest', 'sidak', 'trettien', 'pre existing', '374.20', 'kalipharma', 'built from kit', '737.5', 'index arbitrage', 'common law', 'tie breaking', 'write downs', 'subindustry', '271,124', 'chemplus', 'executive office', 'computer system design', 'american style', 'open top', 'ingersoll rand', '23,403', '520 lawyer', '10 day', 'small time', 'samnick', 'industry supported', 'top yielding', 'self perpetuating', 'circuit breaker', '18 year old', 'pick up', '71,309', '30 day', 'three quarters', 'anti abortion', 'multi crystal', 'best selling', 'superdot', '415.8', 'yen denominated', 'major league', 'cast iron', 'rope sight', 'wine making', 'low ability', 'savings and loan', 'cancer causing', 'colonsville', 'social studies', 'export oriented', 'we japanese', 'program trading', 'capital markets', 'sub markets', 'razor thin', '2003\\\\ 2007', '500 stock', '12 member', '40 year old', '29year', 'custom chip', 'two year', '352.9', 'high risk', 'purepac', 'record keeping', 'video viewing', '69 point', 'enzor', 'capital gains', 'family planning', 'futures related', '64 year old', 'breakey', 'index options', 'single handed', 'one third', 'savers\\\\ investors', 'consumer driven', 'telephone information', 'drag down', 'micronite', '436.01', 'hardest hit', 'father in law', 'fixed rate', 'minneapolis based', 'fastest growing', 'higher salaried', 'light truck', 'muscolina', 'mortgage backed', '75 year old', '11\\\\ 16', '30,537', 'high priced', 'iran contra', 'potables', 'bridgestone\\\\ firestone', 'hummerstone', '17 year old', 'anti abortionists', 'odd year', '9\\\\ 32', 'machine gun toting', 'worst case', 'cash rich', 'substance abusing', '22\\\\ 32', 'closed end', '90 day', 'nipponese', 'red flag', 'jersey based', 'fetal tissue', 'after tax', 'state supervised', 'one hour', '6\\\\ 2', 'minicrash', 'drop off', 'coche dury', 'auto safety', '3057', 'less serious', '18,444', 'financial services', '14\\\\ 32', '234.4', 'lend lease', '58 year old', 'multibillion dollar', 'one week', '70 a share', 'atlanta based', 'cash flow', 'yen support', '1.457', 'big ticket', 'two letter', 'achievement test', 'medium sized', 'dydee', 'double a', 'anti miscarriage', 'housing assistance', 'roof crush', 'anti china', 'test prep', 'third quarter', 'start up', 'ntg', 'bell ringing', 'co founded', 're election', '15 day', '20 stock', 'nylev', 'life insurance', 'stock specialist', 'sanderoff', 'one yen', 'macheski', 'ft se', 'wtd', 'floating rate', 'flightiness', 'year end', 'times stock', 'landonne', 'incentive bonus', 'page one', 'chinese american', 'yeargin', 'rubinfien', 'one year', 'limited partnership', 'single digit', \"creator's\", 'well known', 'tissue transplant', 'mehrens', 'morale damaging', 'most likely successor', 'catch 22', '82,389', 'pricings', 'computer assisted', 'abortion related', 'univest', 'ednie', '143.93', 'prevalance', 'triple c', 'asset sale', '1937 40', 'old fashioned', 'one newspaper', 'property\\\\ casualty', 'erbamont', '8300s', 'post hearing', 'search and seizure', 'investor relations', 'seven day', 'sometimes tawdry', 'greenmailer', 'so called', 'intecknings', 'city owned', 'rent a car', '52 week', '13.625', 'bellringers', 'fifth grade', 'war damaged', '3\\\\ 8', 'male only', 'pre cooked', 'highest pitched', 'five inch', 'nbc owned', 'first rate', 'arighi', 'mutual fund', '278.7', 'iran\\\\ contra', 'inner city', '1.8415', '14 hour', 'durable goods', 'direct investment', '84 month', '372.9', 'continuingly', 'aslacton', 'long term', 'corporate wide', 'current carrying', '4.898', 'nekoosa', 'carnegie mellon', '352.7', 'get out the vote', 'glenham', 'ballantine\\\\ del', 'twindam', 'test practice', 'sports oriented', 'fiber end', 'pramual', 'co owner', 'junk bond', 'no smoking', 'follow up', 'superpremiums', '63 year old', 'anti program', 'self aggrandizing', 'low tech', '2645.90', 'buy out', 'industry wide', 'five point', 'c.j.b.', 'pro forma', 'seven million ton', 'freshbake', 'non religious', 'passers by', 'one house', 'government certified', 'four day', 'intellectual property', 'bell ringer', 'severable', 'small company', 'good faith', 'four color', 'free lance', 'marketing communications', '456.64', 'food shop', 'energy services', 'hard charging', 'corton charlemagne', 'high volume', 'cost benefit', 'red carpet', 'london based', 'pathlogy', 'trockenbeerenauslesen', '14.', 'high tech', 'price depressing', 'government funded', 'gates warren', 'foldability', 'satrum', 'non biodegradable', 'market makers', 'dead eyed', 'state owned', 'cleaner burning', 'middle ground', '1987 88', '8.467', 'equal opportunity', '84 year old', 'ghkm', 'off year', 'do it yourself', 'antitrust law', 'insurance company', 'midwesco', 'anti deficiency', 'computer generated', 'hart scott rodino', 'rate sensitive', '3 4', '62% owned', 'six packs', 'tarwhine', 'blue collar', '12,252', 'toronto based', 'high grade', 'co developers', 'monchecourt', 'money market', 'one month', 'chafic', 'mid 1990s', 'coal fired', 'double digit', 'life style', '1991 2000', 'york based', 'boogieman', 'needle like', 'then speaker', 'calif. based', 'self serving', 'single family', 'good natured', 'veselich', 'lightning fast', 'drobnick', 'stock market', 'delwin', 'dust up', 'male dominated', '238,000 circulation', 'stock price', 'eight count', 'single handedly', 'nearly 30', 'rexinger', '4,393,237', 'pre communist', 'truth in lending', 'wheeland', 'automotive parts', 'circuit board', 'beer belly', 'liquid nitrogen', 'equity purchase', 'gut wrenching', 'securities based', 'flim flammery', 'georgia pacific', '3\\\\ 4', 'billion dollar', 'computer aided', 'mid october', '3,040,000', 'summer\\\\ winter', 'revenue desperate', '150 point', 'fast growing', 'sub segments', 'diceon', 'school board', 'insider trading', 'wine buying', '30 share', 'sport utility', 'hallwood', 'buy outs', 'labor management', '1\\\\ 4', 'interest rate', 'lezovich', 'identity management', 'nissho iwai', 'red and white', '3,250,000', 'high quality', 'car development', 'large scale', 'heavy duty', 'open end', 'stock picking', 'war rationed', 'broad based', 'high yield', 'mid 1970s', 'crystal lattice', 'pro democracy', 'lower priority', 'bottom line', 'year ago', 'sticker shock', 'big time', 'hard drinking', '95.09', 'stirlen', 'credit rating', '16.125', 'stock selection', 'line item', 'ratners', '3,288,453', 'single a', 'lynch mob', 'labor backed', 'electric utility', 'top level', 'clean up', 'three month', 'high balance', 'blue chip', '37 year old', 'fiber optic', 'half hour', '2691.19', 'yttrium containing', 'veraldi', '13,056', 'thin lipped', '47.125', 'word processing', 'rapanelli', '127.03', 'long time', '2160.1', 'one time', 'long tenured', 'chong sik', 'larger than normal', 'melt textured', 'test coaching', 'washington based', 'first time', 'fast food', 'news american', 'anti programmers', 'year earlier', 'computer driven', 'self esteem', '30,841', '143.08', '53 year old', 'strong willed', '374.19', 'six inch', 'off track', 'six month', 'best seller', 'non callable', 'high rise', 'non farm', 'philadelphia based', '1\\\\ 8', 'five day', 'ariail', '7\\\\ 8', 'public relations', '30 year', 'much larger', '100,980', '59 year old', 'replacement car', 'f series', 'high stakes', 'american made', 'old time', 'dai ichi', 'shirt sleeved', '500,004', '31 year old', '497.34', 'security type', 'single lot', '38.375', 'high level', 'triple a rated', 'million a year', 'sacramento based', '143.80', '100 share', 'wfrr', 'four foot high', 'vinken', 'gingl', 'vitulli', 'cray 3', 'heebie jeebies', '190 point', 'twin jet', 'asbestos related', 'n.j. based', 'disaster assistance', 'school sponsored', 'run down', 'radio station', 'risk free', 'amphobiles', 'nine year', 'airline related', 'interest bearing', 'wheel loader', 'pro choice', '5.435', 'close up', 'high speed', 'macmillan\\\\ mcgraw hill', '7.422', 'mid size', 'assembly line', 'near record', '30 minute', 'preparatives', 'prize fighter', 'fourth quarter', 'full length', 'market oriented', 'mo. based', 'world wide', 'low ball', '3.253', 'letter writing', 'parts engineering', '21 month', '10 year', '35500.64', 'self regulatory', '36 minute', '236.74', 'anti morning sickness', 'decade long', 'labor intensive', '1973 75', 'detective story', 'spin off', 'school district', 'lap shoulder', 'non encapsulating', 'court ordered', 'industrial production', 'two week', 'nine month', '1991 1999', 'mouth up', 'cost effective', 'chilver', 'two time losers', 'motor home', 'new car', 'c.d.s', 'pattenden', 'pre 1933', 'crane safety', '705.6', 'romanee conti', 'real estate', 'less than brilliant', '7.272', 'black and white', 'foreign stock', 'card member', '382 37', 'polyproplene', 'money center', 'day to day', 'attorney client', '95,142', 'chinchon', 'money fund', '55 year old', 'policy making', 'n.v', '449.04', 'quantitive', 'unfair trade']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q6E2f2924eUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentences split"
      ],
      "metadata": {
        "id": "JXZjVkZQvwMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequences(df):\n",
        "    new_data = {\"sentences\": [], \"pos_tags\": []}\n",
        "\n",
        "    sentence = []\n",
        "    pos = []\n",
        "\n",
        "    for i,row in df.iterrows():\n",
        "\n",
        "       if (not row[\"word\"] == \"?\" and\n",
        "           not row[\"word\"] == \"!\" and\n",
        "           not row[\"word\"] == \".\"):\n",
        "\n",
        "        sentence.append(row[\"word\"])\n",
        "        pos.append(row[\"POS\"])\n",
        "      \n",
        "       else:\n",
        "\n",
        "        sentence.append(row[\"word\"])\n",
        "        pos.append(row[\"POS\"])\n",
        "\n",
        "        new_data[\"sentences\"].append(sentence)\n",
        "        new_data[\"pos_tags\"].append(pos)\n",
        "\n",
        "        sentence = []\n",
        "        pos = []\n",
        "      \n",
        "    new_df = pd.DataFrame(new_data, columns=[\"sentence\",\"pos_tags\"])\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "B-C--gf7cry0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_s = build_sequences(df_train)\n",
        "df_val_s = build_sequences(df_val)\n",
        "df_test_s = build_sequences(df_test)"
      ],
      "metadata": {
        "id": "6dQdoZaae4ad"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JufqYUQ5rRFc"
      }
    }
  ]
}