{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pranavphoenix/BiLSTM-POS-Tagging/blob/main/BiLSTM_POS_Tagging.ipynb\n",
        "\n",
        "TODO:\n",
        "\n",
        "- evitare di utilizzare la punctuation nel calcolo delle metriche, magari utilizzando l'array di pesi 'sample_weight' che si trova nell'altro notebook;\n",
        "\n",
        "- Provare se i risultati migliorano con preprocessing (e.g. lowerando le parole)+ preprocessing di termini strani;\n",
        "- Aggiustare il notebook perch√© fa cagare;"
      ],
      "metadata": {
        "id": "013gFxfmSmXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxL2umB0D19Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the dataset\n",
        "nltk.download('treebank')\n",
        "\n",
        "data = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "#Downloading Glove Word Embeddings\n",
        "pbar = None\n",
        "def show_progress(block_num, block_size, total_size):\n",
        "    global pbar\n",
        "    if pbar is None:\n",
        "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
        "        pbar.start()\n",
        "\n",
        "    downloaded = block_num * block_size\n",
        "    if downloaded < total_size:\n",
        "        pbar.update(downloaded)\n",
        "    else:\n",
        "        pbar.finish()\n",
        "        pbar = None\n",
        "\n",
        "# Download the GloVe embeddings file\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "urllib.request.urlretrieve(url, 'glove.6B.zip', show_progress)\n",
        "\n",
        "# Extract the zip file\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUWmRON8D80m",
        "outputId": "73636cf2-cbbb-4ff6-a971-0f91ffe4753a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "100% (862182613 of 862182613) |##########| Elapsed Time: 0:02:44 Time:  0:02:44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ignore the 0 padding while calculating accuracy\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "#Function to return one code encoding of tags\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "#Function to convert output into tags\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "metadata": {
        "id": "DiU83wn8FMQd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the files' list\n",
        "fileids = nltk.corpus.treebank.fileids()\n",
        "\n",
        "# Get the Penn Treebank corpus and tokenize the text\n",
        "train_corpus = nltk.corpus.treebank.tagged_sents(fileids[:100])\n",
        "val_corpus = nltk.corpus.treebank.tagged_sents(fileids[100:150])\n",
        "test_corpus = nltk.corpus.treebank.tagged_sents(fileids[150:])\n",
        "\n",
        "# Flatten the lists\n",
        "ignore = [':', '\"', '-LRB-', '-RRB-', ',', '.', \"''\", '``', 'SYM']\n",
        "\n",
        "# train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "# test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist if item[1] != '-NONE-']\n",
        "\n",
        "train_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(train_corpus) for item in sublist]\n",
        "val_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(val_corpus) for item in sublist]\n",
        "test_corpus = [tuple(list(item)+[str(idx)]) for idx,sublist in enumerate(test_corpus) for item in sublist]\n",
        "\n",
        "train_df = pd.DataFrame(train_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "val_df = pd.DataFrame(val_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "test_df = pd.DataFrame(test_corpus, columns = ['word', 'tag', 'sentence'])\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "-ulx6UMBHint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c986b34-f564-4a72-dfc2-1306d7af45fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         word     tag sentence\n",
            "0      Pierre     NNP        0\n",
            "1      Vinken     NNP        0\n",
            "2           ,       ,        0\n",
            "3          61      CD        0\n",
            "4       years     NNS        0\n",
            "...       ...     ...      ...\n",
            "50743      he     PRP     1962\n",
            "50744     has     VBZ     1962\n",
            "50745   faced     VBN     1962\n",
            "50746   *T*-2  -NONE-     1962\n",
            "50747       .       .     1962\n",
            "\n",
            "[50748 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags_train = sorted(list(set([x for x in train_df.tag])))\n",
        "tags_val = sorted(list(set([x for x in val_df.tag])))\n",
        "tags_test = sorted(list(set([x for x in test_df.tag])))\n",
        "\n",
        "print('Train tags number:',len(tags_train))\n",
        "print('Val tags number:',len(tags_val))\n",
        "print('Test tags number:',len(tags_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6MOWORPI8Va",
        "outputId": "f83fea76-7d4f-44b1-abc8-a7d61c9ee209"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tags number: 46\n",
            "Val tags number: 45\n",
            "Test tags number: 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary"
      ],
      "metadata": {
        "id": "q32n9MjbqC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Vocabulary (V1)"
      ],
      "metadata": {
        "id": "jOf1nvNsqGfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the 300 dimensional GLove Word Embeddings\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index =  {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "675MhAFGFQNj",
        "outputId": "d81acf60-02de-4781-b274-78b5c30cffb2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 + Training set OOV (V2)"
      ],
      "metadata": {
        "id": "lxxunk3yqOzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "def update_vocab(df,embeddings_index,embedding_dim):\n",
        "  oov_c = 0\n",
        "  for word in df.word:\n",
        "    if word not in embeddings_index:\n",
        "      oov_c += 1\n",
        "      random_embed = np.random.rand(embedding_dim)\n",
        "      embeddings_index[word] = random_embed\n",
        "  print(\"Added\",oov_c,\"OOV words + respective embeddings to the vocabulary.\")\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = update_vocab(train_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "id": "NuseO3gBjoN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff252b4-6152-40f2-b3cf-7ac3b12d5701"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 2777 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2 + Validation set OOV (V3)"
      ],
      "metadata": {
        "id": "_2RFWZLCtXZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(val_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkSiNQkytU0B",
        "outputId": "b60d022c-a6e0-4e7d-e661-a9e78c07e71f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 951 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V3 + Test set OOV (V4)"
      ],
      "metadata": {
        "id": "JIW5jaKdt0Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = update_vocab(test_df,embeddings_index,embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSoqiGXt4qG",
        "outputId": "3f6ca4f0-2186-4b99-a99b-34864cd80b81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 455 OOV words + respective embeddings to the vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the actual word vocabulary\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "idx2word = OrderedDict()\n",
        "word2idx = OrderedDict()\n",
        "    \n",
        "curr_idx = 0\n",
        "for key in embeddings_index.keys():\n",
        "  word2idx[key] = curr_idx\n",
        "  idx2word[curr_idx] = key\n",
        "  curr_idx += 1\n",
        "\n",
        "#word_listing = list(idx2word.values())\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx2word)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF0VtzwbBMXO",
        "outputId": "9fa38162-e30b-4be7-febd-82b4f329063e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 404183\n",
            "[Debug] Word -> Index vocabulary size: 404183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag vocabulary\n",
        "\n",
        "tag2idx = OrderedDict()\n",
        "\n",
        "curr_id = 0\n",
        "\n",
        "for tag in tags_train:\n",
        "  tag2idx[tag] = curr_id\n",
        "  curr_id += 1\n",
        "\n",
        "print(f'[Debug] Tag -> Index vocabulary size: {len(tag2idx)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaLHdqXbDIzS",
        "outputId": "514aa074-d992-48fa-bd75-857b29549f83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Tag -> Index vocabulary size: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "conf_matrix = []\n",
        "\n",
        "tag_list = tags_train\n",
        "# The integers for each tag are the same as above\n",
        "\n",
        "MAX_LENGTH = max([len(seq) for seq in data]) # maximum words in a sentence\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=tag_list, index=tag_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)"
      ],
      "metadata": {
        "id": "PZFriFx5Fpde"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "val_sentences = val_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "test_sentences = test_df.groupby('sentence').word.apply(list).reset_index()['word']\n",
        "\n",
        "train_tags = train_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "val_tags = val_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "test_tags = test_df.groupby('sentence').tag.apply(list).reset_index()['tag']\n",
        "\n",
        "true_pos_tag = defaultdict(int)\n",
        "false_pos_tag = defaultdict(int)\n",
        "false_neg_tag = defaultdict(int)\n",
        "precision_tags = defaultdict(float)\n",
        "accuracy_tags = defaultdict(float)\n",
        "recall_tags = defaultdict(float)\n",
        "f1score_tags = defaultdict(float)\n"
      ],
      "metadata": {
        "id": "WuLsYIaYIBgn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, val_sentences_X, train_tags_y, test_tags_y, val_tags_y = [], [], [], [], [], []\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "   \n",
        "    train_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in val_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    val_sentences_X.append(sent_int)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sent_int = []\n",
        "    for word in sentence:\n",
        "        sent_int.append(word2idx[word])\n",
        "\n",
        "    test_sentences_X.append(sent_int)\n",
        "\n",
        "for sent_tags in train_tags:\n",
        "    train_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in val_tags:\n",
        "    val_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "for sent_tags in test_tags:\n",
        "    test_tags_y.append([tag2idx[tag] for tag in sent_tags])\n",
        "\n",
        "#Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "val_tags_y = pad_sequences(val_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "tbPprk74bhkC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    A weighted version of keras.objectives.categorical_crossentropy\n",
        "    \n",
        "    Variables:\n",
        "        weights: numpy array of shape (C,) where C is the number of classes\n",
        "    \n",
        "    Usage:\n",
        "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
        "        loss = weighted_categorical_crossentropy(weights)\n",
        "        model.compile(loss=loss,optimizer='adam')\n",
        "    \"\"\"\n",
        "    \n",
        "    weights = K.variable(weights)\n",
        "        \n",
        "    def loss(y_true, y_pred):\n",
        "        # scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        # calc\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ],
      "metadata": {
        "id": "gOiEcOse0xVr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tag2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMukxI5xdZyz",
        "outputId": "f0825d72-79ca-4ae8-c318-dfd8e78223e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('#', 0), ('$', 1), (\"''\", 2), (',', 3), ('-LRB-', 4), ('-NONE-', 5), ('-RRB-', 6), ('.', 7), (':', 8), ('CC', 9), ('CD', 10), ('DT', 11), ('EX', 12), ('FW', 13), ('IN', 14), ('JJ', 15), ('JJR', 16), ('JJS', 17), ('LS', 18), ('MD', 19), ('NN', 20), ('NNP', 21), ('NNPS', 22), ('NNS', 23), ('PDT', 24), ('POS', 25), ('PRP', 26), ('PRP$', 27), ('RB', 28), ('RBR', 29), ('RBS', 30), ('RP', 31), ('SYM', 32), ('TO', 33), ('UH', 34), ('VB', 35), ('VBD', 36), ('VBG', 37), ('VBN', 38), ('VBP', 39), ('VBZ', 40), ('WDT', 41), ('WP', 42), ('WP$', 43), ('WRB', 44), ('``', 45)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.array([1,1,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0])\n",
        "loss = weighted_categorical_crossentropy(weights)\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW4MdSIRNdn3",
        "outputId": "966fa95b-f6df-4495-8a01-e7b7e3506700"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < len(word2idx):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#Building the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, ))) \n",
        "model.add(Embedding(len(word2idx), 300, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2idx))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss=loss, optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_tags_y = one_hot_encoding(train_tags_y, len(tag2idx))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA776HlcMC_h",
        "outputId": "39a0f296-806f-4f44-b59b-4542011f9b56"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 271, 300)          121254900 \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 271, 512)         1140736   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 271, 46)          23598     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 271, 46)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122,419,234\n",
            "Trainable params: 1,164,334\n",
            "Non-trainable params: 121,254,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, one_hot_encoding(train_tags_y, len(tag2idx)),\\\n",
        "          validation_data=(val_sentences_X, one_hot_encoding(val_tags_y, len(tag2idx))),\n",
        "          batch_size=128, epochs= 15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8Dsz2oYyiO",
        "outputId": "8c809bf9-19a9-4c77-f0dc-c5cc6394e1bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "16/16 [==============================] - 103s 6s/step - loss: 0.6496 - accuracy: 0.8501 - ignore_accuracy: 0.3408 - val_loss: 0.2659 - val_accuracy: 0.9193 - val_ignore_accuracy: 0.1736\n",
            "Epoch 2/15\n",
            "16/16 [==============================] - 92s 6s/step - loss: 0.2429 - accuracy: 0.9244 - ignore_accuracy: 0.2204 - val_loss: 0.2240 - val_accuracy: 0.9301 - val_ignore_accuracy: 0.2722\n",
            "Epoch 3/15\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.2102 - accuracy: 0.9339 - ignore_accuracy: 0.3302 - val_loss: 0.2027 - val_accuracy: 0.9353 - val_ignore_accuracy: 0.3326\n",
            "Epoch 4/15\n",
            "16/16 [==============================] - 89s 6s/step - loss: 0.1897 - accuracy: 0.9403 - ignore_accuracy: 0.3895 - val_loss: 0.1828 - val_accuracy: 0.9439 - val_ignore_accuracy: 0.4154\n",
            "Epoch 5/15\n",
            "16/16 [==============================] - 91s 6s/step - loss: 0.1694 - accuracy: 0.9459 - ignore_accuracy: 0.4490 - val_loss: 0.1618 - val_accuracy: 0.9487 - val_ignore_accuracy: 0.4678\n",
            "Epoch 6/15\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.1487 - accuracy: 0.9506 - ignore_accuracy: 0.4993 - val_loss: 0.1413 - val_accuracy: 0.9534 - val_ignore_accuracy: 0.5192\n",
            "Epoch 7/15\n",
            "16/16 [==============================] - 90s 6s/step - loss: 0.1294 - accuracy: 0.9561 - ignore_accuracy: 0.5550 - val_loss: 0.1238 - val_accuracy: 0.9581 - val_ignore_accuracy: 0.5690\n",
            "Epoch 8/15\n",
            "16/16 [==============================] - 90s 6s/step - loss: 0.1130 - accuracy: 0.9609 - ignore_accuracy: 0.6068 - val_loss: 0.1093 - val_accuracy: 0.9621 - val_ignore_accuracy: 0.6092\n",
            "Epoch 9/15\n",
            "16/16 [==============================] - 91s 6s/step - loss: 0.0998 - accuracy: 0.9642 - ignore_accuracy: 0.6407 - val_loss: 0.0979 - val_accuracy: 0.9651 - val_ignore_accuracy: 0.6434\n",
            "Epoch 10/15\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.0894 - accuracy: 0.9669 - ignore_accuracy: 0.6680 - val_loss: 0.0888 - val_accuracy: 0.9673 - val_ignore_accuracy: 0.6659\n",
            "Epoch 11/15\n",
            "16/16 [==============================] - 93s 6s/step - loss: 0.0810 - accuracy: 0.9691 - ignore_accuracy: 0.6880 - val_loss: 0.0815 - val_accuracy: 0.9690 - val_ignore_accuracy: 0.6802\n",
            "Epoch 12/15\n",
            "16/16 [==============================] - 89s 6s/step - loss: 0.0743 - accuracy: 0.9707 - ignore_accuracy: 0.7041 - val_loss: 0.0757 - val_accuracy: 0.9704 - val_ignore_accuracy: 0.6931\n",
            "Epoch 13/15\n",
            "16/16 [==============================] - 90s 6s/step - loss: 0.0688 - accuracy: 0.9720 - ignore_accuracy: 0.7161 - val_loss: 0.0709 - val_accuracy: 0.9714 - val_ignore_accuracy: 0.7019\n",
            "Epoch 14/15\n",
            "16/16 [==============================] - 90s 6s/step - loss: 0.0642 - accuracy: 0.9730 - ignore_accuracy: 0.7257 - val_loss: 0.0671 - val_accuracy: 0.9726 - val_ignore_accuracy: 0.7192\n",
            "Epoch 15/15\n",
            "16/16 [==============================] - 90s 6s/step - loss: 0.0604 - accuracy: 0.9739 - ignore_accuracy: 0.7359 - val_loss: 0.0638 - val_accuracy: 0.9734 - val_ignore_accuracy: 0.7214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_tags_y, len(tag2idx)))\n",
        "acc.append(scores[2]*100)\n",
        "\n",
        "\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in tag2idx.items()})\n",
        "#y_prob_class = model.predict_classes(test_sentences_X, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q4shasqRWUk",
        "outputId": "056356f8-50c1-4667-f199-11ea8759372f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 16s 760ms/step - loss: 0.0566 - accuracy: 0.9758 - ignore_accuracy: 0.7485\n",
            "21/21 [==============================] - 14s 638ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ignore = [':', '-LRB-', '-RRB-', ',', '.', \"''\", '``']\n",
        "for sen_num in range(len(test_tags)):\n",
        "    for i,tag in enumerate(test_tags[sen_num]):\n",
        "      \n",
        "        conf_mat_df[tag][pred_sequence[sen_num][i]] +=1\n",
        "        if test_tags[sen_num][i] == pred_sequence[sen_num][i]:\n",
        "          true_pos_tag[tag] += 1\n",
        "        else:\n",
        "          false_neg_tag[tag] += 1\n",
        "          false_pos_tag[pred_sequence[sen_num][i]] += 1\n",
        "\n",
        "for tag in tag_list[1:]:\n",
        "  if tag not in ignore:\n",
        "    if (true_pos_tag[tag] + false_pos_tag[tag]) != 0:\n",
        "      print(tag)\n",
        "      precision_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_pos_tag[tag])\n",
        "      recall_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag])\n",
        "      f1score_tags[tag] = 2 * precision_tags[tag] * recall_tags[tag] / (precision_tags[tag] + recall_tags[tag])\n",
        "      accuracy_tags[tag] = true_pos_tag[tag] / (true_pos_tag[tag] + false_neg_tag[tag] + false_pos_tag[tag])"
      ],
      "metadata": {
        "id": "ldKQf4WCRqrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "8718f618-639e-400c-dd36-8915c9fa98f7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$\n",
            "-NONE-\n",
            "CC\n",
            "CD\n",
            "DT\n",
            "IN\n",
            "JJ\n",
            "JJR\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-43bb208d047a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprecision_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mrecall_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_neg_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mf1score_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0maccuracy_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_neg_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfalse_pos_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(f1score_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAGbZGkvkY42",
        "outputId": "47c08265-d17f-4423-dfca-be0e4a586921"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for el in f1score_tags.items():\n",
        "  sum += el[1]\n",
        "print(sum/len(f1score_tags.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QzL7xzlOTbq",
        "outputId": "6e339542-cbd0-4d08-b0f8-efab902f50a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8031544390336744\n"
          ]
        }
      ]
    }
  ]
}