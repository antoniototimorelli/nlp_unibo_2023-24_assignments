# -*- coding: utf-8 -*-
"""Edo_first.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zygsavCB-33EBVaA-uCBDtDWaWyBUZ3A
"""

import pandas as pd
import random
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
import warnings
import os
import transformers
from transformers import AutoTokenizer, TFAutoModel,TFBertForSequenceClassification, TFAutoModelForSequenceClassification,TFBertMainLayer,TFBertPreTrainedModel, PreTrainedTokenizerFast
warnings.filterwarnings("ignore")
from tensorflow.keras import layers, models
from sklearn.metrics import f1_score, accuracy_score, classification_report, multilabel_confusion_matrix, ConfusionMatrixDisplay, PrecisionRecallDisplay
import random
import os



#from google.colab import files
#import zipfile


#uploaded = files.upload()


#zip_filename = next(iter(uploaded))


#with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
#    zip_ref.extractall('/content/')

#path_labels_data = "./labels_data"
#path_arguments_data = "./arguments_data"

#path_labels_data = ".\labels_data"
#path_arguments_data = ".\\arguments_data"

def set_reproducibility(seed):
    tf.random.set_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
    transformers.set_seed(seed)
    tf.config.threading.set_inter_op_parallelism_threads(1)
    tf.config.threading.set_intra_op_parallelism_threads(1)

set_reproducibility(42)

path_labels_data = ".\labels_data"
path_arguments_data = ".\\arguments_data"

"""# 1. Corpus"""

def load_file():
    '''Load the file and return the data and labels for training, testing and validation'''

    train_data = pd.read_csv(os.path.join(path_arguments_data, 'arguments-training.tsv'), sep='\t')
    test_data = pd.read_csv(os.path.join(path_arguments_data, 'arguments-test.tsv'), sep='\t')
    valid_data = pd.read_csv(os.path.join(path_arguments_data, 'arguments-validation.tsv'), sep='\t')
    train_labels = pd.read_csv(os.path.join(path_labels_data, 'labels-training.tsv'),sep='\t')
    test_labels = pd.read_csv(os.path.join(path_labels_data, 'labels-test.tsv'),sep='\t')
    valid_labels = pd.read_csv(os.path.join(path_labels_data, 'labels-validation.tsv'),sep='\t')

    return (train_data,train_labels), (test_data,test_labels), (valid_data,valid_labels)




(train_data,train_labels), (test_data,test_labels), (valid_data,valid_labels) = load_file()

print("Trainig data shape: ", train_data.shape)
print("Validation data shape: ", valid_data.shape)
print("Test data shape: ", test_data.shape)

print("Level 2 Classes: ", len(train_labels.columns[1:]))

train_data.head()

train_labels.head()

train_labels.columns

def merge_df(data, labels):
    return pd.merge(data, labels, on='Argument ID')

l2_train_df = merge_df(train_data, train_labels)
l2_test_df = merge_df(test_data, test_labels)
l2_valid_df = merge_df(valid_data, valid_labels)

l2_train_df.shape

datasets = {"Training":l2_train_df, "Validation":l2_valid_df, "Test":l2_test_df}
for i,key in enumerate(datasets.keys()):
    print(f"Number of null values in {key} set: ", datasets[key].isnull().sum().sum())

for column in l2_train_df.iloc[:,4:].columns:
    print(f"Unique values taken by \"{column}\" label: {np.unique(l2_train_df[column].values)}, number = {l2_train_df[column].nunique()}")

l2_train_df.head()

level2_to_level3 = {
    'Openness to change':["Self-direction: thought", "Self-direction: action","Stimulation", "Hedonism"],
    "Self-enhancement":['Hedonism','Achievement', 'Power: dominance', 'Power: resources','Face'],
    "Conservation": ['Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal','Humility'],
    "Self-transcendence":['Humility', 'Benevolence: caring', 'Benevolence: dependability','Universalism: concern', 'Universalism: nature','Universalism: tolerance', 'Universalism: objectivity']
}

counts_train = l2_train_df.iloc[:,4:].eq(1).sum()
total_counts_train = counts_train.sum()
print("Count of Level 2 category in Training Dataset: \n\n{}".format(counts_train))
print("\nTotal count of Level 2 categories in Training Dataset: {}".format(total_counts_train))

def plot_l2_to_l3_count(df:pd.DataFrame, title):
    '''Plot the histogram of the labels'''
    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15,6 ))
    colors = random.sample(list(mcolors.TABLEAU_COLORS.keys()), 4)

    for i, (label, subclasses) in enumerate(level2_to_level3.items()):
        ax = axes[i]
        df[subclasses].eq(1).sum().plot(kind='bar', ax=ax, color=colors[i])
        ax.set_title(f'{label}')
        ax.set_xlabel('Subclasses')
        ax.set_ylabel('Count')
        total_count = df[subclasses].eq(1).sum().sum()
        ax.text(0.5, 1.11, f'Total Count: {total_count}', transform=ax.transAxes, ha='center')

    plt.suptitle(f"Level 2 Categories count within their respective Level 3 Categories: {str.upper(title)} DATA",fontsize=14)
    plt.tight_layout()
    plt.show()

plot_l2_to_l3_count(l2_train_df, "Training")
plot_l2_to_l3_count(l2_test_df, "Test")
plot_l2_to_l3_count(l2_valid_df, "Validation")

l2_train_df.describe()

def get_level3_labels_df(df,labels_df):
    label3_df = labels_df[['Argument ID']].copy()

    for label3, labels2 in level2_to_level3.items():
        start_label = labels2[0]
        end_label = labels2[-1]
        label3_df[label3] = (labels_df.loc[:,start_label:end_label].sum(axis=1) > 0).astype(int)

    l3_df = pd.merge(df, label3_df, on='Argument ID')

    return l3_df

train_df = get_level3_labels_df(train_data,train_labels)
test_df = get_level3_labels_df(test_data,test_labels)
valid_df = get_level3_labels_df(valid_data,valid_labels)
train_df.head()

train_df.shape

def plot_label_count(df: pd.DataFrame, title:str):
    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 6))
    colors = random.sample(list(mcolors.TABLEAU_COLORS.keys()), 4)
    labels = list(level2_to_level3.keys())


    for i,label in enumerate(labels):
        ax = axes[i]
        count_labels = df[label].replace({0: 'Negative: 0', 1: 'Positive: 1'}).value_counts()
        count_labels.plot(kind='bar', ax=ax, color=colors[i], label=label)
        ax.set_title(f'{label}')
        ax.set_xlabel('Label')
        ax.set_ylabel('Count')

    plt.suptitle(f'# Positives and Negatives Examples for each class within the {str.upper(title)} SET', fontsize=12)
    plt.tight_layout()
    plt.show()

plot_label_count(train_df,"training")
plot_label_count(valid_df,"Validation")
plot_label_count(test_df,"Test")

def plot_samples_per_class(df_train, df_valid, df_test):

    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 6))
    axes = axes.flatten()
    colors = random.sample(list(mcolors.TABLEAU_COLORS.keys()), 4)
    labels = list(level2_to_level3.keys())

    for i in range(3):
        ax = axes[i]
        train_counts = df_train[labels].eq(1).sum()
        valid_counts = df_valid[labels].eq(1).sum()
        test_counts = df_test[labels].eq(1).sum()

        train_counts.plot(kind='bar', ax=ax, color=colors[i])
        valid_counts.plot(kind='bar', ax=ax, color=colors[i])
        test_counts.plot(kind='bar', ax=ax, color=colors[i])
        ax.set_title(f'{["TRAIN SET", "VALIDATION SET", "TEST SET"][i]}')
        ax.set_xlabel('Category')
        ax.set_ylabel('Count')
        ax.set_xticklabels(labels)


    plt.suptitle('Count Level 3 Categories ', fontsize=14)
    plt.tight_layout()
    plt.show()


plot_samples_per_class(train_df,valid_df,test_df)

train_df.describe()

"""### Maybe these analyses don't make much sense."""

correlation_matrix = train_df[['Openness to change', 'Self-enhancement', 'Conservation', 'Self-transcendence']].corr()
print(correlation_matrix)

contingency_table = pd.crosstab(index=[train_df['Openness to change'],train_df['Self-enhancement'],train_df['Conservation'], train_df['Self-transcendence']], columns='count')

plt.figure(figsize=(10, 8))
sns.heatmap(contingency_table, annot=True, cmap="coolwarm")
plt.title('Heatmap basata sul coefficiente di contingenza di Pearson')
plt.xlabel('Etichette')
plt.ylabel('Osservazioni')
plt.show()

"""# 2 Models Definition

### BASELINE MODELS
"""

def compute_labels_prob_distribution(df,title):
    print(f"{str.upper(title)} SET:\n")
    for i in df.columns[4:]:
        print(f"Probability distribution of {i} \n{df[i].value_counts(normalize=True).round(2)}")
        print("\n")


compute_labels_prob_distribution(train_df,"train")

def plot_probability_distribution(dataframe, title):
    print(f"{str.upper(title)} SET:\n")
    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 6))
    colors = random.sample(list(mcolors.TABLEAU_COLORS.keys()), 4)
    for i in range(4):
        ax = axes[i]
        for column in dataframe.columns[4:]:
            dataframe[dataframe.columns[i+4]].value_counts(normalize=True).plot(kind='bar', ax=ax, color=colors[i])
            ax.set_title(f'{dataframe.columns[i+4]}')
            ax.set_xlabel('Label')
            ax.set_ylabel('Frequency')
            ax.set_ylim(0, 1)


    plt.tight_layout()
    plt.show()

plot_probability_distribution(train_df, "train")
plot_probability_distribution(valid_df, "validation")
plot_probability_distribution(test_df, "test")

"""## Exploring Sentences"""

train_df.head()

def plot_stance_distribution(train_df,valid_df,test_df):

    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 6))

    train_df["Stance"].value_counts().plot(kind='pie', labels=['in favor of', 'against'],
                                    wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20,
                                    textprops={'fontsize': 15}, ax=ax[0])
    ax[0].set_title('Training Set')
    valid_df["Stance"].value_counts().plot(kind='pie', labels=['in favor of', 'against'],
                                    wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20,
                                    textprops={'fontsize': 15}, ax=ax[1])
    ax[1].set_title('Validation Set')
    test_df["Stance"].value_counts().plot(kind='pie', labels=['in favor of', 'against'],
                                    wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20,
                                    textprops={'fontsize': 15}, ax=ax[2])
    ax[2].set_title('Test Set')


    plt.suptitle('Stance feature Distribution', fontsize=14)
    plt.tight_layout()
    plt.show()

plot_stance_distribution(train_df,valid_df,test_df)

def refine_df(df):
    new_df = df.copy()
    new_df["Stance"] = df['Stance'].replace({'in favor of': 1, 'against': 0})
    new_df.drop(df.columns[4:], axis=1, inplace=True)
    new_df['Label'] = df.iloc[:, 4:].apply(lambda x: list(x), axis=1)
    return new_df

train_dataframe = refine_df(train_df)
test_dataframe = refine_df(test_df)
valid_dataframe = refine_df(valid_df)

display(train_dataframe.head())

train_df["Premise"].describe()

train_df["Conclusion"].describe()

def compute_text_lengths(train_df: pd.DataFrame, test_df:pd.DataFrame, valid_df:pd.DataFrame):
    lengths = dict()

    for i in ['Premise', 'Conclusion']:
        lengths[i] = {
            'Training': {
                'Max Length': train_df[i].apply(len).max(),
                'Min Length': train_df[i].apply(len).min(),
                'Mean Length': train_df[i].apply(len).mean().round(2)
            },
            'Test': {
                'Max Length': test_df[i].apply(len).max(),
                'Min Length': test_df[i].apply(len).min(),
                'Mean Length': test_df[i].apply(len).mean().round(2)
            },
            'Validation': {
                'Max Length': valid_df[i].apply(len).max(),
                'Min Length': valid_df[i].apply(len).min(),
                'Mean Length': valid_df[i].apply(len).mean().round(2)
            }
        }

    return lengths

for i in compute_text_lengths(train_dataframe, test_dataframe, valid_dataframe).items():
    print(f"{i[0]} Feature:")
    print(f"\tTraining Set: \n\t\tMax Length: {i[1]['Training']['Max Length']}, \n\t\tMin Length: {i[1]['Training']['Min Length']}, \n\t\tMean Length: {i[1]['Training']['Mean Length']}")
    print(f"\tValidation Set: \n\t\tMax Length: {i[1]['Validation']['Max Length']}, \n\t\tMin Length: {i[1]['Validation']['Min Length']}, \n\t\tMean Length: {i[1]['Validation']['Mean Length']}")
    print(f"\tTest Set: \n\t\tMax Length: {i[1]['Test']['Max Length']}, \n\t\tMin Length: {i[1]['Test']['Min Length']}, \n\t\tMean Length: {i[1]['Test']['Mean Length']}")
    print("\n")

def plot_sentence_lengths(train: pd.DataFrame, test_df:pd.DataFrame, valid_df:pd.DataFrame):
    features = ['Premise', 'Conclusion']
    train_lengths = train_df[features].apply(lambda x: x.str.len())
    valid_lengths = valid_df[features].apply(lambda x: x.str.len())
    test_lengths = test_df[features].apply(lambda x: x.str.len())

    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 8))
    labels = ['Training', 'Validation', 'Test']
    data = [train_lengths, valid_lengths, test_lengths]
    flierprops = dict(marker='s', markerfacecolor='r', markersize=3,
                  linestyle='none', markeredgecolor='r')


    for i, ax in enumerate(axes):
        ax.boxplot(data[i].values,flierprops=flierprops)
        ax.set_xticklabels(['Premise', 'Conclusion'])
        ax.set_title(labels[i])
        ax.set_ylabel('Sentence Length')

        text_lengths = compute_text_lengths(train_df, test_df, valid_df)
        legend_premise_text = f"{features[0]}: \nMax Length: {text_lengths[features[0]][labels[i]]['Max Length']}\nMin Length: {text_lengths[features[0]][labels[i]]['Min Length']}\nMean Length: {text_lengths[features[0]][labels[i]]['Mean Length']}"
        legend__conclusion_text = f"{features[1]}: \nMax Length: {text_lengths[features[1]][labels[i]]['Max Length']}\nMin Length: {text_lengths[features[1]][labels[i]]['Min Length']}\nMean Length: {text_lengths[features[1]][labels[i]]['Mean Length']}"
        ax.legend([legend_premise_text,legend__conclusion_text], loc='upper right',ncol=1)

    plt.tight_layout()
    plt.show()

plot_sentence_lengths(train_dataframe,test_dataframe,valid_dataframe)

"""p(y_hat = y) = p(y_hat = 0) * p(y = 0) + p(y_hat = 1) * p(y = 1)

p(y_hat = Openess_to_change) = 0.5 * 0.63 + 0.5 * 0.37 = 0.315 + 0.185 = 0.5  
p(y_hat = Self-enhancement) = 0.5 * 0.54 + 0.5 * 0.46 = 0.27 + 0.23 = 0.5      
p(y_hat = Conservation) = 0.5 * 0.24 + 0.5 * 0.0.76 = 0.12 + 0.38 = 0.5          
p(y_hat = Self-trascendence) = 0.5 * 0.24 + 0.5 * 0.0.76 = 0.12 + 0.38 = 0.5

mean = 0.5

"""

#Random Unifor Classifier

num_samples = len(train_df)
num_classes = 4

random_predictions = np.random.randint(2, size=(num_samples, num_classes))

num_zeros = np.count_nonzero(random_predictions == 0)
num_ones = np.count_nonzero(random_predictions == 1)
print("Number of 0s:", num_zeros)
print("Number of 1s:", num_ones)

true_labels = train_df.iloc[:, 4:].values


accuracy = np.mean(np.equal(random_predictions, true_labels))
print("Accuracy: {:.2f}".format(accuracy))

"""p(yhat = y) = P(yhat = 0) * P(y = 0) + P(yhat = 1) * P(y = 1)

p(y_hat = Openess_to_change) = 1 * 0.63 + 0 * 0.37 = 0.63      
p(y_hat = Self-enhancement) = 1 * 0.54 + 0 * 0.46 =  0.54      
p(y_hat = Conservation) =  0 * 0.24 + 1 * 0.76 = 0.76          
p(y_hat = Self-trascendence) = 0 * 0.24 + 1 * 0.76 = 0.76      

mean = 0.63 + 0.54 + 0.76 + 0.76 / 4 = ~ 0.67

"""

#Majoirty Classifier

true_labels = train_df.iloc[:, 4:].values


num_zeros = np.sum(true_labels == 0, axis=0)
num_ones = np.sum(true_labels == 1, axis=0)
print("Number of 0s:", num_zeros)
print("Number of 1s:", num_ones)

majority_predictions = np.where(num_zeros > num_ones, 0, 1)
print("Majority Predictions:", majority_predictions.shape)


accuracy = np.mean(np.equal(majority_predictions, true_labels))
print("Majority Classifier Accuracy: {:.2f}".format(accuracy))

"""## Dataset"""

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

class RandomClassifier(keras.Model):
    def __init__(self, num_classes):
        super(RandomClassifier, self).__init__()
        self.num_classes = num_classes


    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        random_predictions = tf.random.uniform(shape=(batch_size, self.num_classes), minval=0, maxval=2, dtype=tf.int32)
        return random_predictions



class MajorityClassifier(tf.keras.Model):
    def __init__(self, num_categories):
        super(MajorityClassifier, self).__init__()
        self.num_categories = num_categories
        self.majority_classes = tf.Variable(np.zeros(num_categories), trainable=False, dtype=tf.int32)

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        return tf.repeat(tf.expand_dims(self.majority_classes, 0), repeats=batch_size, axis=0)

    def fit(self, x, y):
        labels = y
        threshold = len(labels) / 2
        is_majority = tf.reduce_sum(labels, axis=0) > threshold
        self.majority_classes.assign(tf.cast(is_majority, tf.int32))

def get_initializer(initializer_range):
    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)



class MultilabelClassifier(tf.keras.Model):
    def __init__(self, num_labels, drop_out: int = 0.1, model_type: str = "base"):
        super(MultilabelClassifier, self).__init__()
        self.num_labels = num_labels
        self.model_type = model_type
        
        self.bert_conclusion = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
        
        if model_type == "bert_w_cp":
            self.bert_premise = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
        elif model_type == "bert_w_cps":
            self.bert_stance = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
        
        self.dropout = tf.keras.layers.Dropout(drop_out)
        self.dense = tf.keras.layers.Dense(num_labels, activation='softmax')
        
    def call(self, inputs):
        conclusion_input_ids = inputs['conclusion']['input_ids']
        conclusion_attention_mask = inputs['conclusion']['attention_mask']
        
        if self.model_type == "base":
            outputs = self.bert_conclusion(input_ids=conclusion_input_ids, attention_mask=conclusion_attention_mask)
        elif self.model_type == "bert_w_cp":
            premise_input_ids = inputs['premise']['input_ids']
            premise_attention_mask = inputs['premise']['attention_mask']
            
            conclusion_outputs = self.bert_conclusion(input_ids=conclusion_input_ids, attention_mask=conclusion_attention_mask)
            premise_outputs = self.bert_premise(input_ids=premise_input_ids, attention_mask=premise_attention_mask)
            
            pooled_output = tf.concat([conclusion_outputs.pooler_output, premise_outputs.pooler_output], axis=1)
            pooled_output = self.dropout(pooled_output)
            logits = self.dense(pooled_output)
            return logits
        elif self.model_type == "bert_w_cps":
            stance_input_ids = inputs['stance']['input_ids']
            stance_attention_mask = inputs['stance']['attention_mask']
            
            conclusion_outputs = self.bert_conclusion(input_ids=conclusion_input_ids, attention_mask=conclusion_attention_mask)
            stance_outputs = self.bert_stance(input_ids=stance_input_ids, attention_mask=stance_attention_mask)
            
            pooled_output = tf.concat([conclusion_outputs.pooler_output, stance_outputs.pooler_output], axis=1)
            pooled_output = self.dropout(pooled_output)
            logits = self.dense(pooled_output)
            return logits
        else:
            raise ValueError("Invalid model_type. Supported model types: base, bert_w_cp, bert_w_cps")

def preprocess_data(df,max_length):
    
    conclusion_encodings = tokenizer(list(df["Conclusion"]), truncation=True, padding=True, max_length=max_length)
    premise_encodings = tokenizer(list(df["Premise"]), truncation=True, padding=True, max_length=max_length)
    labels = tf.constant(list(df["Label"]), dtype=tf.float32)
    stances = tf.constant(list(df["Stance"]), dtype=tf.float32)
    argument_ids = tf.constant(list(df["Argument ID"]), dtype=tf.string)  

    # Convert the lists in conclusion_encodings and premise_encodings to tensors
    conclusion_encodings = {key: tf.constant(val) for key, val in conclusion_encodings.items()}
    premise_encodings = {key: tf.constant(val) for key, val in premise_encodings.items()}
    
    dataset = tf.data.Dataset.from_tensor_slices(({"conclusion": conclusion_encodings,"premise": premise_encodings, "stance": stances, "argument_id": argument_ids}, labels))

    return dataset

max_length = 128
train_dataset = preprocess_data(train_dataframe, max_length)
valid_dataset = preprocess_data(valid_dataframe, max_length)
test_dataset = preprocess_data(test_dataframe, max_length)


def check_tokens(tokenizer):
    # Get the special tokens and their corresponding IDs
    special_tokens = tokenizer.special_tokens_map
    special_ids = tokenizer.convert_tokens_to_ids(list(special_tokens.values()))
    print("Special tokens:")
    for token_type, token_list in special_tokens.items():
        print(f"{token_type}: {token_list}")
    # Print the special tokens and their corresponding IDs
    for token, id in zip(special_tokens.keys(), special_ids):
        print(f"{token}: {id}")

model_checkpoint = 'prajjwal1/bert-tiny' 
# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
assert isinstance(tokenizer, PreTrainedTokenizerFast)

# Setting the BOS and EOS token
#tokenizer_M1.bos_token = tokenizer_M1.cls_token
#tokenizer_M1.eos_token = tokenizer_M1.sep_token
check_tokens(tokenizer)

def retrieve_conclusion_premise(dataset):
    for features, _ in dataset.take(1):  
        for i in ["conclusion","premise"]:
            input_ids = features[i]["input_ids"].numpy()[:128]  
            decoded = tokenizer.decode(input_ids)
            print(f"{str.upper(i)} tokens:", decoded)
            print("\n")
            
retrieve_conclusion_premise(train_dataset)

def per_category_f1(y_true, y_pred):
    """
    Compute per-category binary F1-score.

    Parameters:
        y_true (numpy.ndarray): True labels.
        y_pred (numpy.ndarray): Predicted labels.

    Returns:
        per_category_f1_scores (dict): Dictionary containing per-category F1-scores.
    """
    per_category_f1_scores = {}
    categories = np.unique(y_true)
    
    for category in categories:
        category_indices = np.where(y_true == category)
        category_true = y_true[category_indices]
        category_pred = y_pred[category_indices]
        f1 = f1_score(category_true, category_pred)
        per_category_f1_scores[category] = f1
    
    return per_category_f1_scores

def macro_f1(y_true, y_pred):
    """
    Compute macro F1-score.

    Parameters:
        y_true (numpy.ndarray): True labels.
        y_pred (numpy.ndarray): Predicted labels.

    Returns:
        macro_f1_score (float): Macro F1-score.
    """
    per_category_f1_scores = per_category_f1(y_true, y_pred)
    macro_f1_score = np.mean(list(per_category_f1_scores.values()))
    return macro_f1_score

@tf.function
def train_model(model, train_dataset, val_dataset, num_epochs):
    """
    Trains a model using the given training dataset and evaluates it on the validation dataset for the specified number of epochs.

    Args:
        model (tf.keras.Model): The model to train.
        train_dataset (tf.data.Dataset): The training dataset.
        val_dataset (tf.data.Dataset): The validation dataset.
        num_epochs (int): The number of epochs to train the model.

    Returns:
        None
    """

    # Define the optimizer and loss function
    optimizer = keras.optimizers.Adam()
    loss_fn = keras.losses.SparseCategoricalCrossentropy()

    # Define metrics to track
    train_loss_metric = keras.metrics.Mean()
    train_f1_metric = keras.metrics.Mean()
    val_loss_metric = keras.metrics.Mean()
    val_f1_metric = keras.metrics.Mean()

    # Iterate over epochs
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        
        # Training phase
        train_loss_metric.reset_states()
        train_f1_metric.reset_states()
        for batch in train_dataset:
            x, y_true = batch

            with tf.GradientTape() as tape:
                y_pred = model(x, training=True)
                loss = loss_fn(y_true, y_pred)

            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))

            train_loss_metric.update_state(loss)
            train_f1_metric.update_state(per_category_f1(y_true.numpy(), y_pred.numpy()))

        # Validation phase
        val_loss_metric.reset_states()
        val_f1_metric.reset_states()
        for batch in val_dataset:
            x, y_true = batch

            y_pred = model(x, training=False)
            loss = loss_fn(y_true, y_pred)

            val_loss_metric.update_state(loss)
            val_f1_metric.update_state(per_category_f1(y_true.numpy(), y_pred.numpy()))

        # Print metrics
        print("Epoch {}/{} - train_loss: {:.4f} - train_f1: {:.4f} - val_loss: {:.4f} - val_f1: {:.4f}".format(
            epoch + 1, num_epochs, train_loss_metric.result(), train_f1_metric.result(),
            val_loss_metric.result(), val_f1_metric.result()))


# Instantiate the model
model = MultilabelClassifier(num_labels=4, drop_out=0.1, model_type="base")
# Train the model
train_model(model, train_dataset, valid_dataset, num_epochs=3)

"""# 3. Evaluation"""



















