{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zefTqvMBeXRb",
   "metadata": {
    "id": "zefTqvMBeXRb"
   },
   "source": [
    "LAST LINK:\n",
    "* https://huggingface.co/course/chapter7/7?fw=tf\n",
    "\n",
    "LINK:\n",
    "* https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "\n",
    "* https://github.com/Michael-M-Mike/Unibo-NLP-Assignments/blob/main/A2_Seq2Seq_Abstractive_Question_Answering_(QA)_on_CoQA/distilroberta_42.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d23b6c",
   "metadata": {
    "id": "d1d23b6c"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Question Answering, CoQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f451b",
   "metadata": {
    "id": "bd3f451b"
   },
   "source": [
    "## Deadlines\n",
    "\n",
    "* **December 11**, 2022: deadline for having assignments graded by January 11, 2023\n",
    "* **January 11**, 2023: deadline for half-point speed bonus per assignment\n",
    "* **After January 11**, 2023: assignments are still accepted, but there will be no speed bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ada8c8",
   "metadata": {
    "id": "11ada8c8"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c07553",
   "metadata": {
    "id": "47c07553"
   },
   "source": [
    "### Problem\n",
    "\n",
    "Question Answering (QA) on [CoQA](https://stanfordnlp.github.io/coqa/) dataset: a conversational QA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4907f8d",
   "metadata": {
    "id": "b4907f8d"
   },
   "source": [
    "### Task\n",
    "\n",
    "Given a question $Q$, a text passage $P$, the task is to generate the answer $A$.<br>\n",
    "$\\rightarrow A$ can be: (i) a free-form text or (ii) unanswerable;\n",
    "\n",
    "**Note**: an question $Q$ can refer to previous dialogue turns. <br>\n",
    "$\\rightarrow$ dialogue history $H$ may be a valuable input to provide the correct answer $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3760b5",
   "metadata": {
    "id": "9b3760b5"
   },
   "source": [
    "### Models\n",
    "\n",
    "We are going to experiment with transformer-based models to define the following models:\n",
    "\n",
    "1.  $A = f_\\theta(Q, P)$\n",
    "\n",
    "2. $A = f_\\theta(Q, P, H)$\n",
    "\n",
    "where $f_\\theta$ is the transformer-based model we have to define with $\\theta$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfee64",
   "metadata": {
    "id": "66cfee64"
   },
   "source": [
    "## The CoQA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fa650",
   "metadata": {
    "id": "996fa650"
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=16vrgyfoV42Z2AQX0QY7LHTfrgektEKKh\" width=\"750\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e7d0",
   "metadata": {
    "id": "f6e3e7d0"
   },
   "source": [
    "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6c37e",
   "metadata": {
    "id": "bfb6c37e"
   },
   "source": [
    "## Rationales\n",
    "\n",
    "Each QA pair is paired with a rationale $R$: it is a text span extracted from the given text passage $P$. <br>\n",
    "$\\rightarrow$ $R$ is not a requested output, but it can be used as an additional information at training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa786e2",
   "metadata": {
    "id": "daa786e2"
   },
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "* **127k** QA pairs.\n",
    "* **8k** conversations.\n",
    "* **7** diverse domains: Children's Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science.\n",
    "* Average conversation length: **15 turns** (i.e., QA pairs).\n",
    "* Almost **half** of CoQA questions refer back to **conversational history**.\n",
    "* Only **train** and **validation** sets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d68b7",
   "metadata": {
    "id": "d26d68b7"
   },
   "source": [
    "## Dataset snippet\n",
    "\n",
    "The dataset is stored in JSON format. Each dialogue is represented as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"mctest\",\n",
    "    \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
    "    \"filename\": \"mc160.test.41\",\n",
    "    \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. \n",
    "    Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. [...]\" % <-- $P$\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"input_text\": \"What color was Cotton?\",   % <-- $Q_1$\n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"Where did she live?\",\n",
    "            \"turn_id\": 2\n",
    "        },\n",
    "        [...]\n",
    "    ],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"span_start\": 59,   % <-- $R_1$ start index\n",
    "            \"spand_end\": 93,    % <-- $R_1$ end index\n",
    "            \"span_text\": \"a little white kitten named Cotton\",   % <-- $R_1$\n",
    "            \"input_text\" \"white\",   % <-- $A_1$      \n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        [...]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7558c",
   "metadata": {
    "id": "72c7558c"
   },
   "source": [
    "### Simplifications\n",
    "\n",
    "Each dialogue also contains an additional field ```additional_answers```. For simplicity, we **ignore** this field and only consider one groundtruth answer $A$ and text rationale $R$.\n",
    "\n",
    "CoQA only contains 1.3% of unanswerable questions. For simplicity, we **ignore** those QA pairs.\n",
    "\n",
    "# [0] Functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gcvzSCKbNGHv",
   "metadata": {
    "id": "gcvzSCKbNGHv"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "NSKlKfKr7GZe",
   "metadata": {
    "id": "NSKlKfKr7GZe"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "# import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import random \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Display dataframes\n",
    "def display(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:left\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h4 style=\"text-align: left;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "\n",
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cdad7",
   "metadata": {
    "id": "e01cdad7"
   },
   "source": [
    "## [Task 1] Remove unaswerable QA pairs\n",
    "\n",
    "Write your own script to remove unaswerable QA pairs from both train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6643e14",
   "metadata": {
    "id": "f6643e14"
   },
   "source": [
    "## Dataset Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bac70",
   "metadata": {
    "id": "358bac70"
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab3ff",
   "metadata": {
    "id": "5f6ab3ff"
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e42311",
   "metadata": {
    "id": "40e42311"
   },
   "source": [
    "#### Data Inspection\n",
    "\n",
    "Spend some time in checking accurately the dataset format and how to retrieve the tasks' inputs and outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DmuPOnX46NT5",
   "metadata": {
    "id": "DmuPOnX46NT5"
   },
   "outputs": [],
   "source": [
    "train_data = json.load((open('coqa/train.json')))\n",
    "test_data = json.load((open('coqa/test.json')))\n",
    "\n",
    "qas = pd.json_normalize(train_data['data'], ['questions'], ['source', 'id', 'story'])\n",
    "ans = pd.json_normalize(train_data['data'], ['answers'],['id'])\n",
    "train_val_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])\n",
    "train_val_df = train_val_df.loc[train_val_df['input_text_y']!='unknown']\n",
    "\n",
    "qas = pd.json_normalize(test_data['data'], ['questions'], ['source', 'id', 'story'])\n",
    "ans = pd.json_normalize(test_data['data'], ['answers'],['id'])\n",
    "test_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])\n",
    "test_df = test_df.loc[test_df['input_text_y']!='unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57334e0",
   "metadata": {
    "id": "f57334e0"
   },
   "source": [
    "## [Task 2] Train, Validation and Test splits\n",
    "\n",
    "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
    "\n",
    "We'll consider the provided validation set as a test set. <br>\n",
    "$\\rightarrow$ Write your own script to:\n",
    "* Split the train data in train and validation splits (80% train and 20% val)\n",
    "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
    "* Perform splitting using the following seed for reproducibility: 42\n",
    "\n",
    "#### Reproducibility Memo\n",
    "\n",
    "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2sRaINOONUmh",
   "metadata": {
    "id": "2sRaINOONUmh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import *\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yYsAxKF6_zG2",
   "metadata": {
    "id": "yYsAxKF6_zG2"
   },
   "outputs": [],
   "source": [
    "set_reproducibility(42)\n",
    "\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(train_val_df, groups=train_val_df['story']))\n",
    "\n",
    "train_df = train_val_df.iloc[train_inds]\n",
    "val_df = train_val_df.iloc[val_inds].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LAl2ZKTYujo5",
   "metadata": {
    "id": "LAl2ZKTYujo5"
   },
   "outputs": [],
   "source": [
    "print(f'Training set [{train_df.shape}]')\n",
    "print(f'\\tFeatures: {list(train_df.columns)}')\n",
    "display(train_df.loc[10:10,['input_text_x', 'input_text_y', 'span_text', 'story']])\n",
    "\n",
    "print(f'Validation set [{val_df.shape}]')\n",
    "print(f'\\tFeatures: {list(val_df.columns)}')\n",
    "display(val_df.loc[10:10,['input_text_x', 'input_text_y', 'span_text', 'story']])\n",
    "\n",
    "print(f'\\nTest set [{test_df.shape}]')\n",
    "print(f'\\tFeatures: {list(test_df.columns)}')\n",
    "display(test_df.loc[10:10,['input_text_x', 'input_text_y', 'span_text', 'story']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlSIIqCP5K4z",
   "metadata": {
    "id": "qlSIIqCP5K4z"
   },
   "source": [
    "Now we check if there is any overlapping dialogue between train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cul9guslrKP-",
   "metadata": {
    "id": "cul9guslrKP-"
   },
   "outputs": [],
   "source": [
    "set_train = set(train_df['story'])\n",
    "set_val = set(val_df['story'])\n",
    "\n",
    "overlap = False\n",
    "for i in set_train:\n",
    "  if i in set_val:\n",
    "    overlap = True\n",
    "    break\n",
    "\n",
    "print('Overlap' if overlap else 'No overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LFy-wE1wCFBr",
   "metadata": {
    "id": "LFy-wE1wCFBr"
   },
   "outputs": [],
   "source": [
    "features = ['id', 'story','input_text_x', 'input_text_y', 'span_start', 'span_end']\n",
    "\n",
    "train_df_to_ds = train_df[features]\n",
    "val_df_to_ds = val_df[features]\n",
    "test_df_to_ds = test_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4SC7CI-rzl8v",
   "metadata": {
    "id": "4SC7CI-rzl8v"
   },
   "outputs": [],
   "source": [
    "ratio = 2\n",
    "\n",
    "train_samples = round(train_df_to_ds.shape[0] * ratio / 100)\n",
    "val_samples = round(val_df_to_ds.shape[0] * ratio / 100)\n",
    "test_samples = round(test_df_to_ds.shape[0] * ratio / 100) \n",
    "\n",
    "train_dataset = Dataset.from_dict(train_df_to_ds.iloc[:train_samples])\n",
    "val_dataset = Dataset.from_dict(val_df_to_ds.iloc[:val_samples])\n",
    "test_dataset = Dataset.from_dict(test_df_to_ds.iloc[:test_samples])\n",
    "\n",
    "dataset_COQA = DatasetDict({'train':train_dataset,'validation':val_dataset,'test':test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LAQDNCRI0It0",
   "metadata": {
    "id": "LAQDNCRI0It0"
   },
   "outputs": [],
   "source": [
    "dataset_COQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "__IFYntoJIUa",
   "metadata": {
    "id": "__IFYntoJIUa"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "inputs_lengths = [len(x[0])+len(x[1]) for x in zip(train_val_df['input_text_x'],\\\n",
    "                                               train_val_df['story'])]\n",
    "\n",
    "max_length_input = round(np.quantile(list(set(inputs_lengths)), .50)) \n",
    "stride = int(max_length_input/3)\n",
    "# print(f'Max length (Third quartile):{max_length_input}')\n",
    "print(f'Max length (Median):{max_length_input}')\n",
    "print(f'Stride:{stride}')\n",
    "\n",
    "fig_inputs = px.box(list(set(inputs_lengths)))\n",
    "fig_inputs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3x9P944M8SYg",
   "metadata": {
    "id": "3x9P944M8SYg"
   },
   "outputs": [],
   "source": [
    "# outputs_lengths = [len(x) for x in train_val_df['input_text_y']]\n",
    "\n",
    "# max_length_answer = round(np.quantile(list(set(outputs_lengths)), .50))\n",
    "# # print(f'Max length (Third quartile):{max_length_answer}')\n",
    "# print(f'Max length (Median):{max_length_answer}')\n",
    "\n",
    "# fig_inputs = px.box(list(set(outputs_lengths)))\n",
    "# fig_inputs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XXO1jCuOIcuR",
   "metadata": {
    "id": "XXO1jCuOIcuR"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_M1 = 'distilroberta-base'\n",
    "tokenizer_M1 = AutoTokenizer.from_pretrained(model_checkpoint_M1)\n",
    "assert isinstance(tokenizer_M1, PreTrainedTokenizerFast)\n",
    "\n",
    "model_checkpoint_M2 = 'prajjwal1/bert-tiny'\n",
    "tokenizer_M2 = AutoTokenizer.from_pretrained(model_checkpoint_M2)\n",
    "assert isinstance(tokenizer_M2, PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MSHCfua8ht_S",
   "metadata": {
    "id": "MSHCfua8ht_S"
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(data, tokenizer, max_length_input, stride):\n",
    "    questions = [q.strip() for q in data['input_text_x']]\n",
    "\n",
    "    # Tokenize the Question and Context columns\n",
    "    encoded_inputs = tokenizer(\n",
    "        questions,\n",
    "        data['story'],\n",
    "        max_length=max_length_input,\n",
    "        stride = stride,\n",
    "        truncation='only_second',\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    offset_mapping = encoded_inputs.pop('offset_mapping')\n",
    "    sample_map = encoded_inputs.pop('overflow_to_sample_mapping')\n",
    "    \n",
    "    answers = data['input_text_y']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = data['span_start'][0]\n",
    "        end_char = data['span_end'][0]\n",
    "        sequence_ids = encoded_inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        \n",
    "        while sequence_ids[idx] != 1:\n",
    "          idx += 1\n",
    "        context_start = idx\n",
    "        \n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    encoded_inputs['start_positions'] = start_positions\n",
    "    encoded_inputs['end_positions'] = end_positions\n",
    "\n",
    "    return encoded_inputs\n",
    "\n",
    "def prepare_val_features(data, tokenizer, max_length_input, stride):\n",
    "    questions = [q.strip() for q in data['input_text_x']]\n",
    "\n",
    "    # Tokenize the Question and Context columns\n",
    "    encoded_inputs = tokenizer(\n",
    "        questions,\n",
    "        data['story'],\n",
    "        max_length=max_length_input,\n",
    "        stride = stride,\n",
    "        truncation='only_second',\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    sample_map = encoded_inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(encoded_inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(data['id'][sample_idx])\n",
    "\n",
    "        sequence_ids = encoded_inputs.sequence_ids(i)\n",
    "        offset = encoded_inputs['offset_mapping'][i]\n",
    "        encoded_inputs['offset_mapping'][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    encoded_inputs[\"example_id\"] = example_ids\n",
    "\n",
    "    return encoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ung3zCz08GSF",
   "metadata": {
    "id": "Ung3zCz08GSF"
   },
   "source": [
    "* [M1] DistilRoBERTa (distilberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZR491qMJ94HJ",
   "metadata": {
    "id": "ZR491qMJ94HJ"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_M1 = DatasetDict()\n",
    "\n",
    "# Use the `prepare_features` functions\n",
    "tokenized_datasets_M1['train'] = dataset_COQA['train'].map(\n",
    "    lambda datarow: prepare_train_features(datarow, tokenizer_M1, max_length_input, stride),\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset_COQA['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_datasets_M1['validation'] = dataset_COQA['validation'].map(\n",
    "    lambda datarow: prepare_val_features(datarow, tokenizer_M1, max_length_input, stride),\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset_COQA['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d-mTyZHTP9h7",
   "metadata": {
    "id": "d-mTyZHTP9h7"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5RMxNwZlRE2B",
   "metadata": {
    "id": "5RMxNwZlRE2B"
   },
   "source": [
    "* [M2] BERTTiny (bert-tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4nbT0QJVRYMg",
   "metadata": {
    "id": "4nbT0QJVRYMg"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_M2 = DatasetDict()\n",
    "\n",
    "# Use the `prepare_features` functions\n",
    "tokenized_datasets_M2['train'] = dataset_COQA['train'].map(\n",
    "    lambda datarow: prepare_train_features(datarow, tokenizer_M2, max_length_input, stride),\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset_COQA['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_datasets_M2['validation'] = dataset_COQA['validation'].map(\n",
    "    lambda datarow: prepare_val_features(datarow, tokenizer_M2, max_length_input, stride),\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=dataset_COQA['validation'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LkaLnPxyRaiR",
   "metadata": {
    "id": "LkaLnPxyRaiR"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets_M2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "id": "230a21de"
   },
   "source": [
    "## [Task 3] Model definition\n",
    "\n",
    "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
    "\n",
    "* [M1] DistilRoBERTa (distilberta-base)\n",
    "* [M2] BERTTiny (bert-tiny)\n",
    "\n",
    "**Note**: Remember to install the ```transformers``` python package!\n",
    "\n",
    "**Note**: We consider small transformer models for computational reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2IvgKVHJUUuD",
   "metadata": {
    "id": "2IvgKVHJUUuD"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EfroUy-MI33j",
   "metadata": {
    "id": "EfroUy-MI33j"
   },
   "source": [
    "* [M1] DistilRoBERTa (distilberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KxoCOQ9JKR1p",
   "metadata": {
    "id": "KxoCOQ9JKR1p"
   },
   "outputs": [],
   "source": [
    "model_M1 = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x0eG28KvQeuq",
   "metadata": {
    "id": "x0eG28KvQeuq"
   },
   "source": [
    "* [M2] BERTTiny (bert-tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6MVNRkkJQeEz",
   "metadata": {
    "id": "6MVNRkkJQeEz"
   },
   "outputs": [],
   "source": [
    "model_M2 = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint_M2, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e83f28",
   "metadata": {
    "id": "f1e83f28"
   },
   "source": [
    "## [Task 4] Question generation with text passage $P$ and question $Q$\n",
    "\n",
    "We want to define $f_\\theta(P, Q)$. \n",
    "\n",
    "Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
    "\n",
    "#### Formulation\n",
    "\n",
    "Consider a dialogue on text passage $P$. \n",
    "\n",
    "For each question $Q_i$ at dialogue turn $i$, your model should take $P$ and $Q_i$ and generate $A_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iZpcgS-aIwx0",
   "metadata": {
    "id": "iZpcgS-aIwx0"
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vt0w5b8Htdyv",
   "metadata": {
    "id": "Vt0w5b8Htdyv"
   },
   "source": [
    "A dataset collator is a function used in data processing for deep learning models, \n",
    "especially in training and evaluation. It collates, or collects, several examples \n",
    "from a dataset into a batch and performs operations on the batch, such as padding \n",
    "or stacking. This is usually done to make the input data compatible with the model's \n",
    "batch size, which is the number of samples processed together in one forward/backward pass. \n",
    "The dataset collator takes care of the preprocessing required to format the examples in the batch, \n",
    "allowing the data to be efficiently processed by the deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOgSUToQJMrz",
   "metadata": {
    "id": "XOgSUToQJMrz"
   },
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6LTZuLlDtZ-j",
   "metadata": {
    "id": "6LTZuLlDtZ-j"
   },
   "source": [
    "* [M1] DistilRoBERTa (distilberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cSbC-PKJun1f",
   "metadata": {
    "id": "cSbC-PKJun1f"
   },
   "outputs": [],
   "source": [
    "tf_train_dataset_M1 = model_M1.prepare_tf_dataset(\n",
    "    tokenized_datasets_M1['train'],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "tf_eval_dataset_M2 = model_M1.prepare_tf_dataset(\n",
    "    tokenized_datasets_M1['validation'],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YVbXOWhbvWkJ",
   "metadata": {
    "id": "YVbXOWhbvWkJ"
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset_M1) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model_M1.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8QD4qg7YtlBV",
   "metadata": {
    "id": "8QD4qg7YtlBV"
   },
   "source": [
    "* [M2] BERTTiny (bert-tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168DJs8Bjs1",
   "metadata": {
    "id": "f168DJs8Bjs1"
   },
   "source": [
    "## [Task 6] Train and evaluate $f_\\theta(P, Q)$\n",
    "\n",
    "Write your own script to train and evaluate your $f_\\theta(P, Q)$.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Perform multiple train/evaluation seed runs: [42, 2022, 1337].$^1$\n",
    "* Evaluate your models with the following metrics: SQUAD F1-score.$^2$\n",
    "* Fine-tune each transformer-based models for **3 epochs**.\n",
    "* Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
    "\n",
    "$^1$ Remember what we said about code reproducibility in Tutorial 2!\n",
    "\n",
    "$^2$ You can use ```allennlp``` python package for a quick implementation of SQUAD F1-score: ```from allennlp_models.rc.tools import squad```. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P52nl4h9wUft",
   "metadata": {
    "id": "P52nl4h9wUft"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9w_PIMcdlrS4",
   "metadata": {
    "id": "9w_PIMcdlrS4"
   },
   "source": [
    "* [M1] DistilRoBERTa (distilberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ague-v4UwU9j",
   "metadata": {
    "id": "Ague-v4UwU9j"
   },
   "outputs": [],
   "source": [
    "# We're going to do validation afterwards, so no validation mid-training\n",
    "model_M1.fit(tf_train_dataset_M1, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5HEpGp0ZkWJD",
   "metadata": {
    "id": "5HEpGp0ZkWJD"
   },
   "outputs": [],
   "source": [
    "# model_M1.save_model(f'{model_checkpoint_M1}-finetuned-coqa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QlKMzLOyVs2w",
   "metadata": {
    "id": "QlKMzLOyVs2w"
   },
   "source": [
    "* [M2] BERTTiny (bert-tiny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7e98f",
   "metadata": {
    "id": "92c7e98f"
   },
   "source": [
    "## [Task 6] Error Analysis\n",
    "\n",
    "Perform a simple and short error analysis as follows:\n",
    "* Group dialogues by ```source``` and report the worst 5 model errors for each source (w.r.t. SQUAD F1-score).\n",
    "* Inspect observed results and try to provide some comments (e.g., do the models make errors when faced with a particular question type?)$^1$\n",
    "\n",
    "$^1$ Check the [paper](https://arxiv.org/pdf/1808.07042.pdf) for some valuable information about question/answer types (e.g., Table 6, Table 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814004",
   "metadata": {
    "id": "f1814004"
   },
   "source": [
    "# Assignment Evaluation\n",
    "\n",
    "The following assignment points will be awarded for each task as follows:\n",
    "\n",
    "* Task 1, Pre-processing $\\rightarrow$ 0.5 points.\n",
    "* Task 2, Dataset Splitting $\\rightarrow$ 0.5 points.\n",
    "* Task 3 and 4, Models Definition $\\rightarrow$ 1.0 points.\n",
    "* Task 5 and 6, Models Training and Evaluation $\\rightarrow$ 2.0 points.\n",
    "* Task 7, Analysis $\\rightarrow$ 1.0 points.\n",
    "* Report $\\rightarrow$ 1.0 points.\n",
    "\n",
    "**Total** = 6 points <br>\n",
    "\n",
    "We may award an additional 0.5 points for outstanding submissions. \n",
    " \n",
    "**Speed Bonus** = 0.5 extra points <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1b2b9",
   "metadata": {
    "id": "20a1b2b9"
   },
   "source": [
    "# Report\n",
    "\n",
    "We apply the rules described in Assignment 1 regarding the report.\n",
    "* Write a clear and concise report following the given overleaf template (**max 2 pages**).\n",
    "* Report validation and test results in a table.$^1$\n",
    "* **Avoid reporting** code snippets or copy-paste terminal outputs $\\rightarrow$ **Provide a clean schema** of what you want to show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967c209",
   "metadata": {
    "id": "0967c209"
   },
   "source": [
    "# Comments and Organization\n",
    "\n",
    "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!\n",
    "\n",
    "Structure your code for readability and maintenance. If you work with Colab, use sections. \n",
    "\n",
    "This allows you to build clean and modular code, as well as easy to read and to debug (notebooks can be quite tricky time to time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23929660",
   "metadata": {
    "id": "23929660"
   },
   "source": [
    "# FAQ (READ THIS!)\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Does Task 3 also include data tokenization and conversion step?\n",
    "\n",
    "**Answer:** Yes! These steps are usually straightforward since ```transformers``` also offers a specific tokenizer for each model.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(text)\n",
    "%% Alternatively\n",
    "inputs = tokenizer.tokenize(text, add_special_tokens=True, max_length=min(max_length, 512))\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "```\n",
    "\n",
    "**Suggestion**: Hugginface's documentation is full of tutorials and user-friendly APIs.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**Question**: I'm hitting **out of memory error** when training my models, do you have any suggestions?\n",
    "\n",
    "**Answer**: Here are some common workarounds:\n",
    "\n",
    "1. Try decreasing the mini-batch size\n",
    "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56a612",
   "metadata": {
    "id": "9c56a612"
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bac4b9",
   "metadata": {
    "id": "54bac4b9"
   },
   "source": [
    "# The End!\n",
    "\n",
    "Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
