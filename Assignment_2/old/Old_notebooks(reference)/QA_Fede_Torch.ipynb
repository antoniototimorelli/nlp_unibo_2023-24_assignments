{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wymhkIdOx0t7",
        "outputId": "2a3f94f7-0039-466c-f4ef-92528e1c147a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 397, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 529, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 202, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 1124, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4936, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 849, in _parseNoCache\n",
            "    tokens = fn(instring, tokens_start, ret_tokens)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 285, in wrapper\n",
            "    ret = func(*args[limit:])\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/requirements.py\", line 71, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/markers.py\", line 278, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 1124, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 5203, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4352, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3841, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4091, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 4352, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 810, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 3863, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing/core.py\", line 847, in _parseNoCache\n",
            "    for fn in self.parseAction:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 204, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1493, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 167, in emit\n",
            "    style = Style(color=\"red\")\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/style.py\", line 147, in __init__\n",
            "    def _make_color(color: Union[Color, str]) -> Color:\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow_addons\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import *\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "\n",
        "from transformers import BertForQuestionAnswering, TFAutoModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, BertTokenizer"
      ],
      "metadata": {
        "id": "rlx8RG0QyC1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "  def update_to(self, b=1, bsize=1, tsize=None):\n",
        "    if tsize is not None:\n",
        "      self.total = tsize\n",
        "    self.update(b*bsize - self.n)\n",
        "\n",
        "def download_url(url, output_path):\n",
        "  with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
        "    urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):\n",
        "  if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "  data_path = os.path.join(data_path,f'{suffix}.json')\n",
        "\n",
        "  if not os.path.exists(data_path):\n",
        "    print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "    download_url(url=url_path, output_path=data_path)\n",
        "    print(\"Download Completed!\")\n"
      ],
      "metadata": {
        "id": "MxyNAMAgyrAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa',url_path=train_url, suffix='train')\n",
        "\n",
        "#Test Data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path = test_url, suffix='test')"
      ],
      "metadata": {
        "id": "gzO48TEr1E79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = json.load((open('/content/coqa/train.json')))\n",
        "qas = pd.json_normalize(train_data['data'], ['questions'], ['source', 'id', 'story'])\n",
        "ans = pd.json_normalize(train_data['data'], ['answers'],['id'])\n",
        "train_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])"
      ],
      "metadata": {
        "id": "l3lmZX2h1Qie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['q_first_word']=train_df['input_text_x'].str.lower().str.extract(r'(\\w+)')\n",
        "train_df['q_first_two_words']=train_df['input_text_x'].str.lower().str.extract(r'^((?:\\S+\\s+){1}\\S+).*')"
      ],
      "metadata": {
        "id": "uk_yQErc1XY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.loc[train_df['input_text_y']!='unknown']"
      ],
      "metadata": {
        "id": "vzef5d3C1bWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = json.load((open('/content/coqa/test.json')))\n",
        "qas = pd.json_normalize(test_data['data'], ['questions'], ['source', 'id', 'story'])\n",
        "ans = pd.json_normalize(test_data['data'], ['answers'],['id'])\n",
        "test_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])\n",
        "test_df = test_df.loc[test_df['input_text_y']!='unknown']"
      ],
      "metadata": {
        "id": "_3N5SYFV1nUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "4SZBQQib1rUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[['story','input_text_x', 'input_text_y', 'span_text', 'span_start']]\n",
        "val = val[['story','input_text_x', 'input_text_y', 'span_text', 'span_start']]\n",
        "test_df = test_df[['story','input_text_x', 'input_text_y', 'span_text', 'span_start']]\n",
        "train.rename(columns={'input_text_x': 'questions', 'input_text_y': 'answers', 'span_text': 'reasons'}, inplace=True)\n",
        "val.rename(columns={'input_text_x': 'questions', 'input_text_y': 'answers', 'span_text': 'reasons'}, inplace=True)\n",
        "test_df.rename(columns={'input_text_x': 'questions', 'input_text_y': 'answers', 'span_text': 'reasons'}, inplace=True)\n",
        "display(train.head(),val.head(),test_df.head())"
      ],
      "metadata": {
        "id": "XKLKAe_u1y_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForQuestionAnswering.from_pretrained('distilroberta-base')"
      ],
      "metadata": {
        "id": "RJEddB5zd9zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')"
      ],
      "metadata": {
        "id": "FgLfVFgVkkJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 400\n",
        "doc_stride = 200"
      ],
      "metadata": {
        "id": "IEEG8OXCt6eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer(train[\"questions\"][0], train[\"answers\"][0])"
      ],
      "metadata": {
        "id": "iHKHC8Mot6bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(train[\"questions\"])):\n",
        "#   if len(tokenizer(train[\"questions\"][i],train[\"story\"][i])[\"input_ids\"]) > 400:\n",
        "#     break\n"
      ],
      "metadata": {
        "id": "j9rdG8jvuOHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Truncate otherwise too large"
      ],
      "metadata": {
        "id": "LgRpSPo4u27_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# len(tokenizer(questions, stories, max_length = max_length, truncation = \"only_second\")[\"input_ids\"])"
      ],
      "metadata": {
        "id": "x-BjZP8YuOJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_data = tokenizer(questions, stories, max_length = max_length, truncation = \"only_second\", return_overflowing_tokens=True, stride=doc_stride)"
      ],
      "metadata": {
        "id": "nBkYynMhuOMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [len(x) for x in tokenized_data[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "NUixuXtvuOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for x in tokenized_data[\"input_ids\"][:2]:\n",
        "#   print(tokenizer.decode(x))"
      ],
      "metadata": {
        "id": "w6JRkMP0uORM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_data = tokenizer(questions, stories, max_length=max_length,truncation=\"only_second\",return_overflowing_tokens=True, return_offsets_mapping=True, stride=doc_stride)\n",
        "# print(tokenized_data[\"offset_mapping\"][0][:100])"
      ],
      "metadata": {
        "id": "iF3daez6uOTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first_token_id = tokenized_data[\"input_ids\"][0][1]\n",
        "# offsets = tokenized_data[\"offset_mapping\"][0][1]\n",
        "# print(\n",
        "#     tokenizer.convert_ids_to_tokens([first_token_id])[0],\n",
        "#     questions[offsets[0] : offsets[1]],\n",
        "# )"
      ],
      "metadata": {
        "id": "c59u3GEwuOV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence_ids = tokenized_data.sequence_ids()\n",
        "# print(sequence_ids)"
      ],
      "metadata": {
        "id": "LR0u0aKuuOYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Probabilmente sbagliato\n",
        "# start_char = len(answers)\n",
        "# end_char = start_char + len(answers[0])\n",
        "\n",
        "# #Start token index of current span in the text\n",
        "# token_start_index = 0\n",
        "# while sequence_ids[token_start_index] != 1:\n",
        "#   token_start_index += 1\n",
        "\n",
        "# #End token index of current span in text\n",
        "# token_end_index = len(tokenized_data[\"input_ids\"][0]) - 1\n",
        "# while sequence_ids[token_end_index] != 1:\n",
        "#   token_end_index -=1\n",
        "\n",
        "# #Detect if answer is out of span\n",
        "# offsets = tokenized_data[\"offset_mapping\"][0]\n",
        "# if offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char:\n",
        "#   while(token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char):\n",
        "#     token_start_index += 1\n",
        "#   start_position = token_start_index - 1\n",
        "#   while offsets[token_end_index][1] >= end_char:\n",
        "#     token_end_index -= 1\n",
        "#   end_position = token_end_index +1\n",
        "#   print(start_position, end_position)\n",
        "# else:\n",
        "#   print(\"Ask another question\")"
      ],
      "metadata": {
        "id": "NY5cj1ZpuOav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenizer.decode(tokenized_data[\"input_ids\"][0][start_position: end_position + 1]))\n",
        "# print(answers[0])"
      ],
      "metadata": {
        "id": "v5EhVeXUuOdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_on_right = tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "5DMTeipsuOfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_features(train=train):\n",
        "  tokenized_data = tokenizer(train[\"questions\" if pad_on_right else \"story\"].to_list(), train[\"story\" if pad_on_right else \"questions\"].to_list(),\n",
        "                             truncation='only_second' if pad_on_right else 'only_first', max_length = max_length,\n",
        "                             stride=doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True,padding='max_length')\n",
        "  \n",
        "  sample_mapping = tokenized_data.pop(\"overflow_to_sample_mapping\")\n",
        "  offset_mapping = tokenized_data.pop(\"offset_mapping\")\n",
        "\n",
        "  tokenized_data[\"start_positions\"] = []\n",
        "  tokenized_data[\"end_positions\"] = []\n",
        "\n",
        "  for i, offsets in enumerate(offset_mapping):\n",
        "    input_ids = tokenized_data[\"input_ids\"][i]\n",
        "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "    sequence_ids = tokenized_data.sequence_ids(i)\n",
        "    sample_index = sample_mapping[i]\n",
        "    answers = train[[\"answers\", \"span_start\"]].iloc[sample_index]\n",
        "    if answers[\"span_start\"]==0:\n",
        "      tokenized_data[\"start_positions\"].append(cls_index)\n",
        "      tokenized_data[\"end_positions\"].append(cls_index)\n",
        "    else:\n",
        "\n",
        "      start_char = answers[\"span_start\"]\n",
        "      end_char = start_char + len(answers[\"answers\"])\n",
        "\n",
        "      token_start_index = 0\n",
        "      while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "        token_start_index += 1\n",
        "      \n",
        "      token_end_index = len(input_ids) - 1\n",
        "      while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "        token_end_index -= 1\n",
        "\n",
        "      if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index] >= end_char).any():\n",
        "        tokenized_data[\"start_positions\"].append(cls_index)\n",
        "        tokenized_data[\"end_positions\"].append(cls_index)\n",
        "      else:\n",
        "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "          token_start_index +=1\n",
        "        \n",
        "        tokenized_data[\"start_positions\"].append(token_start_index - 1)\n",
        "        while offsets[token_end_index][1] >= end_char:\n",
        "          token_end_index -= 1\n",
        "        tokenized_data[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "  return tokenized_data"
      ],
      "metadata": {
        "id": "iMBi_I1RzBkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = prepare_train_features(train.iloc[:5])"
      ],
      "metadata": {
        "id": "xmGZV1obzBml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenized_train))"
      ],
      "metadata": {
        "id": "zJ58s0xkRQ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(train)\n",
        "print(dataset[\"span_start\"])"
      ],
      "metadata": {
        "id": "isKr9NQ8PVke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = dataset.map(prepare_train_features_d, batched=True, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "5-jWJHpJR0N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-5\n",
        "num_train_epochs=2\n",
        "weight_decay=0.01\n",
        "print(tokenized_train)"
      ],
      "metadata": {
        "id": "2RVrDdDfzBrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = model.prepare_tf_dataset(tokenized_train, shuffle=True, batch_size=32)"
      ],
      "metadata": {
        "id": "gvzC4Ow8B01S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "total_train_steps = len(train) * num_train_epochs\n",
        "\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=learning_rate, num_warmup_steps=0, num_train_steps=total_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "77gt-7DOzBtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = optimizer, jit_compile=True, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "pCcshiG6zBvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train.all(),\n",
        "    validation_data=val,\n",
        "    epochs=num_train_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "1LEwgsSnzBx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answer(question, text):\n",
        "\n",
        "    #tokenize question and text as a pair\n",
        "    input_ids = tokenizer.encode(question, text)\n",
        "    \n",
        "    #string version of tokenized ids\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    #segment IDs\n",
        "    #first occurence of [SEP] token\n",
        "    sep_idx = input_ids.index(tokenizer.sep_token_id)    #number of tokens in segment A (question)\n",
        "    num_seg_a = sep_idx+1    #number of tokens in segment B (text)\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    \n",
        "    #list of 0s and 1s for segment embeddings\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b    \n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    \n",
        "    #model output using input_ids and segment_ids\n",
        "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
        "    \n",
        "    #reconstructing the answer\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)    \n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "                \n",
        "    if answer.startswith(\"[CLS]\"):\n",
        "        answer = \"Ask another question\"\n",
        "    \n",
        "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
      ],
      "metadata": {
        "id": "nokvG-0Oeftw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(questions)):\n",
        "  question_answer(questions[i],stories[i])\n",
        "\n",
        "  print(\"Original answer:\\n\", answers[i])"
      ],
      "metadata": {
        "id": "h9MSy9W4g4Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAExqpkphFBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}