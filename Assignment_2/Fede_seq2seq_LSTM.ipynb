{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAST LINKs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=Gw3IZYrfKl4Z\n",
    "* https://medium.com/analytics-vidhya/fine-tune-a-roberta-encoder-decoder-model-trained-on-mlm-for-text-generation-23da5f3c1858\n",
    "* https://huggingface.co/course/chapter7/7?fw=tf\n",
    "\n",
    "LINKs:\n",
    "* https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "\n",
    "* https://github.com/Michael-M-Mike/Unibo-NLP-Assignments/blob/main/A2_Seq2Seq_Abstractive_Question_Answering_(QA)_on_CoQA/distilroberta_42.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Question Answering, CoQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadlines\n",
    "\n",
    "* **December 11**, 2022: deadline for having assignments graded by January 11, 2023\n",
    "* **January 11**, 2023: deadline for half-point speed bonus per assignment\n",
    "* **After January 11**, 2023: assignments are still accepted, but there will be no speed bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Question Answering (QA) on [CoQA](https://stanfordnlp.github.io/coqa/) dataset: a conversational QA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Given a question $Q$, a text passage $P$, the task is to generate the answer $A$.<br>\n",
    "$\\rightarrow A$ can be: (i) a free-form text or (ii) unanswerable;\n",
    "\n",
    "**Note**: an question $Q$ can refer to previous dialogue turns. <br>\n",
    "$\\rightarrow$ dialogue history $H$ may be a valuable input to provide the correct answer $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "We are going to experiment with transformer-based models to define the following models:\n",
    "\n",
    "1.  $A = f_\\theta(Q, P)$\n",
    "\n",
    "2. $A = f_\\theta(Q, P, H)$\n",
    "\n",
    "where $f_\\theta$ is the transformer-based model we have to define with $\\theta$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CoQA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=16vrgyfoV42Z2AQX0QY7LHTfrgektEKKh\" width=\"750\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "* **127k** QA pairs.\n",
    "* **8k** conversations.\n",
    "* **7** diverse domains: Children's Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science.\n",
    "* Average conversation length: **15 turns** (i.e., QA pairs).\n",
    "* Almost **half** of CoQA questions refer back to **conversational history**.\n",
    "* Only **train** and **validation** sets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset snippet\n",
    "\n",
    "The dataset is stored in JSON format. Each dialogue is represented as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"mctest\",\n",
    "    \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
    "    \"filename\": \"mc160.test.41\",\n",
    "    \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. \n",
    "    Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. [...]\" % <-- $P$\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"input_text\": \"What color was Cotton?\",   % <-- $Q_1$\n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"Where did she live?\",\n",
    "            \"turn_id\": 2\n",
    "        },\n",
    "        [...]\n",
    "    ],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"span_start\": 59,   % <-- $R_1$ start index\n",
    "            \"spand_end\": 93,    % <-- $R_1$ end index\n",
    "            \"span_text\": \"a little white kitten named Cotton\",   % <-- $R_1$\n",
    "            \"input_text\" \"white\",   % <-- $A_1$      \n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        [...]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifications\n",
    "\n",
    "Each dialogue also contains an additional field ```additional_answers```. For simplicity, we **ignore** this field and only consider one groundtruth answer $A$ and text rationale $R$.\n",
    "\n",
    "CoQA only contains 1.3% of unanswerable questions. For simplicity, we **ignore** those QA pairs.\n",
    "\n",
    "# [0] Functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install datasets\n",
    "# %pip install transformers\n",
    "# %pip install tensorflow_addons\n",
    "# %pip install allennlp-models\n",
    "# %pip install plotly==5.13.1\n",
    "\n",
    "\n",
    "# NOTE:\n",
    "#     - SEED ED ERRORE\n",
    "#     - LUNGHEZZA INPUTS E OUTPUTS - https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8\n",
    "#     - GRANDEZZA DATASETS\n",
    "#     - WARNINGS NELLA CREAZIONE DEL MODELLO E NEL TRAINING\n",
    "#     - COME UTILIZZARE SPAN DI TESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html, clear_output\n",
    "from itertools import chain,cycle\n",
    "from copy import deepcopy\n",
    "import urllib.request\n",
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import random \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import *\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, AdamW, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # use the GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # use the CPU\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Display dataframes\n",
    "def display(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:left\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h4 style=\"text-align: left;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "    \n",
    "# Setting seeds for reproducibility\n",
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Check tokenizer special tokens\n",
    "def check_tokens(tokenizer):\n",
    "    # Get the special tokens and their corresponding IDs\n",
    "    special_tokens = tokenizer.special_tokens_map\n",
    "    special_ids = tokenizer.convert_tokens_to_ids(list(special_tokens.values()))\n",
    "    print(\"Special tokens:\")\n",
    "    for token_type, token_list in special_tokens.items():\n",
    "        print(f\"{token_type}: {token_list}\")\n",
    "    # Print the special tokens and their corresponding IDs\n",
    "    for token, id in zip(special_tokens.keys(), special_ids):\n",
    "        print(f\"{token}: {id}\")\n",
    "        \n",
    "# Compute metrics in the trainer\n",
    "def compute_metrics(pred,tokenizer):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    \n",
    "    labels_text = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    preds_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    squad_scores=[]\n",
    "    for i in range(len(preds_text)):\n",
    "        squad_scores.append(compute_f1(str(preds_text[i]), str(labels_text[i])))\n",
    "    mean_squad_f1 = sum(squad_scores)/len(squad_scores)\n",
    "\n",
    "    return {\"squad_f1_score\": mean_squad_f1}\n",
    "\n",
    "# # Generate Answers (on test set)\n",
    "# def generate_answers(test_loader,model,tokenizer):\n",
    "#     i=0\n",
    "#     for batch in tqdm(test_loader):\n",
    "\n",
    "#         example = batch['input_ids'].to(device)\n",
    "#         att_mask = batch['attention_mask'].to(device)\n",
    "#         generated_ids = model.generate(input_ids=example, \n",
    "#                                           attention_mask=att_mask,\n",
    "#                                           max_length=tokenizer.model_max_length\n",
    "#                                          )\n",
    "#         ex = tokenizer.batch_decode(example, skip_special_tokens=True)\n",
    "#         print(ex[0])\n",
    "#         print(test_df['question'][i:i+8])\n",
    "\n",
    "#         generated_answers = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#         print(f'Generated ans: {generated_answers}')\n",
    "#         true = batch[\"labels\"]\n",
    "#         ground_truth = tokenizer.batch_decode(true, skip_special_tokens=True)\n",
    "#         print(f'True ans: {ground_truth}')\n",
    "#         i+=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQuAD metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions taken from [the official evaluation script]\n",
    "(https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/)\n",
    "for SQuAD version 2.0.\n",
    "\"\"\"\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, Sequence, TypeVar, Tuple\n",
    "\n",
    "\n",
    "def make_qid_to_has_ans(dataset):\n",
    "    qid_to_has_ans = {}\n",
    "    for article in dataset:\n",
    "        for p in article[\"paragraphs\"]:\n",
    "            for qa in p[\"qas\"]:\n",
    "                qid_to_has_ans[qa[\"id\"]] = bool(qa[\"answers\"])\n",
    "    return qid_to_has_ans\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_pred: str, a_gold: str) -> int:\n",
    "    return int(normalize_answer(a_pred) == normalize_answer(a_gold))\n",
    "\n",
    "\n",
    "def compute_f1(a_pred: str, a_gold: str) -> float:\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n",
    "    num_same = sum(common.values())\n",
    "    if len(pred_toks) == 0 or len(gold_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return float(pred_toks == gold_toks)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "_P = TypeVar(\"_P\")\n",
    "_G = TypeVar(\"_G\")\n",
    "_T = TypeVar(\"_T\", int, float, Tuple[int, ...], Tuple[float, ...])\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(\n",
    "    metric_fn: Callable[[_P, _G], _T], prediction: _P, ground_truths: Sequence[_G]\n",
    ") -> _T:\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def get_metric_score(prediction: str, gold_answers: Sequence[str]) -> Tuple[int, float]:\n",
    "    exact_scores = metric_max_over_ground_truths(compute_exact, prediction, gold_answers)\n",
    "    f1_scores = metric_max_over_ground_truths(compute_f1, prediction, gold_answers)\n",
    "    return exact_scores, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Remove unaswerable QA pairs\n",
    "\n",
    "Write your own script to remove unaswerable QA pairs from both train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        urllib.request.urlretrieve(url_path, filename=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Inspection\n",
    "\n",
    "Spend some time in checking accurately the dataset format and how to retrieve the tasks' inputs and outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframes and removing unanswerable questions\n",
    "train_data = json.load((open('coqa/train.json')))\n",
    "test_data = json.load((open('coqa/test.json')))\n",
    "\n",
    "qas = pd.json_normalize(train_data['data'], ['questions'], ['source', 'id', 'story'])\n",
    "ans = pd.json_normalize(train_data['data'], ['answers'],['id'])\n",
    "train_val_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])\n",
    "train_val_df = train_val_df.loc[train_val_df['input_text_y']!='unknown']\n",
    "\n",
    "qas = pd.json_normalize(test_data['data'], ['questions'], ['source', 'id', 'story'])\n",
    "ans = pd.json_normalize(test_data['data'], ['answers'],['id'])\n",
    "test_df = pd.merge(qas,ans, left_on=['id','turn_id'], right_on=['id','turn_id'])\n",
    "test_df = test_df.loc[test_df['input_text_y']!='unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing bad turns\n",
    "train_val_df = train_val_df.loc[(train_val_df['bad_turn_x'] != 'True') & (train_val_df['bad_turn_y'] != 'True')]\n",
    "\n",
    "# Removing equal text/answer entries\n",
    "train_val_df = train_val_df[train_val_df.story != train_val_df.input_text_y]\n",
    "test_df = test_df[test_df.story != test_df.input_text_y]\n",
    "\n",
    "# Removing enties with empty answers\n",
    "train_val_df = train_val_df[train_val_df['input_text_y'].str.len()>0]\n",
    "test_df = test_df[test_df['input_text_y'].str.len()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocess\n",
    "def preprocess(ds,columns):\n",
    "#    ds = ds.replace(r'\\n',' ', regex=True)\n",
    "#    ds = ds.replace(r'[^\\w\\s]+', ' ', regex=True)\n",
    "#     for feature in columns:\n",
    "#         ds[feature] = ds[feature].str.lower().str.strip()\n",
    "        \n",
    "    return ds\n",
    "\n",
    "columns = ['story', 'input_text_x', 'span_text', 'input_text_y']\n",
    "\n",
    "train_val_df = preprocess(train_val_df,columns)\n",
    "test_df = preprocess(test_df,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Train, Validation and Test splits\n",
    "\n",
    "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
    "\n",
    "We'll consider the provided validation set as a test set. <br>\n",
    "$\\rightarrow$ Write your own script to:\n",
    "* Split the train data in train and validation splits (80% train and 20% val)\n",
    "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
    "* Perform splitting using the following seed for reproducibility: 42\n",
    "\n",
    "#### Reproducibility Memo\n",
    "\n",
    "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "set_reproducibility(42)\n",
    "\n",
    "train_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 42).split(train_val_df, groups=train_val_df['id']))\n",
    "\n",
    "train_df = train_val_df.iloc[train_inds]\n",
    "val_df = train_val_df.iloc[val_inds].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set [(85823, 11)]\n",
      "\tFeatures: ['input_text_x', 'turn_id', 'bad_turn_x', 'source', 'id', 'story', 'span_start', 'span_end', 'span_text', 'input_text_y', 'bad_turn_y']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:left\"><td style=\"vertical-align:top\"><h4 style=\"text-align: left;\"></h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text_x</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>bad_turn_x</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>span_start</th>\n",
       "      <th>span_end</th>\n",
       "      <th>span_text</th>\n",
       "      <th>input_text_y</th>\n",
       "      <th>bad_turn_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.</td>\n",
       "      <td>151</td>\n",
       "      <td>179</td>\n",
       "      <td>Formally established in 1475</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.</td>\n",
       "      <td>454</td>\n",
       "      <td>494</td>\n",
       "      <td>he Vatican Library is a research library</td>\n",
       "      <td>research</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.</td>\n",
       "      <td>457</td>\n",
       "      <td>511</td>\n",
       "      <td>Vatican Library is a research library for history, law</td>\n",
       "      <td>history, and law</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and?</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.</td>\n",
       "      <td>457</td>\n",
       "      <td>545</td>\n",
       "      <td>Vatican Library is a research library for history, law, philosophy, science and theology</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.</td>\n",
       "      <td>769</td>\n",
       "      <td>879</td>\n",
       "      <td>March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts</td>\n",
       "      <td>a  project</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set [(21452, 12)]\n",
      "\tFeatures: ['index', 'input_text_x', 'turn_id', 'bad_turn_x', 'source', 'id', 'story', 'span_start', 'span_end', 'span_text', 'input_text_y', 'bad_turn_y']\n",
      "Test set [(7917, 9)]\n",
      "\tFeatures: ['input_text_x', 'turn_id', 'source', 'id', 'story', 'span_start', 'span_end', 'span_text', 'input_text_y']\n"
     ]
    }
   ],
   "source": [
    "# Checking the Dataframes\n",
    "print(f'Training set [{train_df.shape}]')\n",
    "print(f'\\tFeatures: {list(train_df.columns)}')\n",
    "display(train_df.head())\n",
    "#display(train_df.loc[11:15,['id', 'input_text_x', 'input_text_y', 'span_text']])\n",
    "\n",
    "print(f'Validation set [{val_df.shape}]')\n",
    "print(f'\\tFeatures: {list(val_df.columns)}')\n",
    "#display(val_df.head())\n",
    "#display(val_df.loc[11:15,['id', 'input_text_x', 'input_text_y', 'span_text']])\n",
    "\n",
    "print(f'Test set [{test_df.shape}]')\n",
    "print(f'\\tFeatures: {list(test_df.columns)}')\n",
    "#display(test_df.head())\n",
    "#display(test_df.loc[11:15,['id', 'input_text_x', 'input_text_y', 'span_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if there is any overlapping dialogue between train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlap\n"
     ]
    }
   ],
   "source": [
    "set_train = set(train_df['id'])\n",
    "set_val = set(val_df['id'])\n",
    "\n",
    "overlap = False\n",
    "for i in set_train:\n",
    "    if i in set_val:\n",
    "        overlap = True\n",
    "        break\n",
    "\n",
    "print('Overlap' if overlap else 'No overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes to Datasets\n",
    "train_df_to_ds = train_df\n",
    "val_df_to_ds = val_df\n",
    "test_df_to_ds = test_df\n",
    "\n",
    "train_df_to_ds = train_df_to_ds.rename(columns={'input_text_x': 'question', 'story': 'context',\\\n",
    "                                               'input_text_y': 'answer', 'span_text': 'text'})\n",
    "val_df_to_ds = val_df_to_ds.rename(columns={'input_text_x': 'question', 'story': 'context',\\\n",
    "                                               'input_text_y': 'answer', 'span_text': 'text'})\n",
    "test_df_to_ds = test_df_to_ds.rename(columns={'input_text_x': 'question', 'story': 'context',\\\n",
    "                                               'input_text_y': 'answer', 'span_text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the dataset is huge and we are more focused on the reasoning on our choices rather than obtaining the best results, we are going to extract a portion of it.\n",
    "The next step is gonna be the truncation of the inputs lengths. The pre-trained models that are gonna be tested can process lengths up to 512, that is why our truncation will be at least equal to this value; moreover, we are going to sort the datasets according to the sum of the lengths of the 'context' and 'question' fields together, expecting to truncate the least possible number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'context' and 'question' fields for each dataframe\n",
    "train_df_to_ds['context_question'] = train_df_to_ds['context'] + train_df_to_ds['question']\n",
    "val_df_to_ds['context_question'] = val_df_to_ds['context'] + val_df_to_ds['question']\n",
    "test_df_to_ds['context_question'] = test_df_to_ds['context'] + test_df_to_ds['question']\n",
    "\n",
    "# Define a function to compute the length of 'context_question'\n",
    "def get_context_question_length(df):\n",
    "    return (df['context_question'].apply(len))\n",
    "\n",
    "# Compute the lengths of 'context_question' for each dataframe\n",
    "train_lengths = get_context_question_length(train_df_to_ds)\n",
    "val_lengths = get_context_question_length(val_df_to_ds)\n",
    "test_lengths = get_context_question_length(test_df_to_ds)\n",
    "\n",
    "# Sort each dataframe by length of 'context + question'\n",
    "train_df_to_ds = train_df_to_ds.iloc[train_lengths.argsort()]\n",
    "val_df_to_ds = val_df_to_ds.iloc[val_lengths.argsort()]\n",
    "test_df_to_ds = test_df_to_ds.iloc[test_lengths.argsort()]\n",
    "\n",
    "# Drop the 'context_question' column from each dataframe\n",
    "train_df_to_ds = train_df_to_ds.drop('context_question', axis=1)\n",
    "val_df_to_ds = val_df_to_ds.drop('context_question', axis=1)\n",
    "test_df_to_ds = test_df_to_ds.drop('context_question', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples(sample):\n",
    "    tokenized_data = tokenizer(sample[\"context\"],sample[\"question\"], truncation=\"only_first\", max_length=512, padding=\"max_length\")\n",
    "    input_ids = tokenized_data[\"input_ids\"]\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "    if sample['context'][0]==-1:\n",
    "        start_position = cls_index\n",
    "        end_position = cls_index\n",
    "    else:\n",
    "        #gold_text = sample['text']\n",
    "        gold_text = sample['text'][sample['span_start']:sample['span_end']]\n",
    "        start_char = sample['span_start']\n",
    "        end_char = sample['span_end']\n",
    "\n",
    "        if sample['context'][start_char-1:end_char-1] == gold_text:\n",
    "            start_char -= -1\n",
    "            end_char -= 1\n",
    "        elif sample['context'][start_char-2:end_char-2] == gold_text:\n",
    "            start_char -= 2\n",
    "            end_char -= 2\n",
    "\n",
    "        start_token = tokenized_data.char_to_token(start_char)\n",
    "        end_token = tokenized_data.char_to_token(end_char)\n",
    "\n",
    "        if start_token is None:\n",
    "            start_token = 512\n",
    "        if end_token is None:\n",
    "            end_token = 512\n",
    "\n",
    "        start_position = start_token\n",
    "        end_position = end_token\n",
    "    return {'input_ids': tokenized_data['input_ids'],\n",
    "            'attention_mask': tokenized_data['attention_mask'],\n",
    "            'start_positions':start_position,\n",
    "            'end_positions':end_position}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8b8813747043cdaaee3fb33bcf6fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1931d5975c84617a4dc1f9c43d42e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f481be0c1aa458a946ec37660ef27ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'turn_id', 'bad_turn_x', 'source', 'id', 'context', 'span_start', 'span_end', 'text', 'answer', 'bad_turn_y', '__index_level_0__', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 3000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train_df_to_ds)\n",
    "eval_ds = Dataset.from_pandas(val_df_to_ds)\n",
    "test_ds = Dataset.from_pandas(test_df_to_ds)\n",
    "flat_train = train_ds.flatten()\n",
    "flat_val = eval_ds.flatten()\n",
    "flat_test = test_ds.flatten()\n",
    "#print(flat_train[0])\n",
    "processed_train_data =flat_train.select(range(3000)).map(process_samples)\n",
    "processed_val_data = flat_val.select(range(3000)).map(process_samples)\n",
    "processed_test_data = flat_test.select(range(1000)).map(process_samples)\n",
    "print(processed_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seq2seq model class\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force and t < max_len else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "input_dim = tokenizer.vocab_size\n",
    "output_dim = tokenizer.vocab_size\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate=1e-3\n",
    "batch_size = 8\n",
    "\n",
    "encoder = Encoder(input_dim,hidden_dim,n_layers,dropout).to(device)\n",
    "decoder = Decoder(output_dim, hidden_dim,n_layers,dropout).to(device)\n",
    "model = Seq2Seq(encoder,decoder,device).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=Seq2Seq)\n",
    "train_iterator = torch.utils.data.DataLoader(processed_train_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          collate_fn=data_collator)\n",
    "val_iterator = torch.utils.data.DataLoader(processed_val_data,batch_size=batch_size,collate_fn=data_collator)\n",
    "\n",
    "\n",
    "# train_iterator, valid_iterator = torchtext.data.BucketIterator.splits((processed_train_data,processed_val_data),\n",
    "#                                                        batch_size=batch_size,\n",
    "#                                                        sort_within_batch=True,\n",
    "#                                                        sort_key=lambda x: len(x['context']),\n",
    "#                                                        device=device\n",
    "#                                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Function\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.context\n",
    "        trg = batch.answer\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src,trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1,output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output,trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
